{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#  Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from math import log\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# install libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = \"../Data/clean_train_data_mok.csv\"\n",
    "valid_path = \"../Data/clean_valid_data_mok.csv\"\n",
    "test_path = \"../Data/clean_test_data_mok.csv\"\n",
    "# Replace 'Column29' with the actual column name that has mixed types\n",
    "dtype = {'Column29': 'str'}\n",
    "\n",
    "train = pd.read_csv(train_path, index_col='Claim Identifier',\n",
    "                    dtype=dtype, low_memory=False)\n",
    "valid = pd.read_csv(valid_path, index_col='Claim Identifier',\n",
    "                    dtype=dtype, low_memory=False)\n",
    "test = pd.read_csv(test_path, index_col='Claim Identifier',\n",
    "                   dtype=dtype, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.dropna(subset=['Claim Injury Type'])\n",
    "y_train_int = train['Claim Injury Type']\n",
    "y_train_str = y_train_int.astype(str)\n",
    "train = train.drop(columns=['Claim Injury Type'])\n",
    "\n",
    "\n",
    "valid = valid.dropna(subset=['Claim Injury Type'])\n",
    "y_valid_int = valid['Claim Injury Type']\n",
    "y_valid_str = y_valid_int.astype(str)\n",
    "valid = valid.drop(columns=['Claim Injury Type'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "model_rf = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "\n",
    "num_features = train.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "model_rf.fit(train [num_features], y_train_str)\n",
    "y_pred_rf = model_rf.predict(valid[num_features])\n",
    "y_pred_rf_train = model_rf.predict(train[num_features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a string\n",
    "# Save the model\n",
    "import pickle\n",
    "\n",
    "def report_and_save (model,model_name,y_pred, y, model_filepath = '../Models'\n",
    "                     , report_filepath = '../Reports'): \n",
    "    \n",
    "    \n",
    "    model_parameters = model.get_params()\n",
    "    model_filename = model_name + '.sav'\n",
    "    full_model_filename_os = os.path.join(model_filepath, model_filename)\n",
    "    report_filename = model_name + '.txt'\n",
    "    full_report_filename_os = os.path.join(report_filepath, report_filename)\n",
    "\n",
    "    try:\n",
    "        with open(full_model_filename_os, 'wb') as file:\n",
    "            pickle.dump(model, file)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while saving the model: {e}\")\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    precision = precision_score(y, y_pred, average='weighted')\n",
    "    recall = recall_score(y, y_pred, average='weighted')\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "\n",
    "    # Save the string into a text file\n",
    "    try:\n",
    "        with open(full_report_filename_os, \"w\") as file:\n",
    "            file.write(model_name)\n",
    "            file.write(\"\\n________________________\\n\")\n",
    "            file.write('model_parameters: \\n')\n",
    "            file.write(str(model_parameters).replace(\",\", \"\\n\"))   \n",
    "            file.write(\"\\n________________________\\n\")\n",
    "            file.write('Accuracy: ')\n",
    "            file.write(str(accuracy))\n",
    "            file.write(\"\\n________________________\\n\")\n",
    "            file.write('Precision: ')\n",
    "            file.write(str(precision))\n",
    "            file.write(\"\\n________________________\\n\")\n",
    "            file.write('Recall: ')\n",
    "            file.write(str(recall))\n",
    "            file.write(\"\\n________________________\\n\")\n",
    "            file.write('Confusion Matrix:\\n')\n",
    "            file.write(str(cm))\n",
    "            file.write(\"\\n________________________\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while writing the report: {e}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_and_save(model_rf, 'RandomForest', y_pred_rf, y_valid_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../Models/Random Forest.sav'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"RandomForest\"\n",
    "filepath = '../Models'\n",
    "filename = model_name + '.sav'\n",
    "filename_os = os.path.join(filepath, filename)\n",
    "filename_os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## try multiple models and choose the best one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# Create the RFE object and rank each feature\n",
    "rfe = RFE(estimator=model_rf, n_features_to_select=4, step=1)\n",
    "rfe.fit(train[num_features], y_train_str)\n",
    "ranking = rfe.ranking_\n",
    "ranking\n",
    "\n",
    "# Get the indices of the 4 most important features\n",
    "indices = [i for i, x in enumerate(ranking) if x == 1]\n",
    "indices\n",
    "\n",
    "#train the model with the 4 most important features\n",
    "model_rf.fit(train[num_features].iloc[:, indices], y_train_str)\n",
    "y_pred_rf = model_rf.predict(valid[num_features].iloc[:, indices])\n",
    "y_pred_rf_train = model_rf.predict(train[num_features].iloc[:, indices])\n",
    "\n",
    "report_and_save(model_rf, 'RandomForest_RFE', y_pred_rf, y_valid_str)\n",
    "\n",
    "precisions_rfe = [] \n",
    "f1_rfe = []\n",
    "best_set_of_features_rfe = []\n",
    "best_score_rfe = 0\n",
    "for n_featrues in range(1, 10):\n",
    "    rfe = RFE(estimator=model_rf, n_features_to_select=n_featrues, step=1)\n",
    "    rfe.fit(train[num_features], y_train_str)\n",
    "    ranking = rfe.ranking_\n",
    "    indices = [i for i, x in enumerate(ranking) if x == 1]\n",
    "    model_rf.fit(train[num_features].iloc[:, indices], y_train_str)\n",
    "    y_pred_rf = model_rf.predict(valid[num_features].iloc[:, indices])\n",
    "    y_pred_rf_train = model_rf.predict(train[num_features].iloc[:, indices])\n",
    "    report_and_save(model_rf, f'RandomForest_RFE_{n_featrues}', y_pred_rf, y_valid_str)\n",
    "    \n",
    "    precisions_rfe.append(precision_score(y_valid_str, y_pred_rf, average='weighted'))\n",
    "    f1_model = f1_score(y_valid_str, y_pred_rf, average='weighted')\n",
    "    f1_rfe.append(f1_model)\n",
    "    if f1_score > best_score:\n",
    "        best_score = f1_score\n",
    "        best_set_of_features = indices\n",
    "    \n",
    "plt.plot(range(1, 10), precisions_rfe, label='Precision')\n",
    "plt.plot(range(1, 10), f1_rfe, label='F1')\n",
    "plt.xlabel('Number of Features')\n",
    "plt.ylabel('Score')\n",
    "plt.legend()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multple layers perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (872194366.py, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[36], line 16\u001b[0;36m\u001b[0m\n\u001b[0;31m    selector = SelectKBest(score_func=f_classif, k=n_featrues)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# multiple layer perceptron\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "model_mlp = MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=1000, random_state=0)\n",
    "model_mlp.fit(train[num_features], y_train_str)\n",
    "y_pred_mlp = model_mlp.predict(valid[num_features])\n",
    "y_pred_mlp_train = model_mlp.predict(train[num_features])\n",
    "\n",
    "report_and_save(model_mlp, 'MLP', y_pred_mlp, y_valid_str)\n",
    "\n",
    "precisions_mpl = []\n",
    "f1_mlp = []\n",
    "best_set_of_features_mlp = []\n",
    "best_score_mlp = 0\n",
    "for n_featrues in range(1, 10):\n",
    "    rfe = RFE(estimator=model_mlp, n_features_to_select=n_featrues, step=1)\n",
    "    rfe.fit(train[num_features], y_train_str)\n",
    "    ranking = rfe.ranking_\n",
    "    indices = [i for i, x in enumerate(ranking) if x == 1]\n",
    "    model_mlp.fit(train[num_features].iloc[:, indices], y_train_str)\n",
    "    y_pred_mlp = model_mlp.predict(valid[num_features].iloc[:, indices])\n",
    "    y_pred_mlp_train = model_mlp.predict(train[num_features].iloc[:, indices])\n",
    "    report_and_save(model_mlp, f'MLP_RFE_{n_featrues}', y_pred_mlp, y_valid_str)\n",
    "    \n",
    "    precisions_mpl.append(precision_score(y_valid_str, y_pred_mlp, average='weighted'))\n",
    "    f1_model = f1_score(y_valid_str, y_pred_mlp, average='weighted')\n",
    "    f1_mlp.append(f1_model)\n",
    "    if f1_score > best_score:\n",
    "        best_score = f1_score\n",
    "        best_set_of_features = indices\n",
    "        \n",
    "plt.plot(range(1, 10), precisions_mpl, label='Precision')\n",
    "plt.plot(range(1, 10), f1_mlp, label='F1')\n",
    "plt.xlabel('Number of Features')\n",
    "plt.ylabel('Score')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
