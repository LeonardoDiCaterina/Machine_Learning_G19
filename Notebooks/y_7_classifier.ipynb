{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import robust scaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "#import one hot encoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "class PreProcessor1:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.status = 'train'\n",
    "        self.scaler = RobustScaler()\n",
    "        self.version = '3.0_frangoInCantine'\n",
    "        self.encoder = OneHotEncoder()\n",
    "        self.max_col_onehot = 5\n",
    "        \n",
    "        self.date_cols = []\n",
    "        self.desc_cols = []\n",
    "        self.code_cols = []\n",
    "        self.casting_methods = []\n",
    "        self.casting_list = [\n",
    "    ('Age at Injury',                       'Int64'),\n",
    "    ('Average Weekly Wage',                 'float64'),\n",
    "    # ('WCB Decision',                        'remove'),\n",
    "    ('Alternative Dispute Resolution',      'string-U-nan'),\n",
    "    ('District Name',                       'none'),\n",
    "    ('Carrier Name',                        'string'),\n",
    "    ('IME-4 Count',                         'Int64'),\n",
    "    ('COVID-19 Indicator',                  'string'),\n",
    "    ('Number of Dependents',                'Int64'),\n",
    "    ('Carrier Type',                         '.str[:2]'),\n",
    "    ('Medical Fee Region',                   'string'),\n",
    "    ('County of Injury',                    'string'),\n",
    "    ('Agreement Reached',                   'Int64'),\n",
    "    ('Attorney/Representative',             'string'),\n",
    "    ('Birth Year',                          'Int64'),\n",
    "    ('Gender',                              'string'),\n",
    "    ('Zip Code',                            '.str[0:5]'),\n",
    "]\n",
    "        self.casted_cols = []\n",
    "        self.transformation_list = [\n",
    "    ('Age at Injury',                   'none',             '-'),\n",
    "    ('Average Weekly Wage',             'log',              'log_Average Weekly Wage'),\n",
    "    # ('WCB Decision',                    'none',             '-'),\n",
    "    ('Alternative Dispute Resolution',  'dummy-YN',         'Alternative Dispute Resolution'),\n",
    "    ('District Name',                   'none',             '-'),\n",
    "    ('Carrier Name',                    'none',             '-'),\n",
    "    ('IME-4 Count',                     'none',             '-'),\n",
    "    ('COVID-19 Indicator',              'dummy-YN',         'COVID-19 Indicator'),\n",
    "    ('Number of Dependents',            'none',             '-'),\n",
    "    ('Carrier Type',                    'oneHot',           '-OneHot'),\n",
    "    ('Carrier Type',                    'freq_encode',      'fe_Carrier Type'),\n",
    "    ('Medical Fee Region',              'oneHot',           '-oneHot'),\n",
    "    ('Medical Fee Region',              'freq_encode',      'fe_Medical Fee Region'),\n",
    "    ('County of Injury',                'oneHot',           '-oneHot'),\n",
    "    ('County of Injury',                'freq_encode',      'fe_County of Injury'),\n",
    "    ('Agreement Reached',               'none',             '-oneHot'),\n",
    "    ('Attorney/Representative',         'dummy-YN',         'Attorney/Representative'),\n",
    "    ('Birth Year',                      'subtract_1900',    'Age'),\n",
    "    ('Gender',                          'oneHot',           '-oneHot'),\n",
    "]     \n",
    "        self.transformed_cols = []\n",
    "        self.fillna_list = []\n",
    "        \n",
    "        self.sclaing_list = [   ('Carrier Type',                     0),\n",
    "                                ('Zip Code',                         0),\n",
    "                                ('log_Average Weekly Wage',          1),\n",
    "                                ('Alternative Dispute Resolution',   0),\n",
    "                                ('COVID-19 Indicator',               0),\n",
    "                                ('fe_Carrier Type',                  1),\n",
    "                                ('fe_Medical Fee Region',            1),\n",
    "                                ('fe_County of Injury',              1),\n",
    "                                ('Attorney/Representative',          0),\n",
    "                                ('Age',                              1),\n",
    "                                ('Assembly Month',                   1),\n",
    "                                ('Assembly Year',                    1),\n",
    "                                ('C-3 Month',                        1),\n",
    "                                ('C-3 Year',                         1),\n",
    "                                ('Accident Month',                   1),\n",
    "                                ('Accident Year',                    1),\n",
    "                                ('C-2 Month',                        1),\n",
    "                                ('C-2 Year',                         1),\n",
    "                                ('First Hearing Month',              0),\n",
    "                                ('First Hearing Year',               0),\n",
    "                                ('fe_WCIO Part Of Body Code',        1),\n",
    "                                ('fe_Industry Code',                 1),\n",
    "                                ('fe_WCIO Nature of Injury Code',    1),\n",
    "                                ('fe_Zip Code',                      1),\n",
    "                                ('fe_WCIO Cause of Injury Code',     1),\n",
    "                                ('WCIO Part Of Body Code',           1),\n",
    "                                ('Industry Code',                    1),\n",
    "                                ('WCIO Nature of Injury Code',       1),\n",
    "                                ('Gender_U',                         0),\n",
    "                                ('Gender_X',                         0),\n",
    "                                ('IME-4 Count',                      1),\n",
    "                                ('Age at Injury',                    1),\n",
    "                                ('District Name',                    0),\n",
    "                                ('Average Weekly Wage',              1),\n",
    "                                ('Medical Fee Region',               0),\n",
    "                                ('Number of Dependents',             1),\n",
    "                                ('Carrier Name',                     0),\n",
    "                                ('Gender_F',                         0),\n",
    "                                ('Agreement Reached',                0),\n",
    "                                ('Gender',                           0),\n",
    "                                ('Birth Year',                       1),\n",
    "                                ('WCB Decision',                     0),\n",
    "                                ('Gender_M',                         0),\n",
    "                                ('WCIO Cause of Injury Code',        1),\n",
    "                                ('County of Injury',                 0)]\n",
    "\n",
    "\n",
    "# ------------------------ internal functions\n",
    "    def update_status(self, status):\n",
    "        if status in ['train', 'valid', 'test']:\n",
    "            self.status = status\n",
    "        else:  \n",
    "            print('Unknown status')\n",
    "    \n",
    "    def __str__(self):\n",
    "        return (f\"PreProcessor1: {self.status}\" +\n",
    "            f\"scaler: {self.scaler}\" +\n",
    "                f\"version: {self.version}\")\n",
    "        \n",
    "        \n",
    "       \n",
    "        \n",
    "# ------------------------casting part\n",
    "        \n",
    "        \n",
    "    def update_casted_cols(self):     \n",
    "        self.casted_cols = [col for col, method in self.casting_list]\n",
    "\n",
    "    def update_casting_methods(self):\n",
    "        self.casting_methods = [method for col, method in self.casting_list]\n",
    "\n",
    "    def set_castings(self,df):\n",
    "        df_cols = df.columns\n",
    "        columns_to_be_casted = set(df_cols) - set(self.casted_cols)\n",
    "    \n",
    "        date_cols = []\n",
    "        date_cols.extend([x for x in df_cols if 'Date' in x])\n",
    "        for col in date_cols:\n",
    "            self.append_casting(col, 'string')\n",
    "        columns_to_be_casted = columns_to_be_casted - set(date_cols)\n",
    "    \n",
    "        desc_cols = []\n",
    "        desc_cols.extend([x for x in df_cols if 'Description' in x])\n",
    "        for col in desc_cols:\n",
    "            self.append_casting(col, 'remove')\n",
    "        columns_to_be_casted = columns_to_be_casted - set(desc_cols)\n",
    "    \n",
    "        code_cols = []\n",
    "        code_cols.extend([x for x in df_cols if 'Code' in x])\n",
    "        for col in code_cols:\n",
    "            self.append_casting(col, 'Int64')\n",
    "        columns_to_be_casted = columns_to_be_casted - set(code_cols)\n",
    "    \n",
    "        if len(columns_to_be_casted) > 0:\n",
    "            print ('Columns that are not casted:')\n",
    "            for col in columns_to_be_casted:\n",
    "                try:\n",
    "                    print (f'-{col}: {df[col].dtype}')\n",
    "                except Exception as e: \n",
    "                    print(e)\n",
    "                    print (f'-{col}: not found')\n",
    "\n",
    "    def look_for_feature_casting(self,feature):\n",
    "        i = 0\n",
    "        for col, method in self.casting_list:\n",
    "            if col == feature:\n",
    "                print(f'-Column {col} is casted as {method} at index {i}')\n",
    "                return method\n",
    "            i += 1\n",
    "        return None\n",
    "\n",
    "    def update_casting_list(self,feature,method):\n",
    "        i = 0\n",
    "        for col, method in self.casting_list:\n",
    "            if col == feature:\n",
    "                self.casting_list[i] = (feature, method)\n",
    "                return\n",
    "            i += 1\n",
    "        #print(f'-Column {feature} not found in casting list. Adding it now')\n",
    "        self.append_casting(feature, method)\n",
    "    \n",
    "    def append_casting(self,feature,method):\n",
    "        self.casting_list.append((feature, method))\n",
    "    \n",
    "        \n",
    "    def cast_pipeline(self,df):\n",
    "        for col, method in self.casting_list:\n",
    "            if method == 'Int64':\n",
    "                df = self.cast_Int64(df, col)\n",
    "            elif method == 'float64':\n",
    "                df = self.cast_Float64(df, col)\n",
    "            elif method == 'string':\n",
    "                df = self.cast_string(df, col)\n",
    "            elif method == 'string-U-nan':\n",
    "                df = self.cast_string_U_nan(df, col)\n",
    "            elif method == '.str[:2]':\n",
    "                df = self.cast_string_2(df, col)\n",
    "                self.append_scaling(col, 0)\n",
    "            elif method == 'datetime64':\n",
    "                df = self.cast_datetime64(df, col)\n",
    "                self.append_scaling(col, 0)\n",
    "            elif method == 'remove':\n",
    "                df = self.cast_remove(df, col)\n",
    "            elif method == '.str[0:5]':\n",
    "                df = self.cast_string_5(df, col)\n",
    "                self.append_scaling(col, 0)\n",
    "                \n",
    "            elif method == 'none':\n",
    "                pass\n",
    "            else:\n",
    "                print(f'Unknown method {method} for column {col}')\n",
    "        return df\n",
    "\n",
    "    def cast_Int64(self, df, col):\n",
    "        try:\n",
    "            df[col] = df[col].astype('Int64')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'-Column {col} not found')\n",
    "        return df\n",
    "\n",
    "\n",
    "    def cast_Float64(self, df, col):\n",
    "        try:\n",
    "            df[col] = df[col].astype('float64')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'-Column {col} not found')\n",
    "\n",
    "        return df\n",
    "\n",
    "    def cast_string(self, df, col):\n",
    "        try:\n",
    "            df[col] = df[col].astype('string')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'-Column {col} not found')\n",
    "\n",
    "        return df\n",
    "\n",
    "    def cast_string_U_nan(self, df, col):\n",
    "        try:\n",
    "            df[col] = df[col].astype('string')\n",
    "            df[col] = df[col].replace('U', 'N')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'-Column {col} not found')\n",
    "        return df\n",
    "\n",
    "    def cast_string_2(self, df, col):\n",
    "        try:\n",
    "            df[col] = df[col].astype('string')\n",
    "            df[col] = df[col].str[:2]\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'-Column {col} not found')\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def cast_string_5(self, df, col):\n",
    "        try:\n",
    "            df[col] = df[col].astype('string')\n",
    "            df[col] = df[col].fillna('00000')\n",
    "            df[col] = df[col].str[:5]\n",
    "            df[col] = df[col].apply(lambda x: int(x) if x.isnumeric() else 0)\n",
    "            #df[col] = df[col].astype('string')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'-Column {col} not found')\n",
    "        \n",
    "        return df\n",
    "\n",
    "\n",
    "    def cast_datetime64(self, df, col):\n",
    "        try:\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'-Column {col} not found')\n",
    "        \n",
    "        return df\n",
    "\n",
    "\n",
    "    def cast_remove(self, df, col):\n",
    "        try:\n",
    "            df.drop(columns=[col], inplace=True)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'-Column {col} not found')\n",
    "        return df\n",
    "    \n",
    "    \n",
    "# ---------------------------------encoding part\n",
    "\n",
    "    def refresh_transformed_cols(self):\n",
    "        self.transformed_cols = [new_col for col, method, new_col in self.transformation_list]\n",
    "        if len(self.transformed_cols) > len(set(self.transformed_cols)):\n",
    "            print('Warning: there are duplicates in the transformation list')\n",
    "    \n",
    "    def update_transformation_methods(self):\n",
    "        self.transformation_methods = set([method for col, method, new_col in self.transformation_list])\n",
    "        \n",
    "    def look_for_feature_transformation(self,feature):\n",
    "        i = 0\n",
    "        result = []\n",
    "        for col, method, new_col in self.transformation_list:   \n",
    "            if col == feature:\n",
    "                print(f'-Column {col} is transformed in {new_col} trough {method} at index {i}')\n",
    "                result.append((method, new_col, i))\n",
    "            i += 1\n",
    "        print(f'-Column {feature} not found in transformation list')\n",
    "        return result\n",
    "    \n",
    "    def look_for_new_col(self,feature):\n",
    "        i = 0\n",
    "        for col, method, new_col in self.transformation_list:\n",
    "            if col == feature:\n",
    "                print(f'-Column {new_col} is made from {method} at index {i}')\n",
    "                return new_col\n",
    "            i += 1\n",
    "        print(f'-Column {feature} not found in transformation list')\n",
    "        return None\n",
    "    \n",
    "    def append_transformation(self,feature,method,new_col):\n",
    "        self.transformation_list.append((feature, method, new_col))\n",
    "\n",
    "\n",
    "    def transformation_pipeline(self,df):\n",
    "        onehot_list = []\n",
    "        for col, method, new_col in self.transformation_list:\n",
    "            if method == 'log':\n",
    "                df = self.transformation_log(df, col, new_col)\n",
    "            elif method == 'subtract_1900':\n",
    "                df = self.transformation_subtract_1900(df, col, new_col)\n",
    "            elif method == 'dummy-YN':\n",
    "                df = self.transformation_dummy_yn(df, col, new_col)\n",
    "            elif method == 'freq_encode':\n",
    "                df = self.transformation_freq_encode(df, col, new_col)\n",
    "            elif method == 'oneHot':\n",
    "                if len(df[col].unique()) < self.max_col_onehot:\n",
    "                    onehot_list.append(col)\n",
    "                else:\n",
    "                    print(f'Column {col} has too many unique values for oneHot encoding: {len(df[col].unique())}')\n",
    "                    print(f'ony {self.max_col_onehot} allowed, change the parameter if needed')\n",
    "                #df = self.transformation_oneHot(df, col, new_col)\n",
    "            elif method == '.str[5:7]':\n",
    "                df = self.transformation_str57(df, col, new_col)\n",
    "            elif method == '.str[0:4]':\n",
    "                df = self.transformation_str04(df, col, new_col)\n",
    "            elif method == '.str[0:5]':\n",
    "                df = self.transformation_str05(df, col, new_col)\n",
    "            elif method == 'remove':\n",
    "                df = self.transformation_remove(df, col)\n",
    "            elif method == 'none':\n",
    "                pass\n",
    "            else:\n",
    "                print(f'Unknown method {method} for column {col}')\n",
    "                \n",
    "        if len(onehot_list) > 0:\n",
    "            self.encoder.fit(df[onehot_list])\n",
    "            X = self.encoder.transform(df[onehot_list])\n",
    "            X = pd.DataFrame(X.toarray(), columns=self.encoder.get_feature_names_out(onehot_list), index = df.index)\n",
    "            df = pd.concat([df, X], axis=1)\n",
    "        return df     \n",
    "\n",
    "\n",
    "    \n",
    "    def set_transformations(self,df)->None:\n",
    "        df_cols = df.columns\n",
    "        self.refresh_transformed_cols()\n",
    "        columns_to_be_transformed = set(df_cols) - set(self.transformed_cols)\n",
    "    \n",
    "        date_cols = []\n",
    "        date_cols.extend([x for x in columns_to_be_transformed if 'Date' in x])\n",
    "        for col in date_cols:\n",
    "            col_year = col.replace('Date', 'Year')\n",
    "            col_month = col.replace('Date', 'Month')\n",
    "            self.append_transformation(col, '.str[5:7]', col_month)\n",
    "            self.append_transformation(col, '.str[0:4]', col_year)\n",
    "            self.append_transformation(col, 'remove', '')\n",
    "        columns_to_be_transformed = columns_to_be_transformed - set(date_cols)\n",
    "            \n",
    "        code_cols = []\n",
    "        code_cols.extend([x for x in columns_to_be_transformed if 'Code' in x])\n",
    "        for col in code_cols:\n",
    "            if len(df[col].unique()) < 10:\n",
    "                self.append_transformation(col, 'oneHot', '-one')\n",
    "            self.append_transformation(col, 'freq_encode', 'fe_'+col)\n",
    "        \n",
    "                    \n",
    "            \n",
    "        \n",
    "        columns_to_be_transformed = columns_to_be_transformed - set(code_cols)\n",
    "    \n",
    "        if len(columns_to_be_transformed) > 0:\n",
    "            print ('Columns that are not transformed:')\n",
    "            for col in columns_to_be_transformed:\n",
    "                try:\n",
    "                    print (f'-{col}: {df[col].dtype}')\n",
    "                except Exception as e: \n",
    "                    print(e)\n",
    "                    print (f'-{col}: not found')\n",
    "                    \n",
    "                    \n",
    "    def transformation_log(self, df, col, new_col):\n",
    "        try:\n",
    "            df[col].fillna(0, inplace=True)\n",
    "            df[new_col] = np.log(df[col] + 1)\n",
    "            self.append_scaling(new_col, 1)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'Column {col} not found')\n",
    "        return df\n",
    "\n",
    "\n",
    "    def transformation_subtract_1900(self, df, col, new_col):\n",
    "        try:\n",
    "            df[new_col] = df[col] - 1900\n",
    "            self.append_scaling(new_col, 1)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'Column {col} not found')\n",
    "        return df\n",
    "\n",
    "\n",
    "    def transformation_dummy_yn(self, df, col, new_col):\n",
    "        try:\n",
    "            df[new_col] = df[col].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "            self.append_scaling(new_col, 0)    \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'Column {col} not found')\n",
    "        return df\n",
    "\n",
    "\n",
    "    def transformation_freq_encode(self, df, col, new_col):\n",
    "        try:\n",
    "            freq = df[col].value_counts(normalize=True)\n",
    "            df.loc[:, new_col] = df[col].map(freq)\n",
    "            self.append_scaling(new_col, 1)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'Column {col} not found')\n",
    "        return df\n",
    "\n",
    "\n",
    "    def transformation_encode(self, df, col, new_col):\n",
    "        try:\n",
    "            df[new_col], _ = df[col].factorize()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'Column {col} not found')\n",
    "        return df\n",
    "\n",
    "\n",
    "    def transformation_str57(self, df, col, new_col):\n",
    "        try:\n",
    "            df[new_col] = df[col].str[5:7].astype('Int64')\n",
    "            self.append_scaling(new_col, 0)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'Column {col} not found')\n",
    "        \n",
    "        return df\n",
    "\n",
    "\n",
    "    def transformation_str04(self, df, col, new_col):\n",
    "        try:\n",
    "            df[new_col] = df[col].str[0:4].astype('Int64')\n",
    "            self.append_scaling(new_col, 0)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'Column {col} not found')\n",
    "        return df\n",
    "    \n",
    "    def transformation_str05(self, df, col, new_col):\n",
    "        try:\n",
    "            df[new_col] = df[col].str[0:5]\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'Column {col} not found')\n",
    "        return df\n",
    "    def transformation_remove(self, df, col):\n",
    "        try:\n",
    "            df.drop(columns=[col], inplace=True)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'Column {col} not found')\n",
    "        return df\n",
    "    \n",
    "    def encoder_fit(pr, X):\n",
    "        pr.encoder.fit(X)\n",
    "        return pr\n",
    "\n",
    "    def encoder_transform(pr, X):\n",
    "        Xcolumns = X.columns\n",
    "        X = pr.encoder.transform(X)\n",
    "        X = pd.DataFrame(X.toarray(), columns=pr.encoder.get_feature_names_out(Xcolumns))\n",
    "        return X\n",
    "    \n",
    "    def transformation_oneHot(self, df, col, new_col):\n",
    "        try:\n",
    "            self.encoder.fit(df[[col]])\n",
    "            X = self.encoder.transform(df[[col]])\n",
    "            X = pd.DataFrame(X.toarray(), columns=self.encoder.get_feature_names_out([col]))\n",
    "            df = pd.concat([df, X], axis=1)\n",
    "            for new_column in X.columns:\n",
    "                self.append_scaling(new_column, 0)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'Column {col} not found')\n",
    "        return df\n",
    "    \n",
    "    def update_transformation_list(self,feature,method,new_col):\n",
    "        i = 0\n",
    "        for col, method, new_col in self.transformation_list:\n",
    "            if col == feature:\n",
    "                self.transformation_list[i] = (feature, method, new_col)\n",
    "                return\n",
    "            i += 1\n",
    "            \n",
    "    \n",
    "    def add_transformation(self,feature,method,new_col):\n",
    "        self.transformation_list.append((feature, method, new_col))\n",
    "        \n",
    "\n",
    "# --------------------------------- fill the missing values\n",
    "\n",
    "    def update_fillna_list(self,df):\n",
    "        self.fillna_list = [(col, 'median') for col in df.columns if df[col].dtype in ['Int64', 'int64', 'float64','Float64']]\n",
    "        self.fillna_list.extend([(col, 'mode') for col in df.columns if df[col].dtype in ['string', 'object']])\n",
    "        print(f'extended fillna_list: {self.fillna_list}')\n",
    "    \n",
    "    def fillna_pipeline(self,df):\n",
    "        print(f'nans in the beginning: {df.isna().sum().sum()}')\n",
    "        for col,method in self.fillna_list:\n",
    "            if method == 'median':\n",
    "                df = self.fillna_median(df, col)\n",
    "            elif method == 'mode':\n",
    "                df = self.fillna_mode(df, col)\n",
    "            elif method == 'mean':\n",
    "                df = self.fillna_mean(df, col)\n",
    "            elif method == 'zero':\n",
    "                df = self.fillna_zero(df, col)\n",
    "            else:\n",
    "                print(f'Unknown method {method} for column {col}')\n",
    "            \n",
    "            print(f'Column {col} is filled with {method} ->num nan: {df[col].isna().sum()}')\n",
    "\n",
    "        print(f'nans in the end: {df.isna().sum().sum()}')\n",
    "\n",
    "        return df\n",
    "\n",
    "    def fillna_median(self, df, col):\n",
    "        try:\n",
    "            med = df[col].median().astype('int64')\n",
    "            df[col].fillna(med, inplace=True)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'Column {col} not found')\n",
    "        return df\n",
    "    \n",
    "    def fillna_mode(self, df, col):\n",
    "        try:\n",
    "            mode = df[col].mode()[0]\n",
    "            df[col].fillna(mode, inplace=True)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'Column {col} not found')\n",
    "        return df\n",
    "\n",
    "    def fillna_mean(self, df, col):\n",
    "        try:\n",
    "            mean = df[col].mean().astype(df[col].dtype.name)\n",
    "            df[col].fillna(mean, inplace=True)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'Column {col} not found')\n",
    "        return df\n",
    "\n",
    "    def fillna_zero(self, df, col):\n",
    "        try:\n",
    "            df[col].fillna(0, inplace=True)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'Column {col} not found')\n",
    "        return df\n",
    "    \n",
    "    def look_for_fillna_method(self,feature):\n",
    "        i = 0\n",
    "        for col, method in self.fillna_list:\n",
    "            if col == feature:\n",
    "                print(f'-Column {col} is filled with {method} at index {i}')\n",
    "                return method\n",
    "            i += 1\n",
    "        print(f'-Column {feature} not found in fillna list')\n",
    "        return None\n",
    "    \n",
    "    def change_fillna_method(self,feature,method):\n",
    "        i = 0\n",
    "        for col, method in self.fillna_list:\n",
    "            if col == feature:\n",
    "                self.fillna_list[i] = (feature, method)\n",
    "                return\n",
    "            i += 1\n",
    "        print(f'-Column {feature} not found in fillna list')\n",
    "        self.add_fillna(feature, method)\n",
    "        \n",
    "    def add_fillna(self,feature,method):      \n",
    "        self.fillna_list.append((feature, method))\n",
    "# --------------------------------- scaling part\n",
    "    def append_scaling(self,feature,yn):\n",
    "        features = [col for col, yn in self.sclaing_list]\n",
    "        if feature in features:\n",
    "            print(f'Feature {feature} is already in scaling list')\n",
    "            return\n",
    "    \n",
    "    \n",
    "        if yn == 'Y'or yn == 'y' or yn == 'yes' or yn == 'Yes' or yn == 1:\n",
    "            self.sclaing_list.append((feature,1))\n",
    "        else:\n",
    "            self.sclaing_list.append((feature,0))\n",
    "\n",
    "    def update_scaling_list(self, feature, yn):\n",
    "        i = 0\n",
    "        for col, yn_ in self.sclaing_list:\n",
    "            if col == feature:\n",
    "                self.sclaing_list[i] = (feature, yn)\n",
    "                return i\n",
    "            i += 1\n",
    "        print(f'Feature {feature} not found in scaling list, adding it now')\n",
    "        self.append_scaling(feature, yn)\n",
    "        return i\n",
    "        \n",
    "        \n",
    "    \n",
    "    def refresh_scaling_list(self,df):\n",
    "        num_cols = [col for col in df.columns if df[col].dtype in ['Int64', 'int64', 'float64','Float64']]\n",
    "        #other_cols = [col for col in df.columns if df[col] not in num_cols]\n",
    "        cols_to_be_updated = set(df.columns) - set(self.sclaing_list)\n",
    "        \n",
    "        for col in cols_to_be_updated:\n",
    "            if col in num_cols:\n",
    "                self.append_scaling(col, 1)\n",
    "            else:\n",
    "                self.append_scaling(col, 0)\n",
    "                \n",
    "        \n",
    "    def scaling_fit (self,sd):\n",
    "        self.scaler.fit(sd)\n",
    "        return self\n",
    "    \n",
    "    def scaling_transform(self,sd):\n",
    "        sd = self.scaler.transform(sd)\n",
    "        sd = pd.DataFrame(sd, columns = sd.columns)\n",
    "        return sd\n",
    "\n",
    "    def scaling_pipeline(self,sd):\n",
    "        #self.refresh_scaling_list(sd)\n",
    "        scaling_cols = [col for col, yn in self.sclaing_list if yn == 1]\n",
    "        scaling_cols = [col for col in sd.columns if col in scaling_cols]\n",
    "        other_cols = [col for col in sd.columns if col not in scaling_cols]\n",
    "        if self.status == 'train':\n",
    "            print(f'scaling_cols: {scaling_cols}')\n",
    "            print(f'info: {sd[scaling_cols].info()}')\n",
    "            self.scaler.fit(sd[scaling_cols])       \n",
    "        X = self.scaler.transform(sd[scaling_cols])\n",
    "        X = pd.DataFrame(X, columns = scaling_cols, index = sd.index)\n",
    "        sd = pd.concat([sd[other_cols], X], axis=1)\n",
    "        return sd\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURENUMBER =  '7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURENUMBER = '7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# install libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data\n",
    "train_path = \"../Data/train_data.csv\"\n",
    "test_path = \"../Data/test_data.csv\"\n",
    "# Replace 'Column29' with the actual column name that has mixed types\n",
    "dtype = {'Column29': 'str'}\n",
    "\n",
    "train = pd.read_csv(train_path, index_col='Claim Identifier',\n",
    "                    dtype=dtype, low_memory=False)\n",
    "test = pd.read_csv(test_path, index_col='Claim Identifier',\n",
    "                   dtype=dtype, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data\n",
    "train_path = \"../Data/train_data.csv\"\n",
    "test_path = \"../Data/test_data.csv\"\n",
    "# Replace 'Column29' with the actual column name that has mixed types\n",
    "dtype = {'Column29': 'str'}\n",
    "\n",
    "train = pd.read_csv(train_path, index_col='Claim Identifier',\n",
    "                    dtype=dtype, low_memory=False)\n",
    "test = pd.read_csv(test_path, index_col='Claim Identifier',\n",
    "                   dtype=dtype, low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generate majority target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.dropna(subset=['Claim Injury Type'])\n",
    "y = train['Claim Injury Type']\n",
    "train = train.dropna(subset=['Claim Injury Type'])\n",
    "y = train['Claim Injury Type']\n",
    "y_str = y.str[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import one hot encoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "encoder.fit(y_str.values.reshape(-1,1))\n",
    "y_hot = encoder.transform(y_str.values.reshape(-1,1)).toarray()\n",
    "y_hot = pd.DataFrame(y_hot, columns=encoder.get_feature_names_out(['Claim Injury Type']), index = y_str.index)\n",
    "y_hot.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_str.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_int = y_str.astype(int)\n",
    "X = train.drop(columns=['Claim Injury Type'])\n",
    "# partition the data X, y and y_2bin\n",
    "X_train, X_valid, y_hot_train,y_hot_valid = train_test_split(\n",
    "    X, y_hot, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = PreProcessor1()\n",
    "print(\"------------updates---------------------\")\n",
    "pr.update_casted_cols()\n",
    "print(\"------------set_casting---------------------\")\n",
    "pr.set_castings(X_train)\n",
    "print(\"------------pipeline---------------------\")\n",
    "df_train = pr.cast_pipeline(X_train)\n",
    "print(\"df after cast_pipeline:\", df_train.shape)\n",
    "print(\"------------fillna---------------------\")\n",
    "pr.update_fillna_list(df_train)   \n",
    "df_train = pr.fillna_pipeline(df_train)\n",
    "print(\"df after fillna_pipeline:\", df_train.shape)\n",
    "print(\"------------transformation---------------------\")\n",
    "pr.set_transformations(df_train)\n",
    "df_train = pr.transformation_pipeline(df_train)\n",
    "print(\"df after transformation_pipeline:\", df_train.shape)\n",
    "df_train= pr.scaling_pipeline(df_train)\n",
    "df_train = df_train.drop(columns=['Agreement Reached', 'WCB Decision'])\n",
    "df_train.info() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr.update_status('valid')\n",
    "print(\"------------updates---------------------\")\n",
    "pr.update_casted_cols()\n",
    "print(\"------------set_casting---------------------\")\n",
    "pr.set_castings(X_valid)\n",
    "print(\"------------pipeline---------------------\")\n",
    "df_valid = pr.cast_pipeline(X_valid)\n",
    "print(\"df after cast_pipeline:\", df_valid.shape)\n",
    "print(\"------------fillna---------------------\")\n",
    "pr.update_fillna_list(df_valid)   \n",
    "df_valid = pr.fillna_pipeline(df_valid)\n",
    "print(\"df after fillna_pipeline:\", df_valid.shape)\n",
    "print(\"------------transformation---------------------\")\n",
    "pr.set_transformations(df_valid)\n",
    "df_valid = pr.transformation_pipeline(df_valid)\n",
    "print(\"df after transformation_pipeline:\", df_valid.shape)\n",
    "df_valid = pr.scaling_pipeline(df_valid)\n",
    "df_valid = df_valid.drop(columns=['Agreement Reached', 'WCB Decision'])\n",
    "\n",
    "df_valid.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"pr.update_status('test')\n",
    "print(\"------------updates---------------------\")\n",
    "pr.update_casted_cols()\n",
    "print(\"------------set_casting---------------------\")\n",
    "pr.set_castings(test)\n",
    "print(\"------------pipeline---------------------\")\n",
    "df_test = pr.cast_pipeline(test)\n",
    "print(\"df after cast_pipeline:\", df_test.shape)\n",
    "print(\"------------fillna---------------------\")\n",
    "pr.update_fillna_list(df_test)   \n",
    "df_test = pr.fillna_pipeline(df_test)\n",
    "print(\"df after fillna_pipeline:\", df_test.shape)\n",
    "print(\"------------transformation---------------------\")\n",
    "pr.set_transformations(df_test)\n",
    "df_test = pr.transformation_pipeline(df_test)\n",
    "print(\"df after transformation_pipeline:\", df_test.shape)\n",
    "df_test = pr.scaling_pipeline(df_test)\n",
    "#drop the columns 'Agreement Reached', 'WCB Decision'\n",
    "df_valid = df_valid.drop(columns=['Agreement Reached', 'WCB Decision'])\n",
    "df_test.info()\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# choosing the subse of y to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "# Get all column names of y_hot\n",
    "columns =list(set(y_hot.columns) -set(['Claim Injury Type_2']))\n",
    "\n",
    "\n",
    "# Generate all possible permutations of y_hot\n",
    "permutations = []\n",
    "for r in range(1, 4):\n",
    "    for combo in itertools.combinations(columns, r):\n",
    "        permutations.append(combo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(y):\n",
    "    p = y.value_counts(normalize=True)\n",
    "    return 1 - np.sum(p**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_n = df_train.select_dtypes(include=['int64', 'float64'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to choose the best partition we can use entropy or gini index\n",
    "- now I'm doing it with entropy, at some point I'll do it with gini index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each permutation, sum the columns\n",
    "tot_entropy= []\n",
    "tot_entropy_1 = []\n",
    "tot_entropy_0 = []\n",
    "len_tot = len(df_train_n)\n",
    "columns_list = df_train_n.columns\n",
    "names_list = []\n",
    "for perm in permutations:\n",
    "    print(perm)\n",
    "    name = ''\n",
    "    for element in perm:\n",
    "        name = name + element[-1] + '_'\n",
    "    names_list.append(name[:-1])\n",
    "\n",
    "    y_hot_train_perm = y_hot_train[list(perm)].sum(axis=1)\n",
    "    df_perm_1 = df_train_n[y_hot_train_perm == 1]\n",
    "    df_perm_0 = df_train_n[y_hot_train_perm == 0]\n",
    "    tot_entropy_permutation = []\n",
    "    tot_entropy_permutation_1 = []\n",
    "    tot_entropy_permutation_0 = []\n",
    "    len_1 = len(df_perm_1)\n",
    "    len_0 = len_tot- len_1\n",
    "    for col in columns_list:\n",
    "        entropy_1 = entropy(df_perm_1[col])\n",
    "        entropy_0 = entropy(df_perm_0[col])\n",
    "        #tot_entropy_permutation.append(entropy_1*(len_1/(len_1+len_0)) + entropy_0*(len_1/(len_1+len_0)))\n",
    "        tot_entropy_permutation_1.append(entropy_1)\n",
    "        tot_entropy_permutation_0.append(entropy_0)\n",
    "        tot_entropy_permutation.append(entropy_1 - entropy_0)\n",
    "    #print(f\"tot_entropy_permutation.shape: {len(tot_entropy_permutation)}\")\n",
    "    #print(f\"average: {np.array(tot_entropy_permutation).mean()}  max: {np.array(tot_entropy_permutation).max()}  min: {np.array(tot_entropy_permutation).min()}\")\n",
    "    tot_entropy_permutation = np.array(tot_entropy_permutation)\n",
    "    tot_entropy_permutation_1 = np.array(tot_entropy_permutation_1)\n",
    "    tot_entropy_permutation_0 = np.array(tot_entropy_permutation_0)\n",
    "\n",
    "    \n",
    "    tot_entropy.append(tot_entropy_permutation.sum())\n",
    "    tot_entropy_1.append(tot_entropy_permutation_1.sum())\n",
    "    tot_entropy_0.append(tot_entropy_permutation_0.sum())\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import comb\n",
    "\n",
    "diving_lines = []\n",
    "div = 0\n",
    "for i in range(1, 4):\n",
    "    div1 = comb(7, i) + div\n",
    "    diving_lines.append(div1)\n",
    "    max_partion = np.argmax(tot_entropy[div:div1]) + div\n",
    "    print(f'Best partition for {i} elements: {permutations[max_partion]}')\n",
    "    div = div1\n",
    "    \n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.bar(names_list, tot_entropy)\n",
    "plt.xlabel('Permutations')\n",
    "#rotate x labels\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('Entropy')\n",
    "plt.title('Entropy of the different permutations')\n",
    "    \n",
    "for line in diving_lines:\n",
    "    plt.axvline(x=line, color='r', linestyle='--')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make 3 suplots\n",
    "fig, axs = plt.subplots(3, 1, figsize=(20, 20))\n",
    "fig.suptitle('Entropy of the different permutations')\n",
    "div= 0\n",
    "for i in range(3):\n",
    "    div1 = diving_lines[i]\n",
    "    axs[i].bar(names_list[div:div1], tot_entropy[div:div1])\n",
    "    max_partion = np.argmax(tot_entropy[div:div1])\n",
    "    print(f'Best partition for {i+1} elements: {max_partion}')\n",
    "    axs[i].axvline(x=max_partion, color='g', linestyle='--')\n",
    "    axs[i].set_title(f'{i+1} elements')\n",
    "    axs[i].set_ylabel('Entropy')\n",
    "    axs[i].set_xlabel('Permutations')\n",
    "    axs[i].set_xticklabels(names_list[div:div1], rotation=45)\n",
    "   \n",
    "    div = div1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "df_s0 = pd.DataFrame(tot_entropy_0, names_list)\n",
    "df_s1 = pd.DataFrame(tot_entropy_1, names_list)\n",
    "df_s01 = pd.concat([df_s0, df_s1], axis=1)\n",
    "\n",
    "df_s01.columns = ['entropy_0', 'entropy_1']\n",
    "\n",
    "df_s01.plot(kind = 'bar', figsize = (15, 10), title = 'Entropy of the two classes')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "y_hot_train['Claim Injury Type_3_5_4'] = y_hot_train['Claim Injury Type_3'] + y_hot_train['Claim Injury Type_5'] + y_hot_train['Claim Injury Type_4']\n",
    "y_hot_valid['Claim Injury Type_3_5_4'] = y_hot_valid['Claim Injury Type_3'] + y_hot_valid['Claim Injury Type_5'] + y_hot_valid['Claim Injury Type_4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation with target\n",
    "numcols = df_train.select_dtypes(include=['Int64', 'float64']).columns\n",
    "corr = df_train[numcols].corrwith(y_hot_train[f'Claim Injury Type_{FEATURENUMBER}'])\n",
    "corr = corr.sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.barplot(x=corr, y=corr.index)\n",
    "plt.title(f'Correlation with Claim Injury Type_{FEATURENUMBER}')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(feature):\n",
    "    return -np.sum([p*np.log2(p) for p in feature.value_counts(normalize=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_col_entropy_corr(f1, f2, n_iter=30, n_frac=0.5):\n",
    "    baseline_s_x1 = entropy(f1)\n",
    "    baseline_s_x2 = entropy(f2)\n",
    "\n",
    "    s_x1 = []\n",
    "    s_x2 = []\n",
    "    for i in range(n_iter):\n",
    "        x1 = f1.sample(frac=0.9, random_state=i)\n",
    "        x2 = f2.sample(frac=0.9, random_state=i)\n",
    "        s_x1.append(baseline_s_x1 - entropy(x1)/baseline_s_x1)\n",
    "        s_x2.append(baseline_s_x2 - entropy(x2)/baseline_s_x2)\n",
    "    return np.corrcoef(s_x1, s_x2)[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import log\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_corr = []\n",
    "for col in numcols:\n",
    "    s_col = two_col_entropy_corr(df_train[col], y_hot_train[f'Claim Injury Type_{FEATURENUMBER}'])\n",
    "    s_col = log((s_col +1)/2)\n",
    "    s_corr.append(s_col)\n",
    "    \n",
    "s_corr = pd.Series(s_corr, index=numcols).sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.barplot(x=s_corr, y=numcols)\n",
    "plt.title(f'Correlation with Claim Injury Type_{FEATURENUMBER}')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the columns with low correlation\n",
    "selected_cols = s_corr[s_corr > -0.8].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correation between the selected columns\n",
    "corr = df_train[selected_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(corr, annot=False, cmap='coolwarm')\n",
    "plt.title('Correlation between selected columns')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the columns with high correlation\n",
    "corr_matrix = corr.abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cols = [col for col in selected_cols if col not in to_drop]\n",
    "len(selected_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_corr(df, n_iter=30, n_frac=0.5):\n",
    "\n",
    "    entropy_matrix = []\n",
    "    for col in df.columns:\n",
    "        s = entropy(df[col])\n",
    "        delta_entropy = []\n",
    "        for i in range(n_iter):\n",
    "            x = df[col].sample(frac=n_frac, random_state=i)\n",
    "            delta_entropy.append(s - entropy(x) / s)\n",
    "        entropy_matrix.append(delta_entropy)\n",
    "    entropy_matrix = pd.DataFrame(entropy_matrix, index=df.columns)\n",
    "    entropy_matrix = entropy_matrix.T.corr()\n",
    "    return entropy_matrix.applymap(lambda x: log((x + 1) / 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_corr_matrix = entropy_corr(df_train[selected_cols])\n",
    "sns.heatmap(entropy_corr_matrix, annot=False, cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper = entropy_corr_matrix.where(np.triu(np.ones(entropy_corr_matrix.shape), k=1).astype(np.bool))\n",
    "to_drop_2 = [column for column in upper.columns if any(upper[column] > -0.2)]\n",
    "to_drop_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cols_2 = [col for col in selected_cols if col not in to_drop_2]\n",
    "len(selected_cols_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_cols_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the classifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "model_full = []\n",
    "model_selected = []\n",
    "model_selected_2 = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### report_and_save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a string\n",
    "# Save the model\n",
    "import pickle\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "def report_and_save (model,model_name,y_pred, y, model_filepath = '../Models'\n",
    "                     , report_filepath = '../Reports', print_report = True): \n",
    "    \n",
    "    \n",
    "    model_parameters = model.get_params()\n",
    "    model_filename = model_name + '.sav'\n",
    "    full_model_filename_os = os.path.join(model_filepath, model_filename)\n",
    "    report_filename = model_name + '.txt'\n",
    "    full_report_filename_os = os.path.join(report_filepath, report_filename)\n",
    "\n",
    "    try:\n",
    "        with open(full_model_filename_os, 'wb') as file:\n",
    "            pickle.dump(model, file)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while saving the model: {e}\")\n",
    "    accuracy = accuracy_score(y, y_pred)\n",
    "    precision = precision_score(y, y_pred, average='weighted')\n",
    "    recall = recall_score(y, y_pred, average='weighted')\n",
    "    cm = confusion_matrix(y, y_pred, normalize='true')\n",
    "\n",
    "    # Save the string into a text file\n",
    "    try:\n",
    "        with open(full_report_filename_os, \"w\") as file:\n",
    "            file.write(model_name)\n",
    "            file.write(\"\\n________________________\\n\")\n",
    "            file.write('model_parameters: \\n')\n",
    "            file.write(str(model_parameters).replace(\",\", \"\\n\"))   \n",
    "            file.write(\"\\n________________________\\n\")\n",
    "            file.write('Accuracy: ')\n",
    "            file.write(str(accuracy))\n",
    "            file.write(\"\\n________________________\\n\")\n",
    "            file.write('Precision: ')\n",
    "            file.write(str(precision))\n",
    "            file.write(\"\\n________________________\\n\")\n",
    "            file.write('Recall: ')\n",
    "            file.write(str(recall))\n",
    "            file.write(\"\\n________________________\\n\")\n",
    "            file.write('Confusion Matrix:\\n')\n",
    "            file.write(str(cm))\n",
    "            file.write(\"\\n________________________\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while writing the report: {e}\")\n",
    "    \n",
    "    if print_report:\n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "        print(f\"Precision: {precision}\")\n",
    "        print(f\"Recall: {recall}\")\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        sns.heatmap(cm, annot=True, cmap='coolwarm')\n",
    "        plt.title(f'Confusion Matrix {model_name}')   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# create a decision tree classifier\n",
    "DecisionTreeClassifier_1 = DecisionTreeClassifier(random_state=0)\n",
    "DecisionTreeClassifier_selected = DecisionTreeClassifier(random_state=0)\n",
    "DecisionTreeClassifier_selected_2 = DecisionTreeClassifier(random_state=0)\n",
    "num_cols = [col for col in df_train.columns if df_train[col].dtype in ['Int64', 'int64', 'float64','Float64']]\n",
    "\n",
    "# train the classifier\n",
    "DecisionTreeClassifier_1.fit(df_train[num_cols], y_hot_train[f'Claim Injury Type_{FEATURENUMBER}'])\n",
    "DecisionTreeClassifier_selected.fit(df_train[selected_cols], y_hot_train[f'Claim Injury Type_{FEATURENUMBER}'])\n",
    "DecisionTreeClassifier_selected_2.fit(df_train[selected_cols_2], y_hot_train[f'Claim Injury Type_{FEATURENUMBER}'])\n",
    "\n",
    "y_pred_full = DecisionTreeClassifier_1.predict(df_valid[num_cols])\n",
    "y_pred_selected = DecisionTreeClassifier_selected.predict(df_valid[selected_cols])\n",
    "y_pred_selected_2 = DecisionTreeClassifier_selected_2.predict(df_valid[selected_cols_2])\n",
    "\n",
    "accuracy_full  = accuracy_score(y_hot_valid[f'Claim Injury Type_{FEATURENUMBER}'], y_pred_full)\n",
    "accuracy_selected = accuracy_score(y_hot_valid[f'Claim Injury Type_{FEATURENUMBER}'], y_pred_selected)\n",
    "accuracy_selected_2 = accuracy_score(y_hot_valid[f'Claim Injury Type_{FEATURENUMBER}'], y_pred_selected_2)\n",
    "\n",
    "f1_full = f1_score(y_hot_valid[f'Claim Injury Type_{FEATURENUMBER}'], y_pred_full, average='macro')\n",
    "f1_selected = f1_score(y_hot_valid[f'Claim Injury Type_{FEATURENUMBER}'], y_pred_selected, average='macro')\n",
    "f1_selected_2 = f1_score(y_hot_valid[f'Claim Injury Type_{FEATURENUMBER}'], y_pred_selected_2, average='macro')\n",
    "\n",
    "model_full.append((\"DecisionTreeClassifier\",accuracy_full, f1_full))\n",
    "model_selected.append((\"DecisionTreeClassifier\",accuracy_selected, f1_selected))\n",
    "model_selected_2.append((\"DecisionTreeClassifier\",accuracy_selected_2, f1_selected_2))\n",
    "\n",
    "\n",
    "report_and_save(DecisionTreeClassifier_1, f'{FEATURENUMBER}_DecisionTreeClassifier_1', y_pred_full, y_hot_valid[f'Claim Injury Type_{FEATURENUMBER}'])\n",
    "report_and_save(DecisionTreeClassifier_selected, f'{FEATURENUMBER}_DecisionTreeClassifier_selected', y_pred_selected, y_hot_valid[f'Claim Injury Type_{FEATURENUMBER}'])\n",
    "report_and_save(DecisionTreeClassifier_selected_2, f'{FEATURENUMBER}_DecisionTreeClassifier_selected_2', y_pred_selected_2, y_hot_valid[f'Claim Injury Type_{FEATURENUMBER}'])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# create a logistic regression classifier\n",
    "LogisticRegression_1 = LogisticRegression(random_state=0)\n",
    "LogisticRegression_selected = LogisticRegression(random_state=0)\n",
    "LogisticRegression_selected_2 = LogisticRegression(random_state=0)\n",
    "\n",
    "# train the classifier\n",
    "LogisticRegression_1.fit(df_train[num_cols], y_hot_train[f'Claim Injury Type_{FEATURENUMBER}'])\n",
    "LogisticRegression_selected.fit(df_train[selected_cols], y_hot_train[f'Claim Injury Type_{FEATURENUMBER}'])\n",
    "LogisticRegression_selected_2.fit(df_train[selected_cols_2], y_hot_train[f'Claim Injury Type_{FEATURENUMBER}'])\n",
    "\n",
    "accuracy_full  = accuracy_score(y_hot_valid[f'Claim Injury Type_{FEATURENUMBER}'], LogisticRegression_1.predict(df_valid[num_cols]))\n",
    "accuracy_selected = accuracy_score(y_hot_valid[f'Claim Injury Type_{FEATURENUMBER}'], LogisticRegression_selected.predict(df_valid[selected_cols]))\n",
    "accuracy_selected_2 = accuracy_score(y_hot_valid[f'Claim Injury Type_{FEATURENUMBER}'], LogisticRegression_selected_2.predict(df_valid[selected_cols_2]))\n",
    "\n",
    "f1_full = f1_score(y_hot_valid[f'Claim Injury Type_{FEATURENUMBER}'], LogisticRegression_1.predict(df_valid[num_cols]), average='macro')\n",
    "f1_selected = f1_score(y_hot_valid[f'Claim Injury Type_{FEATURENUMBER}'], LogisticRegression_selected.predict(df_valid[selected_cols]), average='macro')\n",
    "f1_selected_2 = f1_score(y_hot_valid[f'Claim Injury Type_{FEATURENUMBER}'], LogisticRegression_selected_2.predict(df_valid[selected_cols_2]), average='macro')\n",
    "\n",
    "model_full.append((\"LogisticRegression\",accuracy_full, f1_full))\n",
    "model_selected.append((\"LogisticRegression\",accuracy_selected, f1_selected))\n",
    "model_selected_2.append((\"LogisticRegression\",accuracy_selected_2, f1_selected_2))\n",
    "\n",
    "report_and_save(LogisticRegression_1, f'{FEATURENUMBER}LogisticRegression_1', LogisticRegression_1.predict(df_valid[num_cols]), y_hot_valid[f'Claim Injury Type_{FEATURENUMBER}'])\n",
    "report_and_save(LogisticRegression_selected, f'{FEATURENUMBER}LogisticRegression_selected', LogisticRegression_selected.predict(df_valid[selected_cols]), y_hot_valid[f'Claim Injury Type_{FEATURENUMBER}'])\n",
    "report_and_save(LogisticRegression_selected_2, f'{FEATURENUMBER}LogisticRegression_selected_2', LogisticRegression_selected_2.predict(df_valid[selected_cols_2]), y_hot_valid[f'Claim Injury Type_{FEATURENUMBER}'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "\n",
    "# import random forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# create a random forest classifier\n",
    "RandomForestClassifier_1 = RandomForestClassifier(random_state=random_state)\n",
    "RandomForestClassifier_selected = RandomForestClassifier(random_state=random_state)\n",
    "RandomForestClassifier_selected_2 = RandomForestClassifier(random_state=random_state)\n",
    "\n",
    "# train the classifier\n",
    "RandomForestClassifier_1.fit(df_train[num_cols], y_hot_train[f'Claim Injury Type_{FEATURENUMBER}'])\n",
    "RandomForestClassifier_selected.fit(df_train[selected_cols], y_hot_train[f'Claim Injury Type_{FEATURENUMBER}'])\n",
    "RandomForestClassifier_selected_2.fit(df_train[selected_cols_2], y_hot_train[f'Claim Injury Type_{FEATURENUMBER}'])\n",
    "\n",
    "# evaluate the classifier\n",
    "y_pred_full = RandomForestClassifier_1.predict(df_valid[num_cols])\n",
    "y_pred_selected = RandomForestClassifier_selected.predict(df_valid[selected_cols])\n",
    "y_pred_selected_2 = RandomForestClassifier_selected_2.predict(df_valid[selected_cols_2])\n",
    "\n",
    "accuracy_full  = accuracy_score(y_hot_valid[f'Claim Injury Type_{FEATURENUMBER}'], y_pred_full)\n",
    "accuracy_selected = accuracy_score(y_hot_valid[f'Claim Injury Type_{FEATURENUMBER}'], y_pred_selected)\n",
    "accuracy_selected_2 = accuracy_score(y_hot_valid[f'Claim Injury Type_{FEATURENUMBER}'], y_pred_selected_2)\n",
    "\n",
    "f1_full = f1_score(y_hot_valid[f'Claim Injury Type_{FEATURENUMBER}'], y_pred_full, average='macro')\n",
    "f1_selected = f1_score(y_hot_valid[f'Claim Injury Type_{FEATURENUMBER}'], y_pred_selected, average='macro')\n",
    "f1_selected_2 = f1_score(y_hot_valid[f'Claim Injury Type_{FEATURENUMBER}'], y_pred_selected_2, average='macro')\n",
    "\n",
    "model_full.append((\"RandomForestClassifier\",accuracy_full, f1_full))\n",
    "model_selected.append((\"RandomForestClassifier\",accuracy_selected, f1_selected))\n",
    "model_selected_2.append((\"RandomForestClassifier\",accuracy_selected_2, f1_selected_2))\n",
    "\n",
    "report_and_save(RandomForestClassifier_1, f'{FEATURENUMBER}RandomForestClassifier_1', y_pred_full, y_hot_valid[f'Claim Injury Type_{FEATURENUMBER}'])\n",
    "report_and_save(RandomForestClassifier_selected, f'{FEATURENUMBER}RandomForestClassifier_selected', y_pred_selected, y_hot_valid[f'Claim Injury Type_{FEATURENUMBER}'])\n",
    "report_and_save(RandomForestClassifier_selected_2, f'{FEATURENUMBER}RandomForestClassifier_selected_2', y_pred_selected_2, y_hot_valid[f'Claim Injury Type_{FEATURENUMBER}'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import xgboost classifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# create a xgboost classifier\n",
    "XGBClassifier_1 = XGBClassifier(random_state=random_state)\n",
    "XGBClassifier_selected = XGBClassifier(random_state=random_state)\n",
    "XGBClassifier_selected_2 = XGBClassifier(random_state=random_state)\n",
    "\n",
    "# train the classifier\n",
    "XGBClassifier_1.fit(df_train[num_cols], y_hot_train[f'Claim Injury Type_{FEATURENUMBER}'])\n",
    "XGBClassifier_selected.fit(df_train[selected_cols], y_hot_train[f'Claim Injury Type_{FEATURENUMBER}'])\n",
    "XGBClassifier_selected_2.fit(df_train[selected_cols_2], y_hot_train[f'Claim Injury Type_{FEATURENUMBER}'])\n",
    "\n",
    "# evaluate the classifier\n",
    "y_pred_full = XGBClassifier_1.predict(df_valid[num_cols])\n",
    "y_pred_selected = XGBClassifier_selected.predict(df_valid[selected_cols])\n",
    "y_pred_selected_2 = XGBClassifier_selected_2.predict(df_valid[selected_cols_2])\n",
    "\n",
    "accuracy_full  = accuracy_score(y_hot_valid[f'Claim Injury Type_{FEATURENUMBER}'], y_pred_full)\n",
    "accuracy_selected = accuracy_score(y_hot_valid[f'Claim Injury Type_{FEATURENUMBER}'], y_pred_selected)\n",
    "accuracy_selected_2 = accuracy_score(y_hot_valid[f'Claim Injury Type_{FEATURENUMBER}'], y_pred_selected_2)\n",
    "f1_full = f1_score(y_hot_valid[f'Claim Injury Type_{FEATURENUMBER}'], y_pred_full, average='macro')\n",
    "f1_selected = f1_score(y_hot_valid[f'Claim Injury Type_{FEATURENUMBER}'], y_pred_selected, average='macro')\n",
    "f1_selected_2 = f1_score(y_hot_valid[f'Claim Injury Type_{FEATURENUMBER}'], y_pred_selected_2, average='macro')\n",
    "\n",
    "model_full.append((\"XGBClassifier\",accuracy_full, f1_full))\n",
    "model_selected.append((\"XGBClassifier\",accuracy_selected, f1_selected))\n",
    "model_selected_2.append((\"XGBClassifier\",accuracy_selected_2, f1_selected_2))\n",
    "\n",
    "report_and_save(XGBClassifier_1, f'{FEATURENUMBER}XGBClassifier_1', y_pred_full, y_hot_valid[f'Claim Injury Type_{FEATURENUMBER}'])\n",
    "report_and_save(XGBClassifier_selected, f'{FEATURENUMBER}XGBClassifier_selected', y_pred_selected, y_hot_valid[f'Claim Injury Type_{FEATURENUMBER}'])\n",
    "report_and_save(XGBClassifier_selected_2, f'{FEATURENUMBER}XGBClassifier_selected_2', y_pred_selected_2, y_hot_valid[f'Claim Injury Type_{FEATURENUMBER}'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import lightgbm classifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# create a lightgbm classifier\n",
    "LGBMClassifier_1 = LGBMClassifier(random_state=random_state)\n",
    "LGBMClassifier_selected = LGBMClassifier(random_state=random_state)\n",
    "LGBMClassifier_selected_2 = LGBMClassifier(random_state=random_state)\n",
    "\n",
    "# train the classifier\n",
    "LGBMClassifier_1.fit(df_train[num_cols], y_hot_train[f'Claim Injury Type_{FEATURENUMBER}'])\n",
    "LGBMClassifier_selected.fit(df_train[selected_cols], y_hot_train[f'Claim Injury Type_{FEATURENUMBER}'])\n",
    "LGBMClassifier_selected_2.fit(df_train[selected_cols_2], y_hot_train[f'Claim Injury Type_{FEATURENUMBER}'])\n",
    "\n",
    "# evaluate the classifier\n",
    "y_pred_full = LGBMClassifier_1.predict(df_valid[num_cols])\n",
    "y_pred_selected = LGBMClassifier_selected.predict(df_valid[selected_cols])\n",
    "y_pred_selected_2 = LGBMClassifier_selected_2.predict(df_valid[selected_cols_2])\n",
    "\n",
    "accuracy_full  = accuracy_score(y_hot_valid[f'Claim Injury Type_{FEATURENUMBER}'], y_pred_full)\n",
    "accuracy_selected = accuracy_score(y_hot_valid[f'Claim Injury Type_{FEATURENUMBER}'], y_pred_selected)\n",
    "accuracy_selected_2 = accuracy_score(y_hot_valid[f'Claim Injury Type_{FEATURENUMBER}'], y_pred_selected_2)\n",
    "f1_full = f1_score(y_hot_valid[f'Claim Injury Type_{FEATURENUMBER}'], y_pred_full, average='macro')\n",
    "f1_selected = f1_score(y_hot_valid[f'Claim Injury Type_{FEATURENUMBER}'], y_pred_selected, average='macro')\n",
    "f1_selected_2 = f1_score(y_hot_valid[f'Claim Injury Type_{FEATURENUMBER}'], y_pred_selected_2, average='macro')\n",
    "\n",
    "model_full.append((\"LGBMClassifier\",accuracy_full, f1_full))\n",
    "model_selected.append((\"LGBMClassifier\",accuracy_selected, f1_selected))\n",
    "model_selected_2.append((\"LGBMClassifier\",accuracy_selected_2, f1_selected_2))\n",
    "\n",
    "report_and_save(LGBMClassifier_1, f'{FEATURENUMBER}LGBMClassifier_1', y_pred_full, y_hot_valid[f'Claim Injury Type_{FEATURENUMBER}'])\n",
    "report_and_save(LGBMClassifier_selected, f'{FEATURENUMBER}LGBMClassifier_selected', y_pred_selected, y_hot_valid[f'Claim Injury Type_{FEATURENUMBER}'])\n",
    "report_and_save(LGBMClassifier_selected_2, f'{FEATURENUMBER}LGBMClassifier_selected_2', y_pred_selected_2, y_hot_valid[f'Claim Injury Type_{FEATURENUMBER}'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, accuracy, f1 in model_full:\n",
    "    print(f'{name}: accuracy={accuracy}, f1={f1}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multillayer perceptron\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# create a multilayer perceptron classifier\n",
    "MLPClassifier_1 = MLPClassifier(random_state=random_state)\n",
    "MLPClassifier_selected = MLPClassifier(random_state=random_state)\n",
    "MLPClassifier_selected_2 = MLPClassifier(random_state=random_state)\n",
    "\n",
    "# train the classifier\n",
    "MLPClassifier_1.fit(df_train[num_cols], y_hot_train[f'Claim Injury Type_{FEATURENUMBER}'])\n",
    "MLPClassifier_selected.fit(df_train[selected_cols], y_hot_train[f'Claim Injury Type_{FEATURENUMBER}'])\n",
    "MLPClassifier_selected_2.fit(df_train[selected_cols_2], y_hot_train[f'Claim Injury Type_{FEATURENUMBER}'])\n",
    "\n",
    "# evaluate the classifier\n",
    "y_pred_full = MLPClassifier_1.predict(df_valid[num_cols])\n",
    "y_pred_selected = MLPClassifier_selected.predict(df_valid[selected_cols])\n",
    "y_pred_selected_2 = MLPClassifier_selected_2.predict(df_valid[selected_cols_2])\n",
    "\n",
    "accuracy_full  = accuracy_score(y_hot_valid[f'Claim Injury Type_{FEATURENUMBER}'], y_pred_full)\n",
    "accuracy_selected = accuracy_score(y_hot_valid[f'Claim Injury Type_{FEATURENUMBER}'], y_pred_selected)\n",
    "accuracy_selected_2 = accuracy_score(y_hot_valid[f'Claim Injury Type_{FEATURENUMBER}'], y_pred_selected_2)\n",
    "\n",
    "f1_full = f1_score(y_hot_valid[f'Claim Injury Type_{FEATURENUMBER}'], y_pred_full, average='macro')\n",
    "f1_selected = f1_score(y_hot_valid[f'Claim Injury Type_{FEATURENUMBER}'], y_pred_selected, average='macro')\n",
    "f1_selected_2 = f1_score(y_hot_valid[f'Claim Injury Type_{FEATURENUMBER}'], y_pred_selected_2, average='macro')\n",
    "\n",
    "model_full.append((\"MLPClassifier\",accuracy_full, f1_full))\n",
    "model_selected.append((\"MLPClassifier\",accuracy_selected, f1_selected))\n",
    "model_selected_2.append((\"MLPClassifier\",accuracy_selected_2, f1_selected_2))\n",
    "\n",
    "report_and_save(MLPClassifier_1, f'{FEATURENUMBER}MLPClassifier_1', y_pred_full, y_hot_valid[f'Claim Injury Type_{FEATURENUMBER}'])\n",
    "report_and_save(MLPClassifier_selected, f'{FEATURENUMBER}MLPClassifier_selected', y_pred_selected, y_hot_valid[f'Claim Injury Type_{FEATURENUMBER}'])\n",
    "report_and_save(MLPClassifier_selected_2, f'{FEATURENUMBER}MLPClassifier_selected_2', y_pred_selected_2, y_hot_valid[f'Claim Injury Type_{FEATURENUMBER}'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# svm classifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# create a svm classifier\n",
    "SVC_1 = SVC(random_state=random_state)\n",
    "SVC_selected = SVC(random_state=random_state)\n",
    "SVC_selected_2 = SVC(random_state=random_state)\n",
    "\n",
    "# train the classifier\n",
    "SVC_1.fit(df_train[num_cols], y_hot_train[f'Claim Injury Type_{FEATURENUMBER}'])\n",
    "SVC_selected.fit(df_train[selected_cols], y_hot_train[f'Claim Injury Type_{FEATURENUMBER}'])\n",
    "SVC_selected_2.fit(df_train[selected_cols_2], y_hot_train[f'Claim Injury Type_{FEATURENUMBER}'])\n",
    "\n",
    "# evaluate the classifier\n",
    "y_pred_full = SVC_1.predict(df_valid[num_cols])\n",
    "y_pred_selected = SVC_selected.predict(df_valid[selected_cols])\n",
    "y_pred_selected_2 = SVC_selected_2.predict(df_valid[selected_cols_2])\n",
    "\n",
    "accuracy_full  = accuracy_score(y_hot_valid[f'Claim Injury Type_{FEATURENUMBER}'], y_pred_full)\n",
    "accuracy_selected = accuracy_score(y_hot_valid[f'Claim Injury Type_{FEATURENUMBER}'], y_pred_selected)\n",
    "accuracy_selected_2 = accuracy_score(y_hot_valid[f'Claim Injury Type_{FEATURENUMBER}'], y_pred_selected_2)\n",
    "\n",
    "f1_full = f1_score(y_hot_valid[f'Claim Injury Type_{FEATURENUMBER}'], y_pred_full, average='macro')\n",
    "f1_selected = f1_score(y_hot_valid[f'Claim Injury Type_{FEATURENUMBER}'], y_pred_selected, average='macro')\n",
    "f1_selected_2 = f1_score(y_hot_valid[f'Claim Injury Type_{FEATURENUMBER}'], y_pred_selected_2, average='macro')\n",
    "\n",
    "model_full.append((\"SVC\",accuracy_full, f1_full))\n",
    "model_selected.append((\"SVC\",accuracy_selected, f1_selected))\n",
    "model_selected_2.append((\"SVC\",accuracy_selected_2, f1_selected_2))\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## voting classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the voting classifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# create a voting classifier\n",
    "VotingClassifier_1 = VotingClassifier(estimators=[('DecisionTreeClassifier', DecisionTreeClassifier_1),\n",
    "                                                  ('LogisticRegression', LogisticRegression_1),\n",
    "                                                  ('RandomForestClassifier', RandomForestClassifier_1),\n",
    "                                                  ('XGBClassifier', XGBClassifier_1),\n",
    "                                                  ('LGBMClassifier', LGBMClassifier_1),\n",
    "                                                  ('MLPClassifier', MLPClassifier_1)], voting='soft')\n",
    "                                                  #('SVC', SVC_1)], voting='soft')\n",
    "\n",
    "\n",
    "VotingClassifier_selected = VotingClassifier(estimators=[('DecisionTreeClassifier', DecisionTreeClassifier_selected),\n",
    "                                                         ('LogisticRegression', LogisticRegression_selected),\n",
    "                                                         ('RandomForestClassifier', RandomForestClassifier_selected),\n",
    "                                                         ('XGBClassifier', XGBClassifier_selected),\n",
    "                                                         ('LGBMClassifier', LGBMClassifier_selected),\n",
    "                                                         ('MLPClassifier', MLPClassifier_selected)], voting='soft')\n",
    "                                                         #('SVC', SVC_selected)], voting='soft')\n",
    "\n",
    "VotingClassifier_selected_2 = VotingClassifier(estimators=[('DecisionTreeClassifier', DecisionTreeClassifier_selected_2),\n",
    "                                                           ('LogisticRegression', LogisticRegression_selected_2),\n",
    "                                                           ('RandomForestClassifier', RandomForestClassifier_selected_2),\n",
    "                                                           ('XGBClassifier', XGBClassifier_selected_2),\n",
    "                                                           ('LGBMClassifier', LGBMClassifier_selected_2),\n",
    "                                                           ('MLPClassifier', MLPClassifier_selected_2)], voting='soft')\n",
    "                                                           #('SVC', SVC_selected_2)], voting='soft')\n",
    "\n",
    "\n",
    "\n",
    "# train the classifier\n",
    "VotingClassifier_1.fit(df_train[num_cols], y_hot_train[f'Claim Injury Type_{FEATURENUMBER}'])\n",
    "VotingClassifier_selected.fit(df_train[selected_cols], y_hot_train[f'Claim Injury Type_{FEATURENUMBER}'])\n",
    "VotingClassifier_selected_2.fit(df_train[selected_cols_2], y_hot_train[f'Claim Injury Type_{FEATURENUMBER}'])\n",
    "\n",
    "# evaluate the classifier\n",
    "y_pred_full = VotingClassifier_1.predict(df_valid[num_cols])\n",
    "y_pred_selected = VotingClassifier_selected.predict(df_valid[selected_cols])\n",
    "y_pred_selected_2 = VotingClassifier_selected_2.predict(df_valid[selected_cols_2])\n",
    "\n",
    "accuracy_full  = accuracy_score(y_hot_valid[f'Claim Injury Type_{FEATURENUMBER}'], y_pred_full)\n",
    "accuracy_selected = accuracy_score(y_hot_valid[f'Claim Injury Type_{FEATURENUMBER}'], y_pred_selected)\n",
    "accuracy_selected_2 = accuracy_score(y_hot_valid[f'Claim Injury Type_{FEATURENUMBER}'], y_pred_selected_2)\n",
    "\n",
    "f1_full = f1_score(y_hot_valid[f'Claim Injury Type_{FEATURENUMBER}'], y_pred_full, average='macro')\n",
    "f1_selected = f1_score(y_hot_valid[f'Claim Injury Type_{FEATURENUMBER}'], y_pred_selected, average='macro')\n",
    "f1_selected_2 = f1_score(y_hot_valid[f'Claim Injury Type_{FEATURENUMBER}'], y_pred_selected_2, average='macro')\n",
    "\n",
    "model_full.append((\"VotingClassifier\",accuracy_full, f1_full))\n",
    "model_selected.append((\"VotingClassifier\",accuracy_selected, f1_selected))\n",
    "model_selected_2.append((\"VotingClassifier\",accuracy_selected_2, f1_selected_2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_selected_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the results\n",
    "model_full = pd.DataFrame(model_full, columns=['Model', 'Accuracy', 'F1'])\n",
    "model_selected = pd.DataFrame(model_selected, columns=['Model', 'Accuracy', 'F1'])\n",
    "model_selected_2 = pd.DataFrame(model_selected_2, columns=['Model', 'Accuracy', 'F1'])\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.barplot(x='Model', y='Accuracy', data=model_full)\n",
    "plt.title('Accuracy of the models')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "sns.barplot(x='Model', y='F1', data=model_full)\n",
    "plt.title('Accuracy of the models')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Combine the dataframes for full and selected models\n",
    "model_full['Type'] = 'Full'\n",
    "model_selected['Type'] = 'Selected'\n",
    "model_selected_2['Type'] = 'Selected_2' \n",
    "combined_df = pd.concat([model_full, model_selected])\n",
    "combined_df = pd.concat([combined_df, model_selected_2])\n",
    "\n",
    "# Plot the accuracy\n",
    "plt.figure(figsize=(14, 7))\n",
    "sns.barplot(x='Model', y='Accuracy', hue='Type', data=combined_df)\n",
    "plt.title('Accuracy of Full and Selected Models')\n",
    "# horizontal line at maximum accuracy\n",
    "plt.axhline(combined_df['Accuracy'].max(), color='red', linestyle='--')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Plot the F1 score\n",
    "plt.figure(figsize=(14, 7))\n",
    "sns.barplot(x='Model', y='F1', hue='Type', data=combined_df)\n",
    "plt.title('F1 Score of Full and Selected Models')\n",
    "plt.axhline(combined_df['F1'].max(), color='red', linestyle='--')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
