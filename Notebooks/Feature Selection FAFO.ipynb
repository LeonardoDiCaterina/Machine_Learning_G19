{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import robust scaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "#import one hot encoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "class PreProcessor1:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.status = 'train'\n",
    "        self.scaler = RobustScaler()\n",
    "        self.version = '2.0 11 dec 2024 late frango'\n",
    "        self.encoder = OneHotEncoder()\n",
    "        self.max_col_onehot = 5\n",
    "        \n",
    "        self.date_cols = []\n",
    "        self.desc_cols = []\n",
    "        self.code_cols = []\n",
    "        self.casting_methods = []\n",
    "        self.casting_list = [\n",
    "    ('Age at Injury',                       'Int64'),\n",
    "    ('Average Weekly Wage',                 'float64'),\n",
    "    # ('WCB Decision',                        'remove'),\n",
    "    ('Alternative Dispute Resolution',      'string-U-nan'),\n",
    "    ('District Name',                       'none'),\n",
    "    ('Carrier Name',                        'string'),\n",
    "    ('IME-4 Count',                         'Int64'),\n",
    "    ('COVID-19 Indicator',                  'string'),\n",
    "    ('Number of Dependents',                'Int64'),\n",
    "    ('Carrier Type',                         '.str[:2]'),\n",
    "    ('Medical Fee Region',                   'string'),\n",
    "    ('County of Injury',                    'string'),\n",
    "    ('Agreement Reached',                   'Int64'),\n",
    "    ('Attorney/Representative',             'string'),\n",
    "    ('Birth Year',                          'Int64'),\n",
    "    ('Gender',                              'string'),\n",
    "    ('Zip Code',                            '.str[0:5]'),\n",
    "]\n",
    "        self.casted_cols = []\n",
    "        self.transformation_list = [\n",
    "    ('Age at Injury',                   'none',             '-'),\n",
    "    ('Average Weekly Wage',             'log',              'log_Average Weekly Wage'),\n",
    "    # ('WCB Decision',                    'none',             '-'),\n",
    "    ('Alternative Dispute Resolution',  'dummy-YN',         'Alternative Dispute Resolution'),\n",
    "    ('District Name',                   'none',             '-'),\n",
    "    ('Carrier Name',                    'none',             '-'),\n",
    "    ('IME-4 Count',                     'none',             '-'),\n",
    "    ('COVID-19 Indicator',              'dummy-YN',         'COVID-19 Indicator'),\n",
    "    ('Number of Dependents',            'none',             '-'),\n",
    "    ('Carrier Type',                    'oneHot',           '-OneHot'),\n",
    "    ('Carrier Type',                    'freq_encode',      'fe_Carrier Type'),\n",
    "    ('Medical Fee Region',              'oneHot',           '-oneHot'),\n",
    "    ('Medical Fee Region',              'freq_encode',      'fe_Medical Fee Region'),\n",
    "    ('County of Injury',                'oneHot',           '-oneHot'),\n",
    "    ('County of Injury',                'freq_encode',      'fe_County of Injury'),\n",
    "    ('Agreement Reached',               'none',             '-oneHot'),\n",
    "    ('Attorney/Representative',         'dummy-YN',         'Attorney/Representative'),\n",
    "    ('Birth Year',                      'subtract_1900',    'Age'),\n",
    "    ('Gender',                          'oneHot',           '-oneHot'),\n",
    "]     \n",
    "        self.transformed_cols = []\n",
    "        self.fillna_list = []\n",
    "        \n",
    "        self.sclaing_list = [   ('Carrier Type',                     0),\n",
    "                                ('Zip Code',                         0),\n",
    "                                ('log_Average Weekly Wage',          1),\n",
    "                                ('Alternative Dispute Resolution',   0),\n",
    "                                ('COVID-19 Indicator',               0),\n",
    "                                ('fe_Carrier Type',                  1),\n",
    "                                ('fe_Medical Fee Region',            1),\n",
    "                                ('fe_County of Injury',              1),\n",
    "                                ('Attorney/Representative',          0),\n",
    "                                ('Age',                              1),\n",
    "                                ('Assembly Month',                   1),\n",
    "                                ('Assembly Year',                    1),\n",
    "                                ('C-3 Month',                        1),\n",
    "                                ('C-3 Year',                         1),\n",
    "                                ('Accident Month',                   1),\n",
    "                                ('Accident Year',                    1),\n",
    "                                ('C-2 Month',                        1),\n",
    "                                ('C-2 Year',                         1),\n",
    "                                ('First Hearing Month',              0),\n",
    "                                ('First Hearing Year',               0),\n",
    "                                ('fe_WCIO Part Of Body Code',        1),\n",
    "                                ('fe_Industry Code',                 1),\n",
    "                                ('fe_WCIO Nature of Injury Code',    1),\n",
    "                                ('fe_Zip Code',                      1),\n",
    "                                ('fe_WCIO Cause of Injury Code',     1),\n",
    "                                ('WCIO Part Of Body Code',           1),\n",
    "                                ('Industry Code',                    1),\n",
    "                                ('WCIO Nature of Injury Code',       1),\n",
    "                                ('Gender_U',                         0),\n",
    "                                ('Gender_X',                         0),\n",
    "                                ('IME-4 Count',                      1),\n",
    "                                ('Age at Injury',                    1),\n",
    "                                ('District Name',                    0),\n",
    "                                ('Average Weekly Wage',              1),\n",
    "                                ('Medical Fee Region',               0),\n",
    "                                ('Number of Dependents',             1),\n",
    "                                ('Carrier Name',                     0),\n",
    "                                ('Gender_F',                         0),\n",
    "                                ('Agreement Reached',                0),\n",
    "                                ('Gender',                           0),\n",
    "                                ('Birth Year',                       1),\n",
    "                                ('WCB Decision',                     0),\n",
    "                                ('Gender_M',                         0),\n",
    "                                ('WCIO Cause of Injury Code',        1),\n",
    "                                ('County of Injury',                 0)]\n",
    "\n",
    "\n",
    "# ------------------------ internal functions\n",
    "    def update_status(self, status):\n",
    "        if status in ['train', 'valid', 'test']:\n",
    "            self.status = status\n",
    "        else:  \n",
    "            print('Unknown status')\n",
    "    \n",
    "    def __str__(self):\n",
    "        return (f\"PreProcessor1: {self.status}\" +\n",
    "            f\"scaler: {self.scaler}\" +\n",
    "                f\"version: {self.version}\")\n",
    "        \n",
    "        \n",
    "       \n",
    "        \n",
    "# ------------------------casting part\n",
    "        \n",
    "        \n",
    "    def update_casted_cols(self):     \n",
    "        self.casted_cols = [col for col, method in self.casting_list]\n",
    "\n",
    "    def update_casting_methods(self):\n",
    "        self.casting_methods = [method for col, method in self.casting_list]\n",
    "\n",
    "    def set_castings(self,df):\n",
    "        df_cols = df.columns\n",
    "        columns_to_be_casted = set(df_cols) - set(self.casted_cols)\n",
    "    \n",
    "        date_cols = []\n",
    "        date_cols.extend([x for x in df_cols if 'Date' in x])\n",
    "        for col in date_cols:\n",
    "            self.append_casting(col, 'string')\n",
    "        columns_to_be_casted = columns_to_be_casted - set(date_cols)\n",
    "    \n",
    "        desc_cols = []\n",
    "        desc_cols.extend([x for x in df_cols if 'Description' in x])\n",
    "        for col in desc_cols:\n",
    "            self.append_casting(col, 'remove')\n",
    "        columns_to_be_casted = columns_to_be_casted - set(desc_cols)\n",
    "    \n",
    "        code_cols = []\n",
    "        code_cols.extend([x for x in df_cols if 'Code' in x])\n",
    "        for col in code_cols:\n",
    "            self.append_casting(col, 'Int64')\n",
    "        columns_to_be_casted = columns_to_be_casted - set(code_cols)\n",
    "    \n",
    "        if len(columns_to_be_casted) > 0:\n",
    "            print ('Columns that are not casted:')\n",
    "            for col in columns_to_be_casted:\n",
    "                try:\n",
    "                    print (f'-{col}: {df[col].dtype}')\n",
    "                except Exception as e: \n",
    "                    print(e)\n",
    "                    print (f'-{col}: not found')\n",
    "\n",
    "    def look_for_feature_casting(self,feature):\n",
    "        i = 0\n",
    "        for col, method in self.casting_list:\n",
    "            if col == feature:\n",
    "                print(f'-Column {col} is casted as {method} at index {i}')\n",
    "                return method\n",
    "            i += 1\n",
    "        return None\n",
    "\n",
    "    def update_casting_list(self,feature,method):\n",
    "        i = 0\n",
    "        for col, method in self.casting_list:\n",
    "            if col == feature:\n",
    "                self.casting_list[i] = (feature, method)\n",
    "                return\n",
    "            i += 1\n",
    "        #print(f'-Column {feature} not found in casting list. Adding it now')\n",
    "        self.append_casting(feature, method)\n",
    "    \n",
    "    def append_casting(self,feature,method):\n",
    "        self.casting_list.append((feature, method))\n",
    "    \n",
    "        \n",
    "    def cast_pipeline(self,df):\n",
    "        for col, method in self.casting_list:\n",
    "            if method == 'Int64':\n",
    "                df = self.cast_Int64(df, col)\n",
    "            elif method == 'float64':\n",
    "                df = self.cast_Float64(df, col)\n",
    "            elif method == 'string':\n",
    "                df = self.cast_string(df, col)\n",
    "            elif method == 'string-U-nan':\n",
    "                df = self.cast_string_U_nan(df, col)\n",
    "            elif method == '.str[:2]':\n",
    "                df = self.cast_string_2(df, col)\n",
    "                self.append_scaling(col, 0)\n",
    "            elif method == 'datetime64':\n",
    "                df = self.cast_datetime64(df, col)\n",
    "                self.append_scaling(col, 0)\n",
    "            elif method == 'remove':\n",
    "                df = self.cast_remove(df, col)\n",
    "            elif method == '.str[0:5]':\n",
    "                df = self.cast_string_5(df, col)\n",
    "                self.append_scaling(col, 0)\n",
    "                \n",
    "            elif method == 'none':\n",
    "                pass\n",
    "            else:\n",
    "                print(f'Unknown method {method} for column {col}')\n",
    "        return df\n",
    "\n",
    "    def cast_Int64(self, df, col):\n",
    "        try:\n",
    "            df[col] = df[col].astype('Int64')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'-Column {col} not found')\n",
    "        return df\n",
    "\n",
    "\n",
    "    def cast_Float64(self, df, col):\n",
    "        try:\n",
    "            df[col] = df[col].astype('float64')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'-Column {col} not found')\n",
    "\n",
    "        return df\n",
    "\n",
    "    def cast_string(self, df, col):\n",
    "        try:\n",
    "            df[col] = df[col].astype('string')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'-Column {col} not found')\n",
    "\n",
    "        return df\n",
    "\n",
    "    def cast_string_U_nan(self, df, col):\n",
    "        try:\n",
    "            df[col] = df[col].astype('string')\n",
    "            df[col] = df[col].replace('U', 'N')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'-Column {col} not found')\n",
    "        return df\n",
    "\n",
    "    def cast_string_2(self, df, col):\n",
    "        try:\n",
    "            df[col] = df[col].astype('string')\n",
    "            df[col] = df[col].str[:2]\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'-Column {col} not found')\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def cast_string_5(self, df, col):\n",
    "        try:\n",
    "            df[col] = df[col].astype('string')\n",
    "            df[col] = df[col].fillna('00000')\n",
    "            df[col] = df[col].str[:5]\n",
    "            df[col] = df[col].apply(lambda x: int(x) if x.isnumeric() else 0)\n",
    "            #df[col] = df[col].astype('string')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'-Column {col} not found')\n",
    "        \n",
    "        return df\n",
    "\n",
    "\n",
    "    def cast_datetime64(self, df, col):\n",
    "        try:\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'-Column {col} not found')\n",
    "        \n",
    "        return df\n",
    "\n",
    "\n",
    "    def cast_remove(self, df, col):\n",
    "        try:\n",
    "            df.drop(columns=[col], inplace=True)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'-Column {col} not found')\n",
    "        return df\n",
    "    \n",
    "    \n",
    "# ---------------------------------encoding part\n",
    "\n",
    "    def refresh_transformed_cols(self):\n",
    "        self.transformed_cols = [new_col for col, method, new_col in self.transformation_list]\n",
    "        if len(self.transformed_cols) > len(set(self.transformed_cols)):\n",
    "            print('Warning: there are duplicates in the transformation list')\n",
    "    \n",
    "    def update_transformation_methods(self):\n",
    "        self.transformation_methods = set([method for col, method, new_col in self.transformation_list])\n",
    "        \n",
    "    def look_for_feature_transformation(self,feature):\n",
    "        i = 0\n",
    "        result = []\n",
    "        for col, method, new_col in self.transformation_list:   \n",
    "            if col == feature:\n",
    "                print(f'-Column {col} is transformed in {new_col} trough {method} at index {i}')\n",
    "                result.append((method, new_col, i))\n",
    "            i += 1\n",
    "        print(f'-Column {feature} not found in transformation list')\n",
    "        return result\n",
    "    \n",
    "    def look_for_new_col(self,feature):\n",
    "        i = 0\n",
    "        for col, method, new_col in self.transformation_list:\n",
    "            if col == feature:\n",
    "                print(f'-Column {new_col} is made from {method} at index {i}')\n",
    "                return new_col\n",
    "            i += 1\n",
    "        print(f'-Column {feature} not found in transformation list')\n",
    "        return None\n",
    "    \n",
    "    def append_transformation(self,feature,method,new_col):\n",
    "        self.transformation_list.append((feature, method, new_col))\n",
    "\n",
    "\n",
    "    def transformation_pipeline(self,df):\n",
    "        onehot_list = []\n",
    "        for col, method, new_col in self.transformation_list:\n",
    "            if method == 'log':\n",
    "                df = self.transformation_log(df, col, new_col)\n",
    "            elif method == 'subtract_1900':\n",
    "                df = self.transformation_subtract_1900(df, col, new_col)\n",
    "            elif method == 'dummy-YN':\n",
    "                df = self.transformation_dummy_yn(df, col, new_col)\n",
    "            elif method == 'freq_encode':\n",
    "                df = self.transformation_freq_encode(df, col, new_col)\n",
    "            elif method == 'oneHot':\n",
    "                if len(df[col].unique()) < self.max_col_onehot:\n",
    "                    onehot_list.append(col)\n",
    "                else:\n",
    "                    print(f'Column {col} has too many unique values for oneHot encoding: {len(df[col].unique())}')\n",
    "                    print(f'ony {self.max_col_onehot} allowed, change the parameter if needed')\n",
    "                #df = self.transformation_oneHot(df, col, new_col)\n",
    "            elif method == '.str[5:7]':\n",
    "                df = self.transformation_str57(df, col, new_col)\n",
    "            elif method == '.str[0:4]':\n",
    "                df = self.transformation_str04(df, col, new_col)\n",
    "            elif method == '.str[0:5]':\n",
    "                df = self.transformation_str05(df, col, new_col)\n",
    "            elif method == 'remove':\n",
    "                df = self.transformation_remove(df, col)\n",
    "            elif method == 'none':\n",
    "                pass\n",
    "            else:\n",
    "                print(f'Unknown method {method} for column {col}')\n",
    "                \n",
    "        if len(onehot_list) > 0:\n",
    "            self.encoder.fit(df[onehot_list])\n",
    "            X = self.encoder.transform(df[onehot_list])\n",
    "            X = pd.DataFrame(X.toarray(), columns=self.encoder.get_feature_names_out(onehot_list), index = df.index)\n",
    "            df = pd.concat([df, X], axis=1)\n",
    "        return df     \n",
    "\n",
    "\n",
    "    \n",
    "    def set_transformations(self,df)->None:\n",
    "        df_cols = df.columns\n",
    "        self.refresh_transformed_cols()\n",
    "        columns_to_be_transformed = set(df_cols) - set(self.transformed_cols)\n",
    "    \n",
    "        date_cols = []\n",
    "        date_cols.extend([x for x in columns_to_be_transformed if 'Date' in x])\n",
    "        for col in date_cols:\n",
    "            col_year = col.replace('Date', 'Year')\n",
    "            col_month = col.replace('Date', 'Month')\n",
    "            self.append_transformation(col, '.str[5:7]', col_month)\n",
    "            self.append_transformation(col, '.str[0:4]', col_year)\n",
    "            self.append_transformation(col, 'remove', '')\n",
    "        columns_to_be_transformed = columns_to_be_transformed - set(date_cols)\n",
    "            \n",
    "        code_cols = []\n",
    "        code_cols.extend([x for x in columns_to_be_transformed if 'Code' in x])\n",
    "        for col in code_cols:\n",
    "            if len(df[col].unique()) < 10:\n",
    "                self.append_transformation(col, 'oneHot', '-one')\n",
    "            self.append_transformation(col, 'freq_encode', 'fe_'+col)\n",
    "        \n",
    "                    \n",
    "            \n",
    "        \n",
    "        columns_to_be_transformed = columns_to_be_transformed - set(code_cols)\n",
    "    \n",
    "        if len(columns_to_be_transformed) > 0:\n",
    "            print ('Columns that are not transformed:')\n",
    "            for col in columns_to_be_transformed:\n",
    "                try:\n",
    "                    print (f'-{col}: {df[col].dtype}')\n",
    "                except Exception as e: \n",
    "                    print(e)\n",
    "                    print (f'-{col}: not found')\n",
    "                    \n",
    "                    \n",
    "    def transformation_log(self, df, col, new_col):\n",
    "        try:\n",
    "            df[col].fillna(0, inplace=True)\n",
    "            df[new_col] = np.log(df[col] + 1)\n",
    "            self.append_scaling(new_col, 1)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'Column {col} not found')\n",
    "        return df\n",
    "\n",
    "\n",
    "    def transformation_subtract_1900(self, df, col, new_col):\n",
    "        try:\n",
    "            df[new_col] = df[col] - 1900\n",
    "            self.append_scaling(new_col, 1)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'Column {col} not found')\n",
    "        return df\n",
    "\n",
    "\n",
    "    def transformation_dummy_yn(self, df, col, new_col):\n",
    "        try:\n",
    "            df[new_col] = df[col].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "            self.append_scaling(new_col, 0)    \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'Column {col} not found')\n",
    "        return df\n",
    "\n",
    "\n",
    "    def transformation_freq_encode(self, df, col, new_col):\n",
    "        try:\n",
    "            freq = df[col].value_counts(normalize=True)\n",
    "            df.loc[:, new_col] = df[col].map(freq)\n",
    "            self.append_scaling(new_col, 1)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'Column {col} not found')\n",
    "        return df\n",
    "\n",
    "\n",
    "    def transformation_encode(self, df, col, new_col):\n",
    "        try:\n",
    "            df[new_col], _ = df[col].factorize()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'Column {col} not found')\n",
    "        return df\n",
    "\n",
    "\n",
    "    def transformation_str57(self, df, col, new_col):\n",
    "        try:\n",
    "            df[new_col] = df[col].str[5:7].astype('Int64')\n",
    "            self.append_scaling(new_col, 0)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'Column {col} not found')\n",
    "        \n",
    "        return df\n",
    "\n",
    "\n",
    "    def transformation_str04(self, df, col, new_col):\n",
    "        try:\n",
    "            df[new_col] = df[col].str[0:4].astype('Int64')\n",
    "            self.append_scaling(new_col, 0)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'Column {col} not found')\n",
    "        return df\n",
    "    \n",
    "    def transformation_str05(self, df, col, new_col):\n",
    "        try:\n",
    "            df[new_col] = df[col].str[0:5]\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'Column {col} not found')\n",
    "        return df\n",
    "    def transformation_remove(self, df, col):\n",
    "        try:\n",
    "            df.drop(columns=[col], inplace=True)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'Column {col} not found')\n",
    "        return df\n",
    "    \n",
    "    def encoder_fit(pr, X):\n",
    "        pr.encoder.fit(X)\n",
    "        return pr\n",
    "\n",
    "    def encoder_transform(pr, X):\n",
    "        Xcolumns = X.columns\n",
    "        X = pr.encoder.transform(X)\n",
    "        X = pd.DataFrame(X.toarray(), columns=pr.encoder.get_feature_names_out(Xcolumns))\n",
    "        return X\n",
    "    \n",
    "    def transformation_oneHot(self, df, col, new_col):\n",
    "        try:\n",
    "            self.encoder.fit(df[[col]])\n",
    "            X = self.encoder.transform(df[[col]])\n",
    "            X = pd.DataFrame(X.toarray(), columns=self.encoder.get_feature_names_out([col]))\n",
    "            df = pd.concat([df, X], axis=1)\n",
    "            for new_column in X.columns:\n",
    "                self.append_scaling(new_column, 0)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'Column {col} not found')\n",
    "        return df\n",
    "    \n",
    "    def update_transformation_list(self,feature,method,new_col):\n",
    "        i = 0\n",
    "        for col, method, new_col in self.transformation_list:\n",
    "            if col == feature:\n",
    "                self.transformation_list[i] = (feature, method, new_col)\n",
    "                return\n",
    "            i += 1\n",
    "            \n",
    "    \n",
    "    def add_transformation(self,feature,method,new_col):\n",
    "        self.transformation_list.append((feature, method, new_col))\n",
    "        \n",
    "\n",
    "# --------------------------------- fill the missing values\n",
    "\n",
    "    def update_fillna_list(self,df):\n",
    "        self.fillna_list = [(col, 'median') for col in df.columns if df[col].dtype in ['Int64', 'int64', 'float64','Float64']]\n",
    "        self.fillna_list.extend([(col, 'mode') for col in df.columns if df[col].dtype in ['string', 'object']])\n",
    "        print(f'extended fillna_list: {self.fillna_list}')\n",
    "    \n",
    "    def fillna_pipeline(self,df):\n",
    "        print(f'nans in the beginning: {df.isna().sum().sum()}')\n",
    "        for col,method in self.fillna_list:\n",
    "            if method == 'median':\n",
    "                df = self.fillna_median(df, col)\n",
    "            elif method == 'mode':\n",
    "                df = self.fillna_mode(df, col)\n",
    "            elif method == 'mean':\n",
    "                df = self.fillna_mean(df, col)\n",
    "            elif method == 'zero':\n",
    "                df = self.fillna_zero(df, col)\n",
    "            else:\n",
    "                print(f'Unknown method {method} for column {col}')\n",
    "            \n",
    "            print(f'Column {col} is filled with {method} ->num nan: {df[col].isna().sum()}')\n",
    "\n",
    "        print(f'nans in the end: {df.isna().sum().sum()}')\n",
    "\n",
    "        return df\n",
    "\n",
    "    def fillna_median(self, df, col):\n",
    "        try:\n",
    "            med = df[col].median().astype('int64')\n",
    "            df[col].fillna(med, inplace=True)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'Column {col} not found')\n",
    "        return df\n",
    "    \n",
    "    def fillna_mode(self, df, col):\n",
    "        try:\n",
    "            mode = df[col].mode()[0]\n",
    "            df[col].fillna(mode, inplace=True)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'Column {col} not found')\n",
    "        return df\n",
    "\n",
    "    def fillna_mean(self, df, col):\n",
    "        try:\n",
    "            mean = df[col].mean().astype(df[col].dtype.name)\n",
    "            df[col].fillna(mean, inplace=True)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'Column {col} not found')\n",
    "        return df\n",
    "\n",
    "    def fillna_zero(self, df, col):\n",
    "        try:\n",
    "            df[col].fillna(0, inplace=True)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'Column {col} not found')\n",
    "        return df\n",
    "    \n",
    "    def look_for_fillna_method(self,feature):\n",
    "        i = 0\n",
    "        for col, method in self.fillna_list:\n",
    "            if col == feature:\n",
    "                print(f'-Column {col} is filled with {method} at index {i}')\n",
    "                return method\n",
    "            i += 1\n",
    "        print(f'-Column {feature} not found in fillna list')\n",
    "        return None\n",
    "    \n",
    "    def change_fillna_method(self,feature,method):\n",
    "        i = 0\n",
    "        for col, method in self.fillna_list:\n",
    "            if col == feature:\n",
    "                self.fillna_list[i] = (feature, method)\n",
    "                return\n",
    "            i += 1\n",
    "        print(f'-Column {feature} not found in fillna list')\n",
    "        self.add_fillna(feature, method)\n",
    "        \n",
    "    def add_fillna(self,feature,method):      \n",
    "        self.fillna_list.append((feature, method))\n",
    "# --------------------------------- scaling part\n",
    "    def append_scaling(self,feature,yn):\n",
    "        features = [col for col, yn in self.sclaing_list]\n",
    "        if feature in features:\n",
    "            print(f'Feature {feature} is already in scaling list')\n",
    "            return\n",
    "    \n",
    "    \n",
    "        if yn == 'Y'or yn == 'y' or yn == 'yes' or yn == 'Yes' or yn == 1:\n",
    "            self.sclaing_list.append((feature,1))\n",
    "        else:\n",
    "            self.sclaing_list.append((feature,0))\n",
    "\n",
    "    def update_scaling_list(self, feature, yn):\n",
    "        i = 0\n",
    "        for col, yn_ in self.sclaing_list:\n",
    "            if col == feature:\n",
    "                self.sclaing_list[i] = (feature, yn)\n",
    "                return i\n",
    "            i += 1\n",
    "        print(f'Feature {feature} not found in scaling list, adding it now')\n",
    "        self.append_scaling(feature, yn)\n",
    "        return i\n",
    "        \n",
    "        \n",
    "    \n",
    "    def refresh_scaling_list(self,df):\n",
    "        num_cols = [col for col in df.columns if df[col].dtype in ['Int64', 'int64', 'float64','Float64']]\n",
    "        #other_cols = [col for col in df.columns if df[col] not in num_cols]\n",
    "        cols_to_be_updated = set(df.columns) - set(self.sclaing_list)\n",
    "        \n",
    "        for col in cols_to_be_updated:\n",
    "            if col in num_cols:\n",
    "                self.append_scaling(col, 1)\n",
    "            else:\n",
    "                self.append_scaling(col, 0)\n",
    "                \n",
    "        \n",
    "    def scaling_fit (self,sd):\n",
    "        self.scaler.fit(sd)\n",
    "        return self\n",
    "    \n",
    "    def scaling_transform(self,sd):\n",
    "        sd = self.scaler.transform(sd)\n",
    "        sd = pd.DataFrame(sd, columns = sd.columns)\n",
    "        return sd\n",
    "\n",
    "    def scaling_pipeline(self,sd):\n",
    "        #self.refresh_scaling_list(sd)\n",
    "        scaling_cols = [col for col, yn in self.sclaing_list if yn == 1]\n",
    "        scaling_cols = [col for col in sd.columns]\n",
    "        other_cols = [col for col in sd.columns if col not in scaling_cols]\n",
    "        if self.status == 'train':\n",
    "            print(f'scaling_cols: {scaling_cols}')\n",
    "            print(f'info: {sd[scaling_cols].info()}')\n",
    "            self.scaler.fit(sd[scaling_cols])       \n",
    "        X = self.scaler.transform(sd[scaling_cols])\n",
    "        X = pd.DataFrame(X, columns = scaling_cols)\n",
    "        sd = pd.concat([sd[other_cols], X], axis=1)\n",
    "        return sd\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# install libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data\n",
    "train_path = \"../Data/train_data.csv\"\n",
    "test_path = \"../Data/test_data.csv\"\n",
    "# Replace 'Column29' with the actual column name that has mixed types\n",
    "dtype = {'Column29': 'str'}\n",
    "\n",
    "train = pd.read_csv(train_path, index_col='Claim Identifier',\n",
    "                    dtype=dtype, low_memory=False)\n",
    "test = pd.read_csv(test_path, index_col='Claim Identifier',\n",
    "                   dtype=dtype, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all the rows that have missing values in Claim Injury Type\n",
    "train = train.dropna(subset=['Claim Injury Type'])\n",
    "y = train['Claim Injury Type']\n",
    "y_str = y.str[:1]\n",
    "y_int = y_str.astype(int)\n",
    "X = train.drop(columns=['Claim Injury Type'])\n",
    "# partition the data X, y and y_2bin\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y_int, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------updates---------------------\n",
      "------------set_casting---------------------\n",
      "Columns that are not casted:\n",
      "-WCB Decision: object\n",
      "------------pipeline---------------------\n",
      "Feature Carrier Type is already in scaling list\n",
      "Feature Zip Code is already in scaling list\n",
      "'Industry Code Description'\n",
      "-Column Industry Code Description not found\n",
      "df after cast_pipeline: (459220, 26)\n",
      "------------fillna---------------------\n",
      "extended fillna_list: [('Age at Injury', 'median'), ('Average Weekly Wage', 'median'), ('Birth Year', 'median'), ('IME-4 Count', 'median'), ('Industry Code', 'median'), ('WCIO Cause of Injury Code', 'median'), ('WCIO Nature of Injury Code', 'median'), ('WCIO Part Of Body Code', 'median'), ('Zip Code', 'median'), ('Agreement Reached', 'median'), ('Number of Dependents', 'median'), ('Accident Date', 'mode'), ('Alternative Dispute Resolution', 'mode'), ('Assembly Date', 'mode'), ('Attorney/Representative', 'mode'), ('C-2 Date', 'mode'), ('C-3 Date', 'mode'), ('Carrier Name', 'mode'), ('Carrier Type', 'mode'), ('County of Injury', 'mode'), ('COVID-19 Indicator', 'mode'), ('District Name', 'mode'), ('First Hearing Date', 'mode'), ('Gender', 'mode'), ('Medical Fee Region', 'mode'), ('WCB Decision', 'mode')]\n",
      "nans in the beginning: 1108552\n",
      "Column Age at Injury is filled with median ->num nan: 0\n",
      "Column Average Weekly Wage is filled with median ->num nan: 0\n",
      "Column Birth Year is filled with median ->num nan: 0\n",
      "Column IME-4 Count is filled with median ->num nan: 0\n",
      "Column Industry Code is filled with median ->num nan: 0\n",
      "Column WCIO Cause of Injury Code is filled with median ->num nan: 0\n",
      "Column WCIO Nature of Injury Code is filled with median ->num nan: 0\n",
      "Column WCIO Part Of Body Code is filled with median ->num nan: 0\n",
      "Column Zip Code is filled with median ->num nan: 0\n",
      "Column Agreement Reached is filled with median ->num nan: 0\n",
      "Column Number of Dependents is filled with median ->num nan: 0\n",
      "Column Accident Date is filled with mode ->num nan: 0\n",
      "Column Alternative Dispute Resolution is filled with mode ->num nan: 0\n",
      "Column Assembly Date is filled with mode ->num nan: 0\n",
      "Column Attorney/Representative is filled with mode ->num nan: 0\n",
      "Column C-2 Date is filled with mode ->num nan: 0\n",
      "Column C-3 Date is filled with mode ->num nan: 0\n",
      "Column Carrier Name is filled with mode ->num nan: 0\n",
      "Column Carrier Type is filled with mode ->num nan: 0\n",
      "Column County of Injury is filled with mode ->num nan: 0\n",
      "Column COVID-19 Indicator is filled with mode ->num nan: 0\n",
      "Column District Name is filled with mode ->num nan: 0\n",
      "Column First Hearing Date is filled with mode ->num nan: 0\n",
      "Column Gender is filled with mode ->num nan: 0\n",
      "Column Medical Fee Region is filled with mode ->num nan: 0\n",
      "Column WCB Decision is filled with mode ->num nan: 0\n",
      "nans in the end: 0\n",
      "df after fillna_pipeline: (459220, 26)\n",
      "------------transformation---------------------\n",
      "Warning: there are duplicates in the transformation list\n",
      "Columns that are not transformed:\n",
      "-Average Weekly Wage: float64\n",
      "-Birth Year: Int64\n",
      "-Agreement Reached: Int64\n",
      "-Carrier Name: string\n",
      "-WCB Decision: object\n",
      "-IME-4 Count: Int64\n",
      "-County of Injury: string\n",
      "-Carrier Type: string\n",
      "-District Name: object\n",
      "-Gender: string\n",
      "-Age at Injury: Int64\n",
      "-Medical Fee Region: string\n",
      "-Number of Dependents: Int64\n",
      "Feature log_Average Weekly Wage is already in scaling list\n",
      "Feature Alternative Dispute Resolution is already in scaling list\n",
      "Feature COVID-19 Indicator is already in scaling list\n",
      "Column Carrier Type has too many unique values for oneHot encoding: 8\n",
      "ony 5 allowed, change the parameter if needed\n",
      "Feature fe_Carrier Type is already in scaling list\n",
      "Column Medical Fee Region has too many unique values for oneHot encoding: 5\n",
      "ony 5 allowed, change the parameter if needed\n",
      "Feature fe_Medical Fee Region is already in scaling list\n",
      "Column County of Injury has too many unique values for oneHot encoding: 63\n",
      "ony 5 allowed, change the parameter if needed\n",
      "Feature fe_County of Injury is already in scaling list\n",
      "Feature Attorney/Representative is already in scaling list\n",
      "Feature Age is already in scaling list\n",
      "Feature Assembly Month is already in scaling list\n",
      "Feature Assembly Year is already in scaling list\n",
      "Feature C-3 Month is already in scaling list\n",
      "Feature C-3 Year is already in scaling list\n",
      "Feature Accident Month is already in scaling list\n",
      "Feature Accident Year is already in scaling list\n",
      "Feature C-2 Month is already in scaling list\n",
      "Feature C-2 Year is already in scaling list\n",
      "Feature First Hearing Month is already in scaling list\n",
      "Feature First Hearing Year is already in scaling list\n",
      "Feature fe_WCIO Part Of Body Code is already in scaling list\n",
      "Feature fe_Industry Code is already in scaling list\n",
      "Feature fe_WCIO Nature of Injury Code is already in scaling list\n",
      "Feature fe_Zip Code is already in scaling list\n",
      "Feature fe_WCIO Cause of Injury Code is already in scaling list\n",
      "df after transformation_pipeline: (459220, 45)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 459220 entries, 5877274 to 5803696\n",
      "Data columns (total 45 columns):\n",
      " #   Column                          Non-Null Count   Dtype  \n",
      "---  ------                          --------------   -----  \n",
      " 0   Age at Injury                   459220 non-null  Int64  \n",
      " 1   Alternative Dispute Resolution  459220 non-null  int64  \n",
      " 2   Attorney/Representative         459220 non-null  int64  \n",
      " 3   Average Weekly Wage             459220 non-null  float64\n",
      " 4   Birth Year                      459220 non-null  Int64  \n",
      " 5   Carrier Name                    459220 non-null  string \n",
      " 6   Carrier Type                    459220 non-null  string \n",
      " 7   County of Injury                459220 non-null  string \n",
      " 8   COVID-19 Indicator              459220 non-null  int64  \n",
      " 9   District Name                   459220 non-null  object \n",
      " 10  Gender                          459220 non-null  string \n",
      " 11  IME-4 Count                     459220 non-null  Int64  \n",
      " 12  Industry Code                   459220 non-null  Int64  \n",
      " 13  Medical Fee Region              459220 non-null  string \n",
      " 14  WCIO Cause of Injury Code       459220 non-null  Int64  \n",
      " 15  WCIO Nature of Injury Code      459220 non-null  Int64  \n",
      " 16  WCIO Part Of Body Code          459220 non-null  Int64  \n",
      " 17  Zip Code                        459220 non-null  Int64  \n",
      " 18  Agreement Reached               459220 non-null  Int64  \n",
      " 19  WCB Decision                    459220 non-null  object \n",
      " 20  Number of Dependents            459220 non-null  Int64  \n",
      " 21  log_Average Weekly Wage         459220 non-null  float64\n",
      " 22  fe_Carrier Type                 459220 non-null  Float64\n",
      " 23  fe_Medical Fee Region           459220 non-null  Float64\n",
      " 24  fe_County of Injury             459220 non-null  Float64\n",
      " 25  Age                             459220 non-null  Int64  \n",
      " 26  Assembly Month                  459220 non-null  Int64  \n",
      " 27  Assembly Year                   459220 non-null  Int64  \n",
      " 28  C-3 Month                       459220 non-null  Int64  \n",
      " 29  C-3 Year                        459220 non-null  Int64  \n",
      " 30  Accident Month                  459220 non-null  Int64  \n",
      " 31  Accident Year                   459220 non-null  Int64  \n",
      " 32  C-2 Month                       459220 non-null  Int64  \n",
      " 33  C-2 Year                        459220 non-null  Int64  \n",
      " 34  First Hearing Month             459220 non-null  Int64  \n",
      " 35  First Hearing Year              459220 non-null  Int64  \n",
      " 36  fe_WCIO Part Of Body Code       459220 non-null  Float64\n",
      " 37  fe_Industry Code                459220 non-null  Float64\n",
      " 38  fe_WCIO Nature of Injury Code   459220 non-null  Float64\n",
      " 39  fe_Zip Code                     459220 non-null  Float64\n",
      " 40  fe_WCIO Cause of Injury Code    459220 non-null  Float64\n",
      " 41  Gender_F                        459220 non-null  float64\n",
      " 42  Gender_M                        459220 non-null  float64\n",
      " 43  Gender_U                        459220 non-null  float64\n",
      " 44  Gender_X                        459220 non-null  float64\n",
      "dtypes: Float64(8), Int64(21), float64(6), int64(3), object(2), string(5)\n",
      "memory usage: 173.9+ MB\n"
     ]
    }
   ],
   "source": [
    "pr = PreProcessor1()\n",
    "print(\"------------updates---------------------\")\n",
    "pr.update_casted_cols()\n",
    "print(\"------------set_casting---------------------\")\n",
    "pr.set_castings(X_train)\n",
    "print(\"------------pipeline---------------------\")\n",
    "df_train = pr.cast_pipeline(X_train)\n",
    "print(\"df after cast_pipeline:\", df_train.shape)\n",
    "print(\"------------fillna---------------------\")\n",
    "pr.update_fillna_list(df_train)   \n",
    "df_train = pr.fillna_pipeline(df_train)\n",
    "print(\"df after fillna_pipeline:\", df_train.shape)\n",
    "print(\"------------transformation---------------------\")\n",
    "pr.set_transformations(df_train)\n",
    "df_train = pr.transformation_pipeline(df_train)\n",
    "print(\"df after transformation_pipeline:\", df_train.shape)\n",
    "\n",
    "\n",
    "\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 459220 entries, 5877274 to 5803696\n",
      "Data columns (total 45 columns):\n",
      " #   Column                          Non-Null Count   Dtype  \n",
      "---  ------                          --------------   -----  \n",
      " 0   Age at Injury                   459220 non-null  Int64  \n",
      " 1   Alternative Dispute Resolution  459220 non-null  int64  \n",
      " 2   Attorney/Representative         459220 non-null  int64  \n",
      " 3   Average Weekly Wage             459220 non-null  float64\n",
      " 4   Birth Year                      459220 non-null  Int64  \n",
      " 5   Carrier Name                    459220 non-null  string \n",
      " 6   Carrier Type                    459220 non-null  string \n",
      " 7   County of Injury                459220 non-null  string \n",
      " 8   COVID-19 Indicator              459220 non-null  int64  \n",
      " 9   District Name                   459220 non-null  object \n",
      " 10  Gender                          459220 non-null  string \n",
      " 11  IME-4 Count                     459220 non-null  Int64  \n",
      " 12  Industry Code                   459220 non-null  Int64  \n",
      " 13  Medical Fee Region              459220 non-null  string \n",
      " 14  WCIO Cause of Injury Code       459220 non-null  Int64  \n",
      " 15  WCIO Nature of Injury Code      459220 non-null  Int64  \n",
      " 16  WCIO Part Of Body Code          459220 non-null  Int64  \n",
      " 17  Zip Code                        459220 non-null  Int64  \n",
      " 18  Agreement Reached               459220 non-null  Int64  \n",
      " 19  WCB Decision                    459220 non-null  object \n",
      " 20  Number of Dependents            459220 non-null  Int64  \n",
      " 21  log_Average Weekly Wage         459220 non-null  float64\n",
      " 22  fe_Carrier Type                 459220 non-null  Float64\n",
      " 23  fe_Medical Fee Region           459220 non-null  Float64\n",
      " 24  fe_County of Injury             459220 non-null  Float64\n",
      " 25  Age                             459220 non-null  Int64  \n",
      " 26  Assembly Month                  459220 non-null  Int64  \n",
      " 27  Assembly Year                   459220 non-null  Int64  \n",
      " 28  C-3 Month                       459220 non-null  Int64  \n",
      " 29  C-3 Year                        459220 non-null  Int64  \n",
      " 30  Accident Month                  459220 non-null  Int64  \n",
      " 31  Accident Year                   459220 non-null  Int64  \n",
      " 32  C-2 Month                       459220 non-null  Int64  \n",
      " 33  C-2 Year                        459220 non-null  Int64  \n",
      " 34  First Hearing Month             459220 non-null  Int64  \n",
      " 35  First Hearing Year              459220 non-null  Int64  \n",
      " 36  fe_WCIO Part Of Body Code       459220 non-null  Float64\n",
      " 37  fe_Industry Code                459220 non-null  Float64\n",
      " 38  fe_WCIO Nature of Injury Code   459220 non-null  Float64\n",
      " 39  fe_Zip Code                     459220 non-null  Float64\n",
      " 40  fe_WCIO Cause of Injury Code    459220 non-null  Float64\n",
      " 41  Gender_F                        459220 non-null  float64\n",
      " 42  Gender_M                        459220 non-null  float64\n",
      " 43  Gender_U                        459220 non-null  float64\n",
      " 44  Gender_X                        459220 non-null  float64\n",
      "dtypes: Float64(8), Int64(21), float64(6), int64(3), object(2), string(5)\n",
      "memory usage: 173.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature WCIO Part Of Body Code is already in scaling list\n",
      "Feature Industry Code is already in scaling list\n",
      "Feature C-3 Month is already in scaling list\n",
      "Feature WCIO Nature of Injury Code is already in scaling list\n",
      "Feature Gender_U is already in scaling list\n",
      "Feature Gender_X is already in scaling list\n",
      "Feature log_Average Weekly Wage is already in scaling list\n",
      "Feature Assembly Year is already in scaling list\n",
      "Feature IME-4 Count is already in scaling list\n",
      "Feature C-3 Year is already in scaling list\n",
      "Feature Age at Injury is already in scaling list\n",
      "Feature District Name is already in scaling list\n",
      "Feature Attorney/Representative is already in scaling list\n",
      "Feature Average Weekly Wage is already in scaling list\n",
      "Feature Medical Fee Region is already in scaling list\n",
      "Feature Number of Dependents is already in scaling list\n",
      "Feature Assembly Month is already in scaling list\n",
      "Feature First Hearing Year is already in scaling list\n",
      "Feature Carrier Name is already in scaling list\n",
      "Feature fe_WCIO Cause of Injury Code is already in scaling list\n",
      "Feature Gender_F is already in scaling list\n",
      "Feature Carrier Type is already in scaling list\n",
      "Feature Zip Code is already in scaling list\n",
      "Feature fe_Medical Fee Region is already in scaling list\n",
      "Feature C-2 Month is already in scaling list\n",
      "Feature fe_Carrier Type is already in scaling list\n",
      "Feature fe_Industry Code is already in scaling list\n",
      "Feature Agreement Reached is already in scaling list\n",
      "Feature Age is already in scaling list\n",
      "Feature Accident Year is already in scaling list\n",
      "Feature fe_WCIO Nature of Injury Code is already in scaling list\n",
      "Feature Gender is already in scaling list\n",
      "Feature First Hearing Month is already in scaling list\n",
      "Feature fe_Zip Code is already in scaling list\n",
      "Feature COVID-19 Indicator is already in scaling list\n",
      "Feature Birth Year is already in scaling list\n",
      "Feature Alternative Dispute Resolution is already in scaling list\n",
      "Feature WCB Decision is already in scaling list\n",
      "Feature fe_WCIO Part Of Body Code is already in scaling list\n",
      "Feature Gender_M is already in scaling list\n",
      "Feature WCIO Cause of Injury Code is already in scaling list\n",
      "Feature County of Injury is already in scaling list\n",
      "Feature fe_County of Injury is already in scaling list\n",
      "Feature Accident Month is already in scaling list\n",
      "Feature C-2 Year is already in scaling list\n",
      "scaling_cols: ['Age at Injury', 'Alternative Dispute Resolution', 'Attorney/Representative', 'Average Weekly Wage', 'Birth Year', 'Carrier Name', 'Carrier Type', 'County of Injury', 'COVID-19 Indicator', 'District Name', 'Gender', 'IME-4 Count', 'Industry Code', 'Medical Fee Region', 'WCIO Cause of Injury Code', 'WCIO Nature of Injury Code', 'WCIO Part Of Body Code', 'Zip Code', 'Agreement Reached', 'WCB Decision', 'Number of Dependents', 'log_Average Weekly Wage', 'fe_Carrier Type', 'fe_Medical Fee Region', 'fe_County of Injury', 'Age', 'Assembly Month', 'Assembly Year', 'C-3 Month', 'C-3 Year', 'Accident Month', 'Accident Year', 'C-2 Month', 'C-2 Year', 'First Hearing Month', 'First Hearing Year', 'fe_WCIO Part Of Body Code', 'fe_Industry Code', 'fe_WCIO Nature of Injury Code', 'fe_Zip Code', 'fe_WCIO Cause of Injury Code', 'Gender_F', 'Gender_M', 'Gender_U', 'Gender_X']\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 459220 entries, 5877274 to 5803696\n",
      "Data columns (total 45 columns):\n",
      " #   Column                          Non-Null Count   Dtype  \n",
      "---  ------                          --------------   -----  \n",
      " 0   Age at Injury                   459220 non-null  Int64  \n",
      " 1   Alternative Dispute Resolution  459220 non-null  int64  \n",
      " 2   Attorney/Representative         459220 non-null  int64  \n",
      " 3   Average Weekly Wage             459220 non-null  float64\n",
      " 4   Birth Year                      459220 non-null  Int64  \n",
      " 5   Carrier Name                    459220 non-null  string \n",
      " 6   Carrier Type                    459220 non-null  string \n",
      " 7   County of Injury                459220 non-null  string \n",
      " 8   COVID-19 Indicator              459220 non-null  int64  \n",
      " 9   District Name                   459220 non-null  object \n",
      " 10  Gender                          459220 non-null  string \n",
      " 11  IME-4 Count                     459220 non-null  Int64  \n",
      " 12  Industry Code                   459220 non-null  Int64  \n",
      " 13  Medical Fee Region              459220 non-null  string \n",
      " 14  WCIO Cause of Injury Code       459220 non-null  Int64  \n",
      " 15  WCIO Nature of Injury Code      459220 non-null  Int64  \n",
      " 16  WCIO Part Of Body Code          459220 non-null  Int64  \n",
      " 17  Zip Code                        459220 non-null  Int64  \n",
      " 18  Agreement Reached               459220 non-null  Int64  \n",
      " 19  WCB Decision                    459220 non-null  object \n",
      " 20  Number of Dependents            459220 non-null  Int64  \n",
      " 21  log_Average Weekly Wage         459220 non-null  float64\n",
      " 22  fe_Carrier Type                 459220 non-null  Float64\n",
      " 23  fe_Medical Fee Region           459220 non-null  Float64\n",
      " 24  fe_County of Injury             459220 non-null  Float64\n",
      " 25  Age                             459220 non-null  Int64  \n",
      " 26  Assembly Month                  459220 non-null  Int64  \n",
      " 27  Assembly Year                   459220 non-null  Int64  \n",
      " 28  C-3 Month                       459220 non-null  Int64  \n",
      " 29  C-3 Year                        459220 non-null  Int64  \n",
      " 30  Accident Month                  459220 non-null  Int64  \n",
      " 31  Accident Year                   459220 non-null  Int64  \n",
      " 32  C-2 Month                       459220 non-null  Int64  \n",
      " 33  C-2 Year                        459220 non-null  Int64  \n",
      " 34  First Hearing Month             459220 non-null  Int64  \n",
      " 35  First Hearing Year              459220 non-null  Int64  \n",
      " 36  fe_WCIO Part Of Body Code       459220 non-null  Float64\n",
      " 37  fe_Industry Code                459220 non-null  Float64\n",
      " 38  fe_WCIO Nature of Injury Code   459220 non-null  Float64\n",
      " 39  fe_Zip Code                     459220 non-null  Float64\n",
      " 40  fe_WCIO Cause of Injury Code    459220 non-null  Float64\n",
      " 41  Gender_F                        459220 non-null  float64\n",
      " 42  Gender_M                        459220 non-null  float64\n",
      " 43  Gender_U                        459220 non-null  float64\n",
      " 44  Gender_X                        459220 non-null  float64\n",
      "dtypes: Float64(8), Int64(21), float64(6), int64(3), object(2), string(5)\n",
      "memory usage: 173.9+ MB\n",
      "info: None\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'SENTRY INSURANCE COMPANY'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mpr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaling_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[40], line 672\u001b[0m, in \u001b[0;36mPreProcessor1.scaling_pipeline\u001b[0;34m(self, sd)\u001b[0m\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscaling_cols: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscaling_cols\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    671\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minfo: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msd[scaling_cols]\u001b[38;5;241m.\u001b[39minfo()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 672\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43msd\u001b[49m\u001b[43m[\u001b[49m\u001b[43mscaling_cols\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m       \n\u001b[1;32m    673\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mtransform(sd[scaling_cols])\n\u001b[1;32m    674\u001b[0m X \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(X, columns \u001b[38;5;241m=\u001b[39m scaling_cols)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sklearn/preprocessing/_data.py:1597\u001b[0m, in \u001b[0;36mRobustScaler.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1579\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute the median and quantiles to be used for scaling.\u001b[39;00m\n\u001b[1;32m   1580\u001b[0m \n\u001b[1;32m   1581\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1593\u001b[0m \u001b[38;5;124;03m    Fitted scaler.\u001b[39;00m\n\u001b[1;32m   1594\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1595\u001b[0m \u001b[38;5;66;03m# at fit, convert sparse matrices to csc for optimized computation of\u001b[39;00m\n\u001b[1;32m   1596\u001b[0m \u001b[38;5;66;03m# the quantiles\u001b[39;00m\n\u001b[0;32m-> 1597\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1598\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1599\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1600\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1601\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1602\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1604\u001b[0m q_min, q_max \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquantile_range\n\u001b[1;32m   1605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m q_min \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m q_max \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sklearn/base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[1;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/sklearn/utils/validation.py:929\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pandas_requires_conversion:\n\u001b[1;32m    925\u001b[0m     \u001b[38;5;66;03m# pandas dataframe requires conversion earlier to handle extension dtypes with\u001b[39;00m\n\u001b[1;32m    926\u001b[0m     \u001b[38;5;66;03m# nans\u001b[39;00m\n\u001b[1;32m    927\u001b[0m     \u001b[38;5;66;03m# Use the original dtype for conversion if dtype is None\u001b[39;00m\n\u001b[1;32m    928\u001b[0m     new_dtype \u001b[38;5;241m=\u001b[39m dtype_orig \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m dtype\n\u001b[0;32m--> 929\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    930\u001b[0m     \u001b[38;5;66;03m# Since we converted here, we do not need to convert again later\u001b[39;00m\n\u001b[1;32m    931\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/generic.py:6643\u001b[0m, in \u001b[0;36mNDFrame.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m   6637\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   6638\u001b[0m         ser\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy, errors\u001b[38;5;241m=\u001b[39merrors) \u001b[38;5;28;01mfor\u001b[39;00m _, ser \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   6639\u001b[0m     ]\n\u001b[1;32m   6641\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6642\u001b[0m     \u001b[38;5;66;03m# else, only a single dtype is given\u001b[39;00m\n\u001b[0;32m-> 6643\u001b[0m     new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6644\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_data, axes\u001b[38;5;241m=\u001b[39mnew_data\u001b[38;5;241m.\u001b[39maxes)\n\u001b[1;32m   6645\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/internals/managers.py:430\u001b[0m, in \u001b[0;36mBaseBlockManager.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    428\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 430\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mastype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43musing_cow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/internals/managers.py:363\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[0;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 363\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    364\u001b[0m     result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[1;32m    366\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/internals/blocks.py:758\u001b[0m, in \u001b[0;36mBlock.astype\u001b[0;34m(self, dtype, copy, errors, using_cow, squeeze)\u001b[0m\n\u001b[1;32m    755\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan not squeeze with more than one column.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    756\u001b[0m     values \u001b[38;5;241m=\u001b[39m values[\u001b[38;5;241m0\u001b[39m, :]  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[0;32m--> 758\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43mastype_array_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    760\u001b[0m new_values \u001b[38;5;241m=\u001b[39m maybe_coerce_values(new_values)\n\u001b[1;32m    762\u001b[0m refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/dtypes/astype.py:237\u001b[0m, in \u001b[0;36mastype_array_safe\u001b[0;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    234\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtype\u001b[38;5;241m.\u001b[39mnumpy_dtype\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 237\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m \u001b[43mastype_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;66;03m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m#  trying to convert to float\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/dtypes/astype.py:179\u001b[0m, in \u001b[0;36mastype_array\u001b[0;34m(values, dtype, copy)\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m values\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(values, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;66;03m# i.e. ExtensionArray\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    182\u001b[0m     values \u001b[38;5;241m=\u001b[39m _astype_nansafe(values, dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/pandas/core/arrays/string_.py:511\u001b[0m, in \u001b[0;36mStringArray.astype\u001b[0;34m(self, dtype, copy)\u001b[0m\n\u001b[1;32m    509\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39misna()\n\u001b[1;32m    510\u001b[0m arr[mask] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 511\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[43marr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    512\u001b[0m values[mask] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnan\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m values\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'SENTRY INSURANCE COMPANY'"
     ]
    }
   ],
   "source": [
    "x = pr.scaling_pipeline(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_int = y.str[:1].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature mutual correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = df_train.select_dtypes(include=['float64', 'int64']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corr_matrix = df_train[num_cols].corr()\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix, annot=False, cmap='magma')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### entropy based correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(feature):\n",
    "    return -np.sum([p*np.log2(p) for p in feature.value_counts(normalize=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_corr(df, n_iter=30, n_frac=0.5):\n",
    "\n",
    "    entropy_matrix = []\n",
    "    for col in df.columns:\n",
    "        s = entropy(df[col])\n",
    "        delta_entropy = []\n",
    "        for i in range(n_iter):\n",
    "            x = df[col].sample(frac=n_frac, random_state=i)\n",
    "            delta_entropy.append(s - entropy(x) / s)\n",
    "        entropy_matrix.append(delta_entropy)\n",
    "    entropy_matrix = pd.DataFrame(entropy_matrix, index=df.columns)\n",
    "    return entropy_matrix.T.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_corr_matrix = entropy_corr(df_train[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap((entropy_corr_matrix + 1)/2, annot=False, cmap='magma')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### recursive function for classification based on mutual correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this function classifies the features based on their mutual correlation. Once the most ocorrelated features are found, the function removes it and repeats the process until the correlation.\n",
    "\n",
    "I'm not a big fan of this methd but let's see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_correlated_feature_maxmin(df, report=False, method='linear', elbow=False) -> pd.DataFrame:\n",
    "    features = []\n",
    "    score = []\n",
    "    df_corr_search = df.copy()\n",
    "\n",
    "    if (method == 'linear') | (method == 'spearman') | (method == 'l'):\n",
    "        for i in range(df_corr_search.shape[1]):\n",
    "            corr_matrix = df_corr_search.corr()\n",
    "            # set the diagonal of the dataframe to 0 otherwise the max will always be 1\n",
    "            corr_matrix = corr_matrix - np.diag(np.diag(corr_matrix))\n",
    "            corr_matrix['max'] = abs(corr_matrix).max(axis=1)\n",
    "            feature_to_drop = corr_matrix['max'].idxmax()\n",
    "            score.append(corr_matrix['max'].max())\n",
    "            features.append(feature_to_drop)\n",
    "            df_corr_search = df_corr_search.drop(columns=[feature_to_drop])\n",
    "    elif (method == 'entropy') | (method == 'e') | (method == 'max') | (method == 's'):\n",
    "        print(\"the interpretation of this one should be done with caution\")\n",
    "        for i in range(df_corr_search.shape[1]):\n",
    "            corr_matrix = (entropy_corr(df_corr_search) + 1)/2\n",
    "            # set the diagonal of the dataframe to 0 otherwise the max will always be 1\n",
    "            corr_matrix = corr_matrix - np.diag(np.diag(corr_matrix))\n",
    "            corr_matrix['max'] = abs(corr_matrix).max(axis=1)\n",
    "            feature_to_drop = corr_matrix['max'].idxmax()\n",
    "            score.append(corr_matrix['max'].max())\n",
    "            features.append(feature_to_drop)\n",
    "            df_corr_search = df_corr_search.drop(columns=[feature_to_drop])\n",
    "\n",
    "    else:\n",
    "        print(\"method not recognized\")\n",
    "        return\n",
    "\n",
    "    correlation_feature_scores = pd.DataFrame(\n",
    "        {'feature': features, 'score': score})\n",
    "    correlation_feature_scores['d1'] = correlation_feature_scores['score'].diff(\n",
    "    )\n",
    "    correlation_feature_scores['d2'] = correlation_feature_scores['d1'].diff()\n",
    "    elbow_value = correlation_feature_scores['d2'][:-2].idxmax()\n",
    "    if report:\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        plt.plot(correlation_feature_scores['score'], label='score')\n",
    "        plt.plot(correlation_feature_scores['d1'], label='d1')\n",
    "        plt.plot(correlation_feature_scores['d2'], label='d2')\n",
    "        plt.axvline(elbow_value, color='r', linestyle='--', label='elbow')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    if elbow:\n",
    "        return correlation_feature_scores[['feature', 'score']][:elbow_value], correlation_feature_scores[['feature', 'score']][elbow:]\n",
    "    return_df = correlation_feature_scores[['feature', 'score']]\n",
    "    return_df = pd.DataFrame(\n",
    "        {'feature': features, 'score': score}, index=features)\n",
    "    return_df.drop(columns=['feature'], inplace=True)\n",
    "    return return_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_correlated_features(df, report=False, method='linear', elbow=False) -> pd.DataFrame:\n",
    "    features = []\n",
    "    score = []\n",
    "    df_corr_search = df.copy()\n",
    "\n",
    "    if (method == 'linear') | (method == 'spearman') | (method == 'l'):\n",
    "        for i in range(df_corr_search.shape[1]):\n",
    "            corr_matrix = df_corr_search.corr()\n",
    "            # set the diagonal of the dataframe to 0 otherwise the max will always be 1\n",
    "            corr_matrix = corr_matrix - np.diag(np.diag(corr_matrix))\n",
    "            corr_matrix['avg'] = abs(corr_matrix).mean(axis=1)\n",
    "            corr_matrix = corr_matrix.sort_values(by='avg', ascending=False)\n",
    "            feature_to_drop = corr_matrix.index[0]\n",
    "            features.append(feature_to_drop)\n",
    "            score.append(corr_matrix['avg'].iloc[0])\n",
    "            df_corr_search = df_corr_search.drop(columns=[feature_to_drop])\n",
    "    elif (method == 'entropy') | (method == 'e') | (method == 'ent') | (method == 's'):\n",
    "        print(\"the interpretation of this one should be done with caution\")\n",
    "        for i in range(df_corr_search.shape[1]):\n",
    "            corr_matrix = (entropy_corr(df_corr_search) + 1)/2\n",
    "            # set the diagonal of the dataframe to 0 otherwise the max will always be 1\n",
    "            corr_matrix = corr_matrix - np.diag(np.diag(corr_matrix))\n",
    "            corr_matrix['avg'] = abs(corr_matrix).mean(axis=1)\n",
    "            corr_matrix = corr_matrix.sort_values(by='avg', ascending=False)\n",
    "            feature_to_drop = corr_matrix.index[0]\n",
    "            features.append(feature_to_drop)\n",
    "            score.append(corr_matrix['avg'].iloc[0])\n",
    "            df_corr_search = df_corr_search.drop(columns=[feature_to_drop])\n",
    "\n",
    "    else:\n",
    "        print(\"method not recognized\")\n",
    "        return\n",
    "\n",
    "    correlation_feature_scores = pd.DataFrame(\n",
    "        {'feature': features, 'score': score})\n",
    "    correlation_feature_scores['d1'] = correlation_feature_scores['score'].diff(\n",
    "    )\n",
    "    correlation_feature_scores['d2'] = correlation_feature_scores['d1'].diff()\n",
    "    elbow = correlation_feature_scores['d2'][:-2].idxmax()\n",
    "    if report:\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        plt.plot(correlation_feature_scores['score'], label='score')\n",
    "        plt.plot(correlation_feature_scores['d1'], label='d1')\n",
    "        plt.plot(correlation_feature_scores['d2'], label='d2')\n",
    "        plt.axvline(elbow, color='r', linestyle='--', label='elbow')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    if elbow:\n",
    "        return correlation_feature_scores[['feature', 'score']][:elbow], correlation_feature_scores[['feature', 'score']][elbow:]\n",
    "    return_df = correlation_feature_scores[['feature', 'score']]\n",
    "    return_df = pd.DataFrame(\n",
    "        {'feature': features, 'score': score}, index=features)\n",
    "    return_df.drop(columns=['feature'], inplace=True)\n",
    "    return return_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = classify_correlated_feature_maxmin(df_train[num_cols], report=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_correlated_feature_maxmin(\n",
    "    df_train[num_cols], report=True, method='entropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## correlation with target "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### entropy based correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_col_entropy_corr(f1, f2, n_iter=30, n_frac=0.5):\n",
    "    baseline_s_x1 = entropy(f1)\n",
    "    baseline_s_x2 = entropy(f2)\n",
    "\n",
    "    s_x1 = []\n",
    "    s_x2 = []\n",
    "    for i in range(n_iter):\n",
    "        x1 = f1.sample(frac=0.9, random_state=i)\n",
    "        x2 = f2.sample(frac=0.9, random_state=i)\n",
    "        s_x1.append(baseline_s_x1 - entropy(x1)/baseline_s_x1)\n",
    "        s_x2.append(baseline_s_x2 - entropy(x2)/baseline_s_x2)\n",
    "    return np.corrcoef(s_x1, s_x2)[0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi-Square for categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep1.get_code_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_cat = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi2_feature_importance(f1, target, significance_level=0.05, log=False):\n",
    "    \"\"\"\n",
    "    Evaluate if a feature is important to predict the target using the Chi-squared test.\n",
    "\n",
    "    Parameters:\n",
    "        data (pd.DataFrame): The dataset containing the feature and target.\n",
    "        feature (str): The name of the feature column.\n",
    "        target (str): The name of the target column.\n",
    "        significance_level (float): The threshold for statistical significance (default 0.05).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the chi-squared statistic, p-value, and whether the feature is important.\n",
    "    \"\"\"\n",
    "    # Create a contingency table\n",
    "    contingency_table = pd.crosstab(f1, target)\n",
    "\n",
    "    # Perform the Chi-squared test\n",
    "    chi2_stat, p_value, _, _ = chi2_contingency(contingency_table)\n",
    "\n",
    "    total_observations = contingency_table.sum().sum()\n",
    "    rows, cols = contingency_table.shape\n",
    "\n",
    "    cramers_v = np.sqrt(\n",
    "        chi2_stat / (total_observations * min(cols - 1, rows - 1)))\n",
    "\n",
    "    is_important = p_value < significance_level\n",
    "\n",
    "    if log:\n",
    "        if cramers_v < 0.3:\n",
    "            print(f\" {is_important} the test is not significant {cramers_v}\")\n",
    "        elif cramers_v < 0.5:\n",
    "            print(f\" {is_important} the test is weakly significant {cramers_v}\")\n",
    "        elif cramers_v < 0.7:\n",
    "            print(f\" {is_important} the test is moderately significant {cramers_v}\")\n",
    "        elif cramers_v < 1:\n",
    "            print(f\" {is_important} the test is highly significant {cramers_v}\")\n",
    "        else:\n",
    "            print(f\"the test has no sense {cramers_v}\")\n",
    "\n",
    "    # Determine if the feature is important\n",
    "    is_important = p_value < significance_level\n",
    "    return cramers_v, p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function for classification based on correlation with target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to evalueate if a feature is relevant or not, we can use the correlation with the target trough the chi-square test for categorical data and entropy for numerical data. in this test the p-value is used to determine if the feature is relevant or not but it is not the only parameter to be considered. the cramers_v is also important to determine the strength of the correlation especiallli in the case of underrepresented classes like ours. \n",
    "\n",
    "To classify the features based on the correlation with the target we will multiply the coplement of the p-value by the cramers_v. instead of using the cramers_v we will use the sigmoid function of it to give more importance to the features with higher correlation anf havily penalize the features with low correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid function\n",
    "def sigmoid(x, a=20):\n",
    "    return 1 / (1 + np.exp(-a * (x - 0.5)))\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "x = np.linspace(0, 1, 100)\n",
    "y = sigmoid(x)\n",
    "plt.plot(x, y)\n",
    "plt.title('Sigmoid Function')\n",
    "plt.xlabel('x')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_correlation_w_target(f1, target, significance_level=0.05, log=False, method='chi2', parameter_sigmoid=20):\n",
    "\n",
    "    if method == 'chi2':\n",
    "        cramers_v, p_value = chi2_feature_importance(\n",
    "            f1, target, significance_level, log)\n",
    "        score = (1 - p_value)*sigmoid(cramers_v, a=parameter_sigmoid)\n",
    "\n",
    "    elif method == 'entropy' or method == 'ent' or method == 'e' or method == 's':\n",
    "        score = (two_col_entropy_corr(f1, target) + 1) / 2\n",
    "\n",
    "    elif method == 'linear' or method == 'spearman' or method == 'l':\n",
    "        score = np.abs(f1.corr(target))\n",
    "\n",
    "    elif method == 'all':\n",
    "        cramers_v, p_value = chi2_feature_importance(\n",
    "            f1, target, significance_level, log)\n",
    "        try:\n",
    "            score1 = (1 - p_value)*sigmoid(cramers_v, a=parameter_sigmoid)\n",
    "        except Exception as e:\n",
    "            score1 = np.nan\n",
    "        try:\n",
    "            score2 = (two_col_entropy_corr(f1, target) + 1) / 2\n",
    "        except Exception as e:\n",
    "            score2 = np.nan\n",
    "        try:\n",
    "            score3 = np.abs(f1.corr(target))\n",
    "        except Exception as e:\n",
    "            score3 = np.nan\n",
    "        return score1, score2, score3\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_feature_correlation_scores(df, target):\n",
    "    corr_scores = pd.DataFrame()\n",
    "    for col in df.columns:\n",
    "        score1, score2, score3 = classify_correlation_w_target(df[col], target)\n",
    "        corr_scores[col] = [score1, score2, score3]\n",
    "    corr_scores = corr_scores.T\n",
    "    corr_scores.columns = ['chi2', 'entropy', 'linear']\n",
    "\n",
    "    corr_scores.index.name = 'feature'\n",
    "\n",
    "    lin_corr_df = classify_correlated_feature_maxmin(\n",
    "        df_train[num_cols], report=False, method='linear')\n",
    "    ent_corr_df = classify_correlated_feature_maxmin(\n",
    "        df_train[num_cols], report=False, method='entropy')\n",
    "\n",
    "    lin_corr_df = lin_corr_df.columns = ['linear_x']\n",
    "    ent_corr_df = ent_corr_df.columns = ['entropy_x']\n",
    "\n",
    "    corr_scores = pd.concat([lin_corr_df, ent_corr_df, corr_scores], axis=1)\n",
    "\n",
    "    return corr_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with the correlation scores\n",
    "corr_scores = pd.DataFrame()\n",
    "for var in df_train_cat.columns:\n",
    "    print(var)\n",
    "    s1, s2, s3 = classify_correlation_w_target(\n",
    "        df_train[var], y_train, method='all')\n",
    "    corr_scores[var] = [s1, s2, s3]\n",
    "\n",
    "corr_scores = corr_scores.T\n",
    "corr_scores.columns = ['chi2_y', 'entropy_y', 'linear_y']\n",
    "corr_scores.index.name = 'feature'\n",
    "\n",
    "lin_corr_df = classify_correlated_feature_maxmin(\n",
    "    df_train[num_cols], report=False, method='linear')\n",
    "lin_corr_df.columns = ['linear_x']\n",
    "lin_corr_df['linear_x'] = (1 - lin_corr_df['linear_x'])\n",
    "lin_corr_df['linear_x'] = lin_corr_df['linear_x'].abs()\n",
    "s_corr_df = classify_correlated_feature_maxmin(\n",
    "    df_train[num_cols], report=False, method='entropy', elbow=False)\n",
    "s_corr_df.columns = ['entropy_x']\n",
    "s_corr_df['entropy_x'] = (1 - s_corr_df['entropy_x'])\n",
    "s_corr_df['entropy_x'] = s_corr_df['entropy_x'].abs()\n",
    "\n",
    "feature_selection = pd.concat([lin_corr_df, s_corr_df, corr_scores], axis=1)\n",
    "pd.concat([lin_corr_df, s_corr_df, corr_scores], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formula 1 score\n",
    "points_in_formula1 = [25, 18, 15, 12, 10, 8, 6, 4, 2, 1]\n",
    "\n",
    "feature_selection['points'] = np.zeros(feature_selection.shape[0])\n",
    "for col in feature_selection.columns:\n",
    "    x = feature_selection[col].sort_values(ascending=False, inplace=False)\n",
    "    for place in range(10):\n",
    "        feature_selection.loc[x.index[place],\n",
    "                              'points'] += points_in_formula1[place]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first correlation with the target then the correlation between the features\n",
    "\n",
    "col_y = feature_selection.columns.str.contains('_y')\n",
    "col_x = feature_selection.columns.str.contains('_x')\n",
    "\n",
    "# maximum score among the features in col_y\n",
    "\n",
    "feature_selection['max_y'] = feature_selection[col_y].max(axis=1)\n",
    "feature_selection['max_x'] = feature_selection[col_x].max(axis=1)\n",
    "\n",
    "\n",
    "feature_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# choiche maker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a class called choose_features\n",
    "# it gets a dataframe as input and returns the best features\n",
    "\n",
    "class ChooseFeatures:\n",
    "\n",
    "    def init(self, method, n_features):\n",
    "        self.method = method\n",
    "        self.n_features = n_features\n",
    "        self.corr_scores = None\n",
    "\n",
    "    def fit_corr_scores(self,df, target):\n",
    "        corr_scores = pd.DataFrame()\n",
    "        for col in df.columns:\n",
    "            score1, score2, score3 = self.classify_correlation_w_target(df[col], target)\n",
    "            corr_scores[col] = [score1, score2, score3]\n",
    "        corr_scores = corr_scores.T\n",
    "        corr_scores.columns = ['chi2', 'entropy', 'linear']\n",
    "\n",
    "        corr_scores.index.name = 'feature'\n",
    "\n",
    "        lin_corr_df = classify_correlated_feature_maxmin(df_train[num_cols], report=False, method='linear')\n",
    "        ent_corr_df = classify_correlated_feature_maxmin(df_train[num_cols], report=False, method='entropy')\n",
    "\n",
    "        lin_corr_df = lin_corr_df.columns = ['linear_x']\n",
    "        ent_corr_df = ent_corr_df.columns = ['entropy_x']\n",
    "\n",
    "        self.corr_scores = pd.concat([lin_corr_df, ent_corr_df, corr_scores], axis=1)\n",
    "\n",
    "\n",
    "    def formula1_score(self, df_input):\n",
    "        df = df_input.copy()\n",
    "        points_in_formula1 = [25, 18, 15, 12, 10, 8, 6, 4, 2, 1]\n",
    "        df['points'] = np.zeros(df.shape[0])\n",
    "        for col in df.columns:\n",
    "            x = df[col].sort_values(ascending=False, inplace=False)\n",
    "            for place in range(10):\n",
    "                df.loc[x.index[place], 'points'] += points_in_formula1[place]\n",
    "        x = df['points'].sort_values(ascending=False, inplace=False)\n",
    "        return x.index[:self.n_features]\n",
    "    \n",
    "    def y_before_x(self, df):\n",
    "        col_y = df.columns.str.contains('_y')\n",
    "        col_x = df.columns.str.contains('_x')\n",
    "        df['max_y'] = df[col_y].max(axis=1)\n",
    "        df['max_x'] = df[col_x].max(axis=1)\n",
    "        df.sort_values(by=['max_y','max_x'], ascending=False, inplace=True)\n",
    "        #if max_y is the same, then remove the one with the lower max_x\n",
    "        df = df.drop_duplicates(subset='max_y', keep='first')\n",
    "        return df.index[:self.n_features]\n",
    "    \n",
    "    def classify_correlation_w_target(f1, target, significance_level=0.05, log=False, method='chi2', parameter_sigmoid=20):\n",
    "\n",
    "    if method == 'chi2':\n",
    "        cramers_v, p_value = self.chi2_feature_importance(\n",
    "            f1, target, significance_level, log)\n",
    "        score = (1 - p_value)*self.sigmoid(cramers_v, a=parameter_sigmoid)\n",
    "\n",
    "    elif method == 'entropy' or method == 'ent' or method == 'e' or method == 's':\n",
    "        score = (self.two_col_entropy_corr(f1, target) + 1) / 2\n",
    "\n",
    "    elif method == 'linear' or method == 'spearman' or method == 'l':\n",
    "        score = np.abs(f1.corr(target))\n",
    "\n",
    "    elif method == 'all':\n",
    "        cramers_v, p_value = self.chi2_feature_importance(\n",
    "            f1, target, significance_level, log)\n",
    "        try:\n",
    "            score1 = (1 - p_value)*self.sigmoid(cramers_v, a=parameter_sigmoid)\n",
    "        except Exception as e:\n",
    "            score1 = np.nan\n",
    "        try:\n",
    "            score2 = (self.two_col_entropy_corr(f1, target) + 1) / 2\n",
    "        except Exception as e:\n",
    "            score2 = np.nan\n",
    "        try:\n",
    "            score3 = np.abs(f1.corr(target))\n",
    "        except Exception as e:\n",
    "            score3 = np.nan\n",
    "        return score1, score2, score3\n",
    "\n",
    "    return score\n",
    "\n",
    "    def sigmoid(self,x, a=20):\n",
    "        return 1 / (1 + np.exp(-a * (x - 0.5)))\n",
    "    \n",
    "    def entropy(self,feature):\n",
    "        return -np.sum([p*np.log2(p) for p in feature.value_counts(normalize=True)])\n",
    "    \n",
    "    def two_col_entropy_corr(self,f1, f2, n_iter=30, n_frac=0.5):\n",
    "        baseline_s_x1 = self.entropy(f1)\n",
    "        baseline_s_x2 = self.entropy(f2)\n",
    "\n",
    "        s_x1 = []\n",
    "        s_x2 = []\n",
    "        for i in range(n_iter):\n",
    "            x1 = f1.sample(frac=0.9, random_state=i)\n",
    "            x2 = f2.sample(frac=0.9, random_state=i)\n",
    "            s_x1.append(baseline_s_x1 - self.entropy(x1)/baseline_s_x1)\n",
    "            s_x2.append(baseline_s_x2 - self.entropy(x2)/baseline_s_x2)\n",
    "        return np.corrcoef(s_x1, s_x2)[0, 1]\n",
    "    \n",
    "    def chi2_feature_importance(self,f1, target, significance_level=0.05, log=False):\n",
    "        \"\"\"\n",
    "        Evaluate if a feature is important to predict the target using the Chi-squared test.\n",
    "\n",
    "        Parameters:\n",
    "            data (pd.DataFrame): The dataset containing the feature and target.\n",
    "            feature (str): The name of the feature column.\n",
    "            target (str): The name of the target column.\n",
    "            significance_level (float): The threshold for statistical significance (default 0.05).\n",
    "\n",
    "        Returns:\n",
    "            dict: A dictionary containing the chi-squared statistic, p-value, and whether the feature is important.\n",
    "        \"\"\"\n",
    "        # Create a contingency table\n",
    "        contingency_table = pd.crosstab(f1, target)\n",
    "\n",
    "        # Perform the Chi-squared test\n",
    "        chi2_stat, p_value, _, _ = chi2_contingency(contingency_table)\n",
    "\n",
    "        total_observations = contingency_table.sum().sum()\n",
    "        rows, cols = contingency_table.shape\n",
    "\n",
    "        cramers_v = np.sqrt(\n",
    "            chi2_stat / (total_observations * min(cols - 1, rows - 1)))\n",
    "\n",
    "        is_important = p_value < significance_level\n",
    "\n",
    "        if log:\n",
    "            if cramers_v < 0.3:\n",
    "                print(f\" {is_important} the test is not significant {cramers_v}\")\n",
    "            elif cramers_v < 0.5:\n",
    "                print(f\" {is_important} the test is weakly significant {cramers_v}\")\n",
    "            elif cramers_v < 0.7:\n",
    "                print(f\" {is_important} the test is moderately significant {cramers_v}\")\n",
    "            elif cramers_v < 1:\n",
    "                print(f\" {is_important} the test is highly significant {cramers_v}\")\n",
    "            else:\n",
    "                print(f\"the test has no sense {cramers_v}\")\n",
    "\n",
    "        # Determine if the feature is important\n",
    "        is_important = p_value < significance_level\n",
    "        return cramers_v, p_value\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## random forest feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify the features fro most useful to least useful\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "clf.fit(df_train[num_cols], y_train)\n",
    "y_pred = clf.predict(df_valid[num_cols])\n",
    "print('Accuracy: ', accuracy_score(y_valid, y_pred))\n",
    "\n",
    "# get the feature importance\n",
    "feature_importances = clf.feature_importances_\n",
    "feature_importances = pd.Series(\n",
    "    feature_importances, index=df_train[num_cols].columns)\n",
    "feature_importances = feature_importances.sort_values(ascending=False)\n",
    "feature_importances.columns = ['fi_RandomForestClassifier']\n",
    "plt.figure(figsize=(12, 10))\n",
    "feature_importances.plot(kind='bar')\n",
    "plt.title('Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with the correlation scores\n",
    "corr_scores = pd.DataFrame()\n",
    "for var in df_dummies:\n",
    "    print(var)\n",
    "    s1, s2, s3 = classify_correlation_w_target(\n",
    "        df_train[var], y_train, method='all')\n",
    "    corr_scores[var] = [s1, s2, s3]\n",
    "\n",
    "corr_scores = corr_scores.T\n",
    "corr_scores.columns = ['chi2_y', 'entropy_y', 'linear_y']\n",
    "# name teh index column\n",
    "corr_scores.index.name = 'feature'\n",
    "lin_corr_df = classify_correlated_feature_maxmin(\n",
    "    df_train[num_cols], report=False, method='linear')\n",
    "lin_corr_df.columns = ['linear_x']\n",
    "s_corr_df = classify_correlated_feature_maxmin(\n",
    "    df_train[num_cols], report=False, method='entropy', elbow=False)\n",
    "s_corr_df.columns = ['entropy_x']\n",
    "\n",
    "\n",
    "feature_selection = pd.concat([lin_corr_df, s_corr_df, corr_scores], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.Series(\n",
    "    feature_importances, index=df_train[num_cols].columns)\n",
    "feature_importances = feature_importances.sort_values(ascending=False)\n",
    "plt.figure(figsize=(12, 10))\n",
    "feature_importances.plot(kind='bar')\n",
    "plt.title('Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "num_cols\n",
    "rfe = RFE(estimator=model, n_features_to_select=4)\n",
    "rfe.fit(df_train[num_cols], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe_cols = df_train[num_cols].columns[rfe.support_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[num_cols].columns[rfe.support_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model.fit(df_train[rfe_cols], y_train)\n",
    "y_pred = model.predict(df_valid[rfe_cols])\n",
    "accuracy_score(y_valid, y_pred)\n",
    "\n",
    "\n",
    "print(classification_report(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## columns chosen by our methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representation problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add noise to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data_from_existing(existing_data, noise_level=0.1, random_state=None):\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "\n",
    "    # Add Gaussian noise to each feature in the existing data\n",
    "    synthetic_data = existing_data.copy()\n",
    "    for column in synthetic_data.columns:\n",
    "        noise = np.random.normal(\n",
    "            0, noise_level, size=synthetic_data[column].shape)\n",
    "        synthetic_data[column] += noise\n",
    "\n",
    "    return synthetic_data\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# Assuming `existing_data` is a DataFrame with your existing data\n",
    "existing_data = pd.DataFrame({\n",
    "    'f1': np.random.rand(100),\n",
    "    'f2': np.random.rand(100),\n",
    "    'target': np.random.rand(100)\n",
    "})\n",
    "\n",
    "synthetic_data = generate_synthetic_data_from_existing(\n",
    "    existing_data, noise_level=0.1, random_state=42)\n",
    "print(synthetic_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def representation_treshold(len_y, unique_y):\n",
    "    reurn(len_y/unique_y)-(m.sqrt(len_y/8 * 100)/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "def generate_synthetic_data_using_smote(existing_data, target_column, sampling_strategy='auto', random_state=None):\n",
    "    # Separate features and target\n",
    "    X = existing_data.drop(columns=[target_column])\n",
    "    y = existing_data[target_column]\n",
    "\n",
    "    # Apply SMOTE to generate synthetic data\n",
    "    smote = SMOTE(sampling_strategy=sampling_strategy,\n",
    "                  random_state=random_state)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "    # Combine resampled features and target into a DataFrame\n",
    "    resampled_data = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "    resampled_data[target_column] = y_resampled\n",
    "\n",
    "    return resampled_data\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# Assuming `existing_data` is a DataFrame with your existing data\n",
    "existing_data = pd.DataFrame({\n",
    "    'f1': np.random.rand(100),\n",
    "    'f2': np.random.rand(100),\n",
    "    # Imbalanced target\n",
    "    'target': np.random.choice([0, 1], size=100, p=[0.9, 0.1])\n",
    "})\n",
    "\n",
    "synthetic_data = generate_synthetic_data_using_smote(\n",
    "    existing_data, target_column='target', random_state=42)\n",
    "print(synthetic_data.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
