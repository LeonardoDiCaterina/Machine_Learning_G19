{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import robust scaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "#import one hot encoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "class PreProcessor1:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.status = 'train'\n",
    "        self.scaler = RobustScaler()\n",
    "        self.version = '1.0 10 dec 2024 frango'\n",
    "        self.encoder = OneHotEncoder()\n",
    "        \n",
    "        self.date_cols = []\n",
    "        self.desc_cols = []\n",
    "        self.code_cols = []\n",
    "        self.casting_methods = []\n",
    "        self.casting_list = [\n",
    "    ('Age at Injury',                       'Int64'),\n",
    "    ('Average Weekly Wage',                 'float64'),\n",
    "    # ('WCB Decision',                        'remove'),\n",
    "    ('Alternative Dispute Resolution',      'string-U-nan'),\n",
    "    ('District Name',                       'none'),\n",
    "    ('Carrier Name',                        'string'),\n",
    "    ('IME-4 Count',                         'Int64'),\n",
    "    ('COVID-19 Indicator',                  'string'),\n",
    "    ('Number of Dependents',                'Int64'),\n",
    "    ('Carrier Type',                         '.str[:2]'),\n",
    "    ('Medical Fee Region',                   'string'),\n",
    "    ('County of Injury',                    'string'),\n",
    "    ('Agreement Reached',                   'Int64'),\n",
    "    ('Attorney/Representative',             'string'),\n",
    "    ('Birth Year',                          'Int64'),\n",
    "    ('Gender',                              'string'),\n",
    "    ('Zip Code',                            '.str[0:5]'),\n",
    "]\n",
    "        self.casted_cols = []\n",
    "        self.transformation_list = [\n",
    "    ('Age at Injury',                   'none',             '-'),\n",
    "    ('Average Weekly Wage',             'log',              'log_Average Weekly Wage'),\n",
    "    # ('WCB Decision',                    'none',             '-'),\n",
    "    ('Alternative Dispute Resolution',  'dummy-YN',         'Alternative Dispute Resolution'),\n",
    "    ('District Name',                   'none',             '-'),\n",
    "    ('Carrier Name',                    'none',             '-'),\n",
    "    ('IME-4 Count',                     'none',             '-'),\n",
    "    ('COVID-19 Indicator',              'dummy-YN',         'COVID-19 Indicator'),\n",
    "    ('Number of Dependents',            'none',             '-'),\n",
    "    ('Carrier Type',                    'oneHot',           '-OneHot'),\n",
    "    ('Carrier Type',                    'freq_encode',      'fe_Carrier Type'),\n",
    "    ('Medical Fee Region',              'oneHot',           '-oneHot'),\n",
    "    ('Medical Fee Region',              'freq_encode',      'fe_Medical Fee Region'),\n",
    "    ('County of Injury',                'oneHot',           '-oneHot'),\n",
    "    ('County of Injury',                'freq_encode',      'fe_County of Injury'),\n",
    "    ('Agreement Reached',               'none',             '-oneHot'),\n",
    "    ('Attorney/Representative',         'dummy-YN',         'Attorney/Representative'),\n",
    "    ('Birth Year',                      'subtract_1900',    'Age'),\n",
    "    ('Gender',                          'oneHot',           '-oneHot'),\n",
    "]     \n",
    "        self.transformed_cols = []\n",
    "        self.fillna_list = []\n",
    "\n",
    "\n",
    "# ------------------------ internal functions\n",
    "    def update_status(self, status):\n",
    "        if status in ['train', 'valid', 'test']:\n",
    "            self.status = status\n",
    "        else:  \n",
    "            print('Unknown status')\n",
    "    \n",
    "    def __str__(self):\n",
    "        return (f\"PreProcessor1: {self.status}\" +\n",
    "            f\"scaler: {self.scaler}\" +\n",
    "                f\"version: {self.version}\")\n",
    "        \n",
    "        \n",
    "       \n",
    "        \n",
    "# ------------------------casting part\n",
    "        \n",
    "        \n",
    "    def update_casted_cols(self):     \n",
    "        self.casted_cols = [col for col, method in self.casting_list]\n",
    "\n",
    "    def update_casting_methods(self):\n",
    "        self.casting_methods = [method for col, method in self.casting_list]\n",
    "\n",
    "    def set_castings(self,df):\n",
    "        cols = df.columns\n",
    "        columns_to_be_casted = set(cols) - set(self.casted_cols)\n",
    "    \n",
    "        date_cols = []\n",
    "        date_cols.extend([x for x in df_cols if 'Date' in x])\n",
    "        for col in date_cols:\n",
    "            self.append_casting(col, 'string')\n",
    "        columns_to_be_casted = columns_to_be_casted - set(date_cols)\n",
    "    \n",
    "        desc_cols = []\n",
    "        desc_cols.extend([x for x in df_cols if 'Description' in x])\n",
    "        for col in desc_cols:\n",
    "            self.append_casting(col, 'remove')\n",
    "        columns_to_be_casted = columns_to_be_casted - set(desc_cols)\n",
    "    \n",
    "        code_cols = []\n",
    "        code_cols.extend([x for x in df_cols if 'Code' in x])\n",
    "        for col in code_cols:\n",
    "            self.append_casting(col, 'int64')\n",
    "        columns_to_be_casted = columns_to_be_casted - set(code_cols)\n",
    "    \n",
    "        if len(columns_to_be_casted) > 0:\n",
    "            print ('Columns that are not casted:')\n",
    "            for col in columns_to_be_casted:\n",
    "                try:\n",
    "                    print (f'-{col}: {df[col].dtype}')\n",
    "                except Exception as e: \n",
    "                    print(e)\n",
    "                    print (f'-{col}: not found')\n",
    "\n",
    "    def look_for_feature_casting(self,feature):\n",
    "        i = 0\n",
    "        for col, method in self.casting_list:\n",
    "            if col == feature:\n",
    "                print(f'-Column {col} is casted as {method} at index {i}')\n",
    "                return method\n",
    "            i += 1\n",
    "        return None\n",
    "\n",
    "    def update_casting_list(self,feature,method):\n",
    "        i = 0\n",
    "        for col, method in self.casting_list:\n",
    "            if col == feature:\n",
    "                self.casting_list[i] = (feature, method)\n",
    "                return\n",
    "            i += 1\n",
    "        #print(f'-Column {feature} not found in casting list. Adding it now')\n",
    "        self.append_casting(feature, method)\n",
    "    \n",
    "    def append_casting(self,feature,method):\n",
    "        self.casting_list.append((feature, method))\n",
    "    \n",
    "        \n",
    "    def cast_pipeline(self,df):\n",
    "        for col, method in self.casting_list:\n",
    "            if method == 'Int64':\n",
    "                df = self.cast_Int64(df, col)\n",
    "            elif method == 'float64':\n",
    "                df = self.cast_Float64(df, col)\n",
    "            elif method == 'string':\n",
    "                df = self.cast_string(df, col)\n",
    "            elif method == 'string-U-nan':\n",
    "                df = self.cast_string_U_nan(df, col)\n",
    "            elif method == '.str[:2]':\n",
    "                df = self.cast_string_2(df, col)\n",
    "            elif method == 'datetime64':\n",
    "                df = self.cast_datetime64(df, col)\n",
    "            elif method == 'remove':\n",
    "                df = self.cast_remove(df, col)\n",
    "            elif method == '.str[0:5]':\n",
    "                df = self.cast_string_5(df, col)\n",
    "                \n",
    "            elif method == 'none':\n",
    "                pass\n",
    "            else:\n",
    "                print(f'Unknown method {method} for column {col}')\n",
    "        return df\n",
    "\n",
    "    def cast_Int64(self, df, col):\n",
    "        try:\n",
    "            df[col] = df[col].astype('Int64')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'-Column {col} not found')\n",
    "        return df\n",
    "\n",
    "\n",
    "    def cast_Float64(self, df, col):\n",
    "        try:\n",
    "            df[col] = df[col].astype('float64')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'-Column {col} not found')\n",
    "\n",
    "        return df\n",
    "\n",
    "    def cast_string(self, df, col):\n",
    "        try:\n",
    "            df[col] = df[col].astype('string')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'-Column {col} not found')\n",
    "\n",
    "        return df\n",
    "\n",
    "    def cast_string_U_nan(self, df, col):\n",
    "        try:\n",
    "            df[col] = df[col].astype('string')\n",
    "            df[col] = df[col].replace('U', 'N')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'-Column {col} not found')\n",
    "        return df\n",
    "\n",
    "    def cast_string_2(self, df, col):\n",
    "        try:\n",
    "            df[col] = df[col].astype('string')\n",
    "            df[col] = df[col].str[:2]\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'-Column {col} not found')\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def cast_string_5(self, df, col):\n",
    "        try:\n",
    "            df[col] = df[col].astype('string')\n",
    "            df[col] = df[col].fillna('00000')\n",
    "            df[col] = df[col].str[:5]\n",
    "            df[col] = df[col].apply(lambda x: int(x) if x.isnumeric() else 0)\n",
    "            #df[col] = df[col].astype('string')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'-Column {col} not found')\n",
    "        \n",
    "        return df\n",
    "\n",
    "\n",
    "    def cast_datetime64(self, df, col):\n",
    "        try:\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'-Column {col} not found')\n",
    "        \n",
    "        return df\n",
    "\n",
    "\n",
    "    def cast_remove(self, df, col):\n",
    "        try:\n",
    "            df.drop(columns=[col], inplace=True)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'-Column {col} not found')\n",
    "        return df\n",
    "    \n",
    "    \n",
    "# ---------------------------------encoding part\n",
    "\n",
    "    def update_transformed_cols(self):\n",
    "        self.transformed_cols = [new_col for col, method, new_col in self.transformation_list]\n",
    "        if len(self.transformed_cols) > len(set(self.transformed_cols)):\n",
    "            print('Warning: there are duplicates in the transformation list')\n",
    "    \n",
    "    def update_transformation_methods(self):\n",
    "        self.transformation_methods = set([method for col, method, new_col in self.transformation_list])\n",
    "        \n",
    "    def look_for_feature_transformation(self,feature):\n",
    "        i = 0\n",
    "        result = []\n",
    "        for col, method, new_col in self.transformation_list:   \n",
    "            if col == feature:\n",
    "                print(f'-Column {col} is transformed in {new_col} trough {method} at index {i}')\n",
    "                result.append((method, new_col, i))\n",
    "            i += 1\n",
    "        print(f'-Column {feature} not found in transformation list')\n",
    "        return result\n",
    "    \n",
    "    def look_for_new_col(self,feature):\n",
    "        i = 0\n",
    "        for col, method, new_col in self.transformation_list:\n",
    "            if col == feature:\n",
    "                print(f'-Column {new_col} is made from {method} at index {i}')\n",
    "                return new_col\n",
    "            i += 1\n",
    "        print(f'-Column {feature} not found in transformation list')\n",
    "        return None\n",
    "    \n",
    "    def append_transformation(self,feature,method,new_col):\n",
    "        self.transformation_list.append((feature, method, new_col))\n",
    "\n",
    "\n",
    "    def transformation_pipeline(self,df):\n",
    "        for col, method, new_col in self.transformation_list:\n",
    "            print(f'-------Column {col} to {new_col}')\n",
    "            if method == 'log':\n",
    "                df = self.transformation_log(df, col, new_col)\n",
    "            elif method == 'subtract_1900':\n",
    "                df = self.transformation_subtract_1900(df, col, new_col)\n",
    "            elif method == 'dummy-YN':\n",
    "                df = self.transformation_dummy_yn(df, col, new_col)\n",
    "            elif method == 'freq_encode':\n",
    "                df = self.transformation_freq_encode(df, col, new_col)\n",
    "            elif method == 'oneHot':\n",
    "                df = self.transformation_oneHot(df, col, new_col)\n",
    "            elif method == '.str[5:7]':\n",
    "                df = self.transformation_str57(df, col, new_col)\n",
    "            elif method == '.str[0:4]':\n",
    "                df = self.transformation_str04(df, col, new_col)\n",
    "            elif method == '.str[0:5]':\n",
    "                df = self.transformation_str05(df, col, new_col)\n",
    "            elif method == 'remove':\n",
    "                df = self.transformation_remove(df, col)\n",
    "            elif method == 'none':\n",
    "                pass\n",
    "            else:\n",
    "                print(f'Unknown method {method} for column {col}')\n",
    "        return df     \n",
    "\n",
    "\n",
    "    \n",
    "    def set_transformations(self,df):\n",
    "        cols = df.columns\n",
    "        self.update_transformed_cols()\n",
    "        columns_to_be_transformed = set(cols) - set(self.transformed_cols)\n",
    "    \n",
    "        date_cols = []\n",
    "        date_cols.extend([x for x in df_cols if 'Date' in x])\n",
    "        for col in date_cols:\n",
    "            col_year = col.replace('Date', 'Year')\n",
    "            col_month = col.replace('Date', 'Month')\n",
    "            self.append_transformation(col, '.str[5:7]', col_month)\n",
    "            self.append_transformation(col, '.str[0:4]', col_year)\n",
    "            self.append_transformation(col, 'remove', '')\n",
    "        columns_to_be_transformed = columns_to_be_transformed - set(date_cols)\n",
    "            \n",
    "        code_cols = []\n",
    "        code_cols.extend([x for x in df_cols if 'Code' in x])\n",
    "        for col in code_cols:\n",
    "            self.append_transformation(col, 'oneHot', '-one')\n",
    "            self.append_transformation(col, 'freq_encode', 'fe_'+col)\n",
    "        \n",
    "                    \n",
    "            \n",
    "        \n",
    "        columns_to_be_transformed = columns_to_be_transformed - set(code_cols)\n",
    "    \n",
    "        if len(columns_to_be_transformed) > 0:\n",
    "            print ('Columns that are not transformed:')\n",
    "            for col in columns_to_be_transformed:\n",
    "                try:\n",
    "                    print (f'-{col}: {df[col].dtype}')\n",
    "                except Exception as e: \n",
    "                    print(e)\n",
    "                    print (f'-{col}: not found')\n",
    "        return df\n",
    "                    \n",
    "                    \n",
    "    def transformation_log(self, df, col, new_col):\n",
    "        try:\n",
    "            df[col].fillna(0, inplace=True)\n",
    "            df[new_col] = np.log(df[col] + 1)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'Column {col} not found')\n",
    "        return df\n",
    "\n",
    "\n",
    "    def transformation_subtract_1900(self, df, col, new_col):\n",
    "        try:\n",
    "            df[new_col] = df[col] - 1900\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'Column {col} not found')\n",
    "        return df\n",
    "\n",
    "\n",
    "    def transformation_dummy_yn(self, df, col, new_col):\n",
    "        try:\n",
    "            df[new_col] = df[col].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'Column {col} not found')\n",
    "        return df\n",
    "\n",
    "\n",
    "    def transformation_freq_encode(self, df, col, new_col):\n",
    "        try:\n",
    "            freq = df[col].value_counts(normalize=True)\n",
    "            df.loc[:, new_col] = df[col].map(freq)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'Column {col} not found')\n",
    "        return df\n",
    "\n",
    "\n",
    "    def transformation_encode(self, df, col, new_col):\n",
    "        try:\n",
    "            df[new_col], _ = df[col].factorize()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'Column {col} not found')\n",
    "        return df\n",
    "\n",
    "\n",
    "    def transformation_str57(self, df, col, new_col):\n",
    "        try:\n",
    "            df[new_col] = df[col].str[5:7].astype('Int64')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'Column {col} not found')\n",
    "        \n",
    "        return df\n",
    "\n",
    "\n",
    "    def transformation_str04(self, df, col, new_col):\n",
    "        try:\n",
    "            df[new_col] = df[col].str[0:4].astype('Int64')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'Column {col} not found')\n",
    "        return df\n",
    "    \n",
    "    def transformation_str05(self, df, col, new_col):\n",
    "        try:\n",
    "            df[new_col] = df[col].str[0:5]\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'Column {col} not found')\n",
    "        return df\n",
    "    def transformation_remove(self, df, col):\n",
    "        try:\n",
    "            df.drop(columns=[col], inplace=True)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'Column {col} not found')\n",
    "        return df\n",
    "    \n",
    "    def encoder_fit(pr, X):\n",
    "        pr.encoder.fit(X)\n",
    "        return pr\n",
    "\n",
    "    def encoder_transform(pr, X):\n",
    "        Xcolumns = X.columns\n",
    "        X = pr.encoder.transform(X)\n",
    "        X = pd.DataFrame(X.toarray(), columns=pr.encoder.get_feature_names_out(Xcolumns))\n",
    "        return X\n",
    "    \n",
    "    def transformation_oneHot(self, df, col, new_col):\n",
    "        try:\n",
    "            self.encoder.fit(df[[col]])\n",
    "            X = self.encoder.transform(df[[col]])\n",
    "            X = pd.DataFrame(X.toarray(), columns=self.encoder.get_feature_names_out([col]))\n",
    "            df = pd.concat([df, X], axis=1)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'Column {col} not found')\n",
    "        return df\n",
    "    \n",
    "    def update_transformation_list(self,feature,method,new_col):\n",
    "        i = 0\n",
    "        for col, method, new_col in self.transformation_list:\n",
    "            if col == feature:\n",
    "                self.transformation_list[i] = (feature, method, new_col)\n",
    "                return\n",
    "            i += 1\n",
    "            \n",
    "    \n",
    "    def add_transformation(self,feature,method,new_col):\n",
    "        self.transformation_list.append((feature, method, new_col))\n",
    "        \n",
    "\n",
    "# --------------------------------- fill the missing values\n",
    "\n",
    "    def update_fillna_list(self,df):\n",
    "        self.fillna_list = [(col, 'median') for col in df.columns if df[col].dtype in ['Int64', 'int64', 'float64','Float64']]\n",
    "    def fillna_pipeline(self,df):\n",
    "        for col,method in self.fillna_list:\n",
    "            if method == 'median':\n",
    "                df = self.fillna_median(df, col)\n",
    "            elif method == 'mode':\n",
    "                df = self.fillna_mode(df, col)\n",
    "            elif method == 'mean':\n",
    "                df = self.fillna_mean(df, col)\n",
    "            elif method == 'zero':\n",
    "                df = self.fillna_zero(df, col)\n",
    "            else:\n",
    "                print(f'Unknown method {method} for column {col}')\n",
    "                \n",
    "        return df\n",
    "\n",
    "    def fillna_median(self, df, col):\n",
    "        try:\n",
    "            df[col].fillna(df[col].median(), inplace=True)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'Column {col} not found')\n",
    "        return df\n",
    "    \n",
    "    def fillna_mode(self, df, col):\n",
    "        try:\n",
    "            df[col].fillna(df[col].mode(), inplace=True)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'Column {col} not found')\n",
    "        return df\n",
    "\n",
    "    def fillna_mean(self, df, col):\n",
    "        try:\n",
    "            df[col].fillna(df[col].mean(), inplace=True)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'Column {col} not found')\n",
    "        return df\n",
    "\n",
    "    def fillna_zero(self, df, col):\n",
    "        try:\n",
    "            df[col].fillna(0, inplace=True)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f'Column {col} not found')\n",
    "        return df\n",
    "    \n",
    "    def look_for_fillna_method(self,feature):\n",
    "        i = 0\n",
    "        for col, method in self.fillna_list:\n",
    "            if col == feature:\n",
    "                print(f'-Column {col} is filled with {method} at index {i}')\n",
    "                return method\n",
    "            i += 1\n",
    "        print(f'-Column {feature} not found in fillna list')\n",
    "        return None\n",
    "    \n",
    "    def change_fillna_method(self,feature,method):\n",
    "        i = 0\n",
    "        for col, method in self.fillna_list:\n",
    "            if col == feature:\n",
    "                self.fillna_list[i] = (feature, method)\n",
    "                return\n",
    "            i += 1\n",
    "        print(f'-Column {feature} not found in fillna list')\n",
    "        self.add_fillna(feature, method)\n",
    "        \n",
    "    def add_fillna(self,feature,method):      \n",
    "        self.fillna_list.append((feature, method))\n",
    "# --------------------------------- scaling part\n",
    "    def scaling_fit (self,sd):\n",
    "        self.scaler.fit(sd)\n",
    "        return self\n",
    "    \n",
    "    def scaling_transform(self,sd):\n",
    "        sd = self.scaler.transform(sd)\n",
    "        sd = pd.DataFrame(sd, columns = sd.columns)\n",
    "        return sd\n",
    "\n",
    "    def scaling_pipeline(self,sd):\n",
    "        \n",
    "        if self.status == 'train':\n",
    "            self.scaler.fit(sd)       \n",
    "            sd_columns = sd.columns \n",
    "        sd = self.scaler.transform(sd)\n",
    "        sd = pd.DataFrame(sd, columns = sd_columns)\n",
    "        return sd\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# supppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# install libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data\n",
    "train_path = \"../Data/train_data.csv\"\n",
    "test_path = \"../Data/test_data.csv\"\n",
    "# Replace 'Column29' with the actual column name that has mixed types\n",
    "dtype = {'Column29': 'str'}\n",
    "\n",
    "train = pd.read_csv(train_path, index_col='Claim Identifier',\n",
    "                    dtype=dtype, low_memory=False)\n",
    "test = pd.read_csv(test_path, index_col='Claim Identifier',\n",
    "                   dtype=dtype, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all the rows that have missing values in Claim Injury Type\n",
    "train = train.dropna(subset=['Claim Injury Type'])\n",
    "y = train['Claim Injury Type']\n",
    "y_str = y.str[:1]\n",
    "y_int = y_str.astype(int)\n",
    "X = train.drop(columns=['Claim Injury Type'])\n",
    "# partition the data X, y and y_2bin\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y_int, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Accident Date', 'Age at Injury', 'Alternative Dispute Resolution',\n",
       "       'Assembly Date', 'Attorney/Representative', 'Average Weekly Wage',\n",
       "       'Birth Year', 'C-2 Date', 'C-3 Date', 'Carrier Name', 'Carrier Type',\n",
       "       'Claim Injury Type', 'County of Injury', 'COVID-19 Indicator',\n",
       "       'District Name', 'First Hearing Date', 'Gender', 'IME-4 Count',\n",
       "       'Industry Code', 'Industry Code Description', 'Medical Fee Region',\n",
       "       'OIICS Nature of Injury Description', 'WCIO Cause of Injury Code',\n",
       "       'WCIO Cause of Injury Description', 'WCIO Nature of Injury Code',\n",
       "       'WCIO Nature of Injury Description', 'WCIO Part Of Body Code',\n",
       "       'WCIO Part Of Body Description', 'Zip Code', 'Agreement Reached',\n",
       "       'WCB Decision', 'Number of Dependents'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr = PreProcessor1()\n",
    "print(\"------------updates---------------------\")\n",
    "pr.update_casted_cols()\n",
    "print(\"------------set_casting---------------------\")\n",
    "pr.set_castings(X_train)\n",
    "print(\"------------pipeline---------------------\")\n",
    "df_train = pr.cast_pipeline(X_train)\n",
    "print(\"df after cast_pipeline:\", df_train.shape)\n",
    "print(\"------------updates---------------------\")\n",
    "pr.update_transformation_methods()\n",
    "print(\"------------set_transform---------------------\")\n",
    "pr.set_transformations(df_train)\n",
    "print(f\"df after set_transformations: {df_train.shape}\")\n",
    "print(\"------------pipeline---------------------\")\n",
    "df_train = pr.transformation_pipeline(df_train)\n",
    "print(\"df after transformation_pipeline:\", df_train.shape)\n",
    "print(\"------------fillna---------------------\")\n",
    "pr.update_fillna_list(df_train)   \n",
    "df_train = pr.fillna_pipeline(df_train)\n",
    "print(\"df after fillna_pipeline:\", df_train.shape)\n",
    "print(\"------------scaling---------------------\")\n",
    "num_cols = df_train.select_dtypes(include=['Int64', 'int64', 'float64','Float64']).columns\n",
    "df_train[num_cols] = pr.scaling_pipeline(df_train[num_cols])\n",
    "\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_int = y.str[:1].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature mutual correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = df_train.select_dtypes(include=['float64', 'int64']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corr_matrix = df_train[num_cols].corr()\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix, annot=False, cmap='magma')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### entropy based correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(feature):\n",
    "    return -np.sum([p*np.log2(p) for p in feature.value_counts(normalize=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_corr(df, n_iter=30, n_frac=0.5):\n",
    "\n",
    "    entropy_matrix = []\n",
    "    for col in df.columns:\n",
    "        s = entropy(df[col])\n",
    "        delta_entropy = []\n",
    "        for i in range(n_iter):\n",
    "            x = df[col].sample(frac=n_frac, random_state=i)\n",
    "            delta_entropy.append(s - entropy(x) / s)\n",
    "        entropy_matrix.append(delta_entropy)\n",
    "    entropy_matrix = pd.DataFrame(entropy_matrix, index=df.columns)\n",
    "    return entropy_matrix.T.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_corr_matrix = entropy_corr(df_train[num_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap((entropy_corr_matrix + 1)/2, annot=False, cmap='magma')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### recursive function for classification based on mutual correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this function classifies the features based on their mutual correlation. Once the most ocorrelated features are found, the function removes it and repeats the process until the correlation.\n",
    "\n",
    "I'm not a big fan of this methd but let's see"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_correlated_feature_maxmin(df, report=False, method='linear', elbow=False) -> pd.DataFrame:\n",
    "    features = []\n",
    "    score = []\n",
    "    df_corr_search = df.copy()\n",
    "\n",
    "    if (method == 'linear') | (method == 'spearman') | (method == 'l'):\n",
    "        for i in range(df_corr_search.shape[1]):\n",
    "            corr_matrix = df_corr_search.corr()\n",
    "            # set the diagonal of the dataframe to 0 otherwise the max will always be 1\n",
    "            corr_matrix = corr_matrix - np.diag(np.diag(corr_matrix))\n",
    "            corr_matrix['max'] = abs(corr_matrix).max(axis=1)\n",
    "            feature_to_drop = corr_matrix['max'].idxmax()\n",
    "            score.append(corr_matrix['max'].max())\n",
    "            features.append(feature_to_drop)\n",
    "            df_corr_search = df_corr_search.drop(columns=[feature_to_drop])\n",
    "    elif (method == 'entropy') | (method == 'e') | (method == 'max') | (method == 's'):\n",
    "        print(\"the interpretation of this one should be done with caution\")\n",
    "        for i in range(df_corr_search.shape[1]):\n",
    "            corr_matrix = (entropy_corr(df_corr_search) + 1)/2\n",
    "            # set the diagonal of the dataframe to 0 otherwise the max will always be 1\n",
    "            corr_matrix = corr_matrix - np.diag(np.diag(corr_matrix))\n",
    "            corr_matrix['max'] = abs(corr_matrix).max(axis=1)\n",
    "            feature_to_drop = corr_matrix['max'].idxmax()\n",
    "            score.append(corr_matrix['max'].max())\n",
    "            features.append(feature_to_drop)\n",
    "            df_corr_search = df_corr_search.drop(columns=[feature_to_drop])\n",
    "\n",
    "    else:\n",
    "        print(\"method not recognized\")\n",
    "        return\n",
    "\n",
    "    correlation_feature_scores = pd.DataFrame(\n",
    "        {'feature': features, 'score': score})\n",
    "    correlation_feature_scores['d1'] = correlation_feature_scores['score'].diff(\n",
    "    )\n",
    "    correlation_feature_scores['d2'] = correlation_feature_scores['d1'].diff()\n",
    "    elbow_value = correlation_feature_scores['d2'][:-2].idxmax()\n",
    "    if report:\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        plt.plot(correlation_feature_scores['score'], label='score')\n",
    "        plt.plot(correlation_feature_scores['d1'], label='d1')\n",
    "        plt.plot(correlation_feature_scores['d2'], label='d2')\n",
    "        plt.axvline(elbow_value, color='r', linestyle='--', label='elbow')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    if elbow:\n",
    "        return correlation_feature_scores[['feature', 'score']][:elbow_value], correlation_feature_scores[['feature', 'score']][elbow:]\n",
    "    return_df = correlation_feature_scores[['feature', 'score']]\n",
    "    return_df = pd.DataFrame(\n",
    "        {'feature': features, 'score': score}, index=features)\n",
    "    return_df.drop(columns=['feature'], inplace=True)\n",
    "    return return_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_correlated_features(df, report=False, method='linear', elbow=False) -> pd.DataFrame:\n",
    "    features = []\n",
    "    score = []\n",
    "    df_corr_search = df.copy()\n",
    "\n",
    "    if (method == 'linear') | (method == 'spearman') | (method == 'l'):\n",
    "        for i in range(df_corr_search.shape[1]):\n",
    "            corr_matrix = df_corr_search.corr()\n",
    "            # set the diagonal of the dataframe to 0 otherwise the max will always be 1\n",
    "            corr_matrix = corr_matrix - np.diag(np.diag(corr_matrix))\n",
    "            corr_matrix['avg'] = abs(corr_matrix).mean(axis=1)\n",
    "            corr_matrix = corr_matrix.sort_values(by='avg', ascending=False)\n",
    "            feature_to_drop = corr_matrix.index[0]\n",
    "            features.append(feature_to_drop)\n",
    "            score.append(corr_matrix['avg'].iloc[0])\n",
    "            df_corr_search = df_corr_search.drop(columns=[feature_to_drop])\n",
    "    elif (method == 'entropy') | (method == 'e') | (method == 'ent') | (method == 's'):\n",
    "        print(\"the interpretation of this one should be done with caution\")\n",
    "        for i in range(df_corr_search.shape[1]):\n",
    "            corr_matrix = (entropy_corr(df_corr_search) + 1)/2\n",
    "            # set the diagonal of the dataframe to 0 otherwise the max will always be 1\n",
    "            corr_matrix = corr_matrix - np.diag(np.diag(corr_matrix))\n",
    "            corr_matrix['avg'] = abs(corr_matrix).mean(axis=1)\n",
    "            corr_matrix = corr_matrix.sort_values(by='avg', ascending=False)\n",
    "            feature_to_drop = corr_matrix.index[0]\n",
    "            features.append(feature_to_drop)\n",
    "            score.append(corr_matrix['avg'].iloc[0])\n",
    "            df_corr_search = df_corr_search.drop(columns=[feature_to_drop])\n",
    "\n",
    "    else:\n",
    "        print(\"method not recognized\")\n",
    "        return\n",
    "\n",
    "    correlation_feature_scores = pd.DataFrame(\n",
    "        {'feature': features, 'score': score})\n",
    "    correlation_feature_scores['d1'] = correlation_feature_scores['score'].diff(\n",
    "    )\n",
    "    correlation_feature_scores['d2'] = correlation_feature_scores['d1'].diff()\n",
    "    elbow = correlation_feature_scores['d2'][:-2].idxmax()\n",
    "    if report:\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        plt.plot(correlation_feature_scores['score'], label='score')\n",
    "        plt.plot(correlation_feature_scores['d1'], label='d1')\n",
    "        plt.plot(correlation_feature_scores['d2'], label='d2')\n",
    "        plt.axvline(elbow, color='r', linestyle='--', label='elbow')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    if elbow:\n",
    "        return correlation_feature_scores[['feature', 'score']][:elbow], correlation_feature_scores[['feature', 'score']][elbow:]\n",
    "    return_df = correlation_feature_scores[['feature', 'score']]\n",
    "    return_df = pd.DataFrame(\n",
    "        {'feature': features, 'score': score}, index=features)\n",
    "    return_df.drop(columns=['feature'], inplace=True)\n",
    "    return return_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = classify_correlated_feature_maxmin(df_train[num_cols], report=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_correlated_feature_maxmin(\n",
    "    df_train[num_cols], report=True, method='entropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## correlation with target "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### entropy based correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_col_entropy_corr(f1, f2, n_iter=30, n_frac=0.5):\n",
    "    baseline_s_x1 = entropy(f1)\n",
    "    baseline_s_x2 = entropy(f2)\n",
    "\n",
    "    s_x1 = []\n",
    "    s_x2 = []\n",
    "    for i in range(n_iter):\n",
    "        x1 = f1.sample(frac=0.9, random_state=i)\n",
    "        x2 = f2.sample(frac=0.9, random_state=i)\n",
    "        s_x1.append(baseline_s_x1 - entropy(x1)/baseline_s_x1)\n",
    "        s_x2.append(baseline_s_x2 - entropy(x2)/baseline_s_x2)\n",
    "    return np.corrcoef(s_x1, s_x2)[0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi-Square for categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep1.get_code_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_cat = df_train[['Alternative Dispute Resolution',\n",
    "                         'COVID-19 Indicator',\n",
    "                         'IME-4 Count', 'Number of Dependents', 'Accident Year_', 'Accident Month_',\n",
    "                         'Assembly Year_', 'Assembly Month_', 'C-2 Year_', 'C-2 Month_',\n",
    "                         'C-3 Year_', 'C-3 Month_', 'First Hearing Year_', 'Agreement Reached',\n",
    "                         'First Hearing Month_', 'Gender_F', 'Gender_M', 'Gender_U', 'Gender_X',\n",
    "                         'Attorney/Representative_N', 'Attorney/Representative_Y',\n",
    "                         'Carrier Type Code_1A', 'Carrier Type Code_2A', 'Carrier Type Code_3A',\n",
    "                         'Carrier Type Code_4A', 'Carrier Type Code_5A', 'Carrier Type Code_5C',\n",
    "                         'Carrier Type Code_5D', 'Carrier Type Code_UN',\n",
    "                         'First Hearing held_False', 'First Hearing held_True',\n",
    "                         'County of Injury_fe', 'Zip Code_fe', 'Carrier Code__fe',\n",
    "                         'Medical Fee Code__fe', 'Carrier Type Code_fe']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi2_feature_importance(f1, target, significance_level=0.05, log=False):\n",
    "    \"\"\"\n",
    "    Evaluate if a feature is important to predict the target using the Chi-squared test.\n",
    "\n",
    "    Parameters:\n",
    "        data (pd.DataFrame): The dataset containing the feature and target.\n",
    "        feature (str): The name of the feature column.\n",
    "        target (str): The name of the target column.\n",
    "        significance_level (float): The threshold for statistical significance (default 0.05).\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the chi-squared statistic, p-value, and whether the feature is important.\n",
    "    \"\"\"\n",
    "    # Create a contingency table\n",
    "    contingency_table = pd.crosstab(f1, target)\n",
    "\n",
    "    # Perform the Chi-squared test\n",
    "    chi2_stat, p_value, _, _ = chi2_contingency(contingency_table)\n",
    "\n",
    "    total_observations = contingency_table.sum().sum()\n",
    "    rows, cols = contingency_table.shape\n",
    "\n",
    "    cramers_v = np.sqrt(\n",
    "        chi2_stat / (total_observations * min(cols - 1, rows - 1)))\n",
    "\n",
    "    is_important = p_value < significance_level\n",
    "\n",
    "    if log:\n",
    "        if cramers_v < 0.3:\n",
    "            print(f\" {is_important} the test is not significant {cramers_v}\")\n",
    "        elif cramers_v < 0.5:\n",
    "            print(f\" {is_important} the test is weakly significant {cramers_v}\")\n",
    "        elif cramers_v < 0.7:\n",
    "            print(f\" {is_important} the test is moderately significant {cramers_v}\")\n",
    "        elif cramers_v < 1:\n",
    "            print(f\" {is_important} the test is highly significant {cramers_v}\")\n",
    "        else:\n",
    "            print(f\"the test has no sense {cramers_v}\")\n",
    "\n",
    "    # Determine if the feature is important\n",
    "    is_important = p_value < significance_level\n",
    "    return cramers_v, p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function for classification based on correlation with target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid function\n",
    "def sigmoid(x, a=20):\n",
    "    return 1 / (1 + np.exp(-a * (x - 0.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_correlation_w_target(f1, target, significance_level=0.05, log=False, method='chi2', parameter_sigmoid=20):\n",
    "\n",
    "    if method == 'chi2':\n",
    "        cramers_v, p_value = chi2_feature_importance(\n",
    "            f1, target, significance_level, log)\n",
    "        score = (1 - p_value)*sigmoid(cramers_v, a=parameter_sigmoid)\n",
    "\n",
    "    elif method == 'entropy' or method == 'ent' or method == 'e' or method == 's':\n",
    "        score = (two_col_entropy_corr(f1, target) + 1) / 2\n",
    "\n",
    "    elif method == 'linear' or method == 'spearman' or method == 'l':\n",
    "        score = np.abs(f1.corr(target))\n",
    "\n",
    "    elif method == 'all':\n",
    "        cramers_v, p_value = chi2_feature_importance(\n",
    "            f1, target, significance_level, log)\n",
    "        try:\n",
    "            score1 = (1 - p_value)*sigmoid(cramers_v, a=parameter_sigmoid)\n",
    "        except Exception as e:\n",
    "            score1 = np.nan\n",
    "        try:\n",
    "            score2 = (two_col_entropy_corr(f1, target) + 1) / 2\n",
    "        except Exception as e:\n",
    "            score2 = np.nan\n",
    "        try:\n",
    "            score3 = np.abs(f1.corr(target))\n",
    "        except Exception as e:\n",
    "            score3 = np.nan\n",
    "        return score1, score2, score3\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_feature_correlation_scores(df, target):\n",
    "    corr_scores = pd.DataFrame()\n",
    "    for col in df.columns:\n",
    "        score1, score2, score3 = classify_correlation_w_target(df[col], target)\n",
    "        corr_scores[col] = [score1, score2, score3]\n",
    "    corr_scores = corr_scores.T\n",
    "    corr_scores.columns = ['chi2', 'entropy', 'linear']\n",
    "\n",
    "    corr_scores.index.name = 'feature'\n",
    "\n",
    "    lin_corr_df = classify_correlated_feature_maxmin(\n",
    "        df_train[num_cols], report=False, method='linear')\n",
    "    ent_corr_df = classify_correlated_feature_maxmin(\n",
    "        df_train[num_cols], report=False, method='entropy')\n",
    "\n",
    "    lin_corr_df = lin_corr_df.columns = ['linear_x']\n",
    "    ent_corr_df = ent_corr_df.columns = ['entropy_x']\n",
    "\n",
    "    corr_scores = pd.concat([lin_corr_df, ent_corr_df, corr_scores], axis=1)\n",
    "\n",
    "    return corr_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with the correlation scores\n",
    "corr_scores = pd.DataFrame()\n",
    "for var in df_train_cat.columns:\n",
    "    print(var)\n",
    "    s1, s2, s3 = classify_correlation_w_target(\n",
    "        df_train[var], y_train, method='all')\n",
    "    corr_scores[var] = [s1, s2, s3]\n",
    "\n",
    "corr_scores = corr_scores.T\n",
    "corr_scores.columns = ['chi2_y', 'entropy_y', 'linear_y']\n",
    "corr_scores.index.name = 'feature'\n",
    "\n",
    "lin_corr_df = classify_correlated_feature_maxmin(\n",
    "    df_train[num_cols], report=False, method='linear')\n",
    "lin_corr_df.columns = ['linear_x']\n",
    "lin_corr_df['linear_x'] = (1 - lin_corr_df['linear_x'])\n",
    "lin_corr_df['linear_x'] = lin_corr_df['linear_x'].abs()\n",
    "s_corr_df = classify_correlated_feature_maxmin(\n",
    "    df_train[num_cols], report=False, method='entropy', elbow=False)\n",
    "s_corr_df.columns = ['entropy_x']\n",
    "s_corr_df['entropy_x'] = (1 - s_corr_df['entropy_x'])\n",
    "s_corr_df['entropy_x'] = s_corr_df['entropy_x'].abs()\n",
    "\n",
    "feature_selection = pd.concat([lin_corr_df, s_corr_df, corr_scores], axis=1)\n",
    "pd.concat([lin_corr_df, s_corr_df, corr_scores], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formula 1 score\n",
    "points_in_formula1 = [25, 18, 15, 12, 10, 8, 6, 4, 2, 1]\n",
    "\n",
    "feature_selection['points'] = np.zeros(feature_selection.shape[0])\n",
    "for col in feature_selection.columns:\n",
    "    x = feature_selection[col].sort_values(ascending=False, inplace=False)\n",
    "    for place in range(10):\n",
    "        feature_selection.loc[x.index[place],\n",
    "                              'points'] += points_in_formula1[place]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first correlation with the target then the correlation between the features\n",
    "\n",
    "col_y = feature_selection.columns.str.contains('_y')\n",
    "col_x = feature_selection.columns.str.contains('_x')\n",
    "\n",
    "# maximum score among the features in col_y\n",
    "\n",
    "feature_selection['max_y'] = feature_selection[col_y].max(axis=1)\n",
    "feature_selection['max_x'] = feature_selection[col_x].max(axis=1)\n",
    "\n",
    "\n",
    "feature_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# choiche maker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a class called choose_features\n",
    "# it gets a dataframe as input and returns the best features\n",
    "\n",
    "class ChooseFeatures:\n",
    "\n",
    "    def init(self, method, n_features):\n",
    "        self.method = method\n",
    "        self.n_features = n_features\n",
    "\n",
    "    def fit(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def formula1_score(self, df_input):\n",
    "        df = df_input.copy()\n",
    "        points_in_formula1 = [25, 18, 15, 12, 10, 8, 6, 4, 2, 1]\n",
    "        df['points'] = np.zeros(df.shape[0])\n",
    "        for col in df.columns:\n",
    "            x = df[col].sort_values(ascending=False, inplace=False)\n",
    "            for place in range(10):\n",
    "                df.loc[x.index[place], 'points'] += points_in_formula1[place]\n",
    "        x = df['points'].sort_values(ascending=False, inplace=False)\n",
    "        return x.index[:self.n_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## random forest feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify the features fro most useful to least useful\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "clf.fit(df_train[num_cols], y_train)\n",
    "y_pred = clf.predict(df_valid[num_cols])\n",
    "print('Accuracy: ', accuracy_score(y_valid, y_pred))\n",
    "\n",
    "# get the feature importance\n",
    "feature_importances = clf.feature_importances_\n",
    "feature_importances = pd.Series(\n",
    "    feature_importances, index=df_train[num_cols].columns)\n",
    "feature_importances = feature_importances.sort_values(ascending=False)\n",
    "feature_importances.columns = ['fi_RandomForestClassifier']\n",
    "plt.figure(figsize=(12, 10))\n",
    "feature_importances.plot(kind='bar')\n",
    "plt.title('Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with the correlation scores\n",
    "corr_scores = pd.DataFrame()\n",
    "for var in df_dummies:\n",
    "    print(var)\n",
    "    s1, s2, s3 = classify_correlation_w_target(\n",
    "        df_train[var], y_train, method='all')\n",
    "    corr_scores[var] = [s1, s2, s3]\n",
    "\n",
    "corr_scores = corr_scores.T\n",
    "corr_scores.columns = ['chi2_y', 'entropy_y', 'linear_y']\n",
    "# name teh index column\n",
    "corr_scores.index.name = 'feature'\n",
    "lin_corr_df = classify_correlated_feature_maxmin(\n",
    "    df_train[num_cols], report=False, method='linear')\n",
    "lin_corr_df.columns = ['linear_x']\n",
    "s_corr_df = classify_correlated_feature_maxmin(\n",
    "    df_train[num_cols], report=False, method='entropy', elbow=False)\n",
    "s_corr_df.columns = ['entropy_x']\n",
    "\n",
    "\n",
    "feature_selection = pd.concat([lin_corr_df, s_corr_df, corr_scores], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.Series(\n",
    "    feature_importances, index=df_train[num_cols].columns)\n",
    "feature_importances = feature_importances.sort_values(ascending=False)\n",
    "plt.figure(figsize=(12, 10))\n",
    "feature_importances.plot(kind='bar')\n",
    "plt.title('Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "num_cols\n",
    "rfe = RFE(estimator=model, n_features_to_select=4)\n",
    "rfe.fit(df_train[num_cols], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe_cols = df_train[num_cols].columns[rfe.support_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[num_cols].columns[rfe.support_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model.fit(df_train[rfe_cols], y_train)\n",
    "y_pred = model.predict(df_valid[rfe_cols])\n",
    "accuracy_score(y_valid, y_pred)\n",
    "\n",
    "\n",
    "print(classification_report(y_valid, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## columns chosen by our methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representation problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add noise to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data_from_existing(existing_data, noise_level=0.1, random_state=None):\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "\n",
    "    # Add Gaussian noise to each feature in the existing data\n",
    "    synthetic_data = existing_data.copy()\n",
    "    for column in synthetic_data.columns:\n",
    "        noise = np.random.normal(\n",
    "            0, noise_level, size=synthetic_data[column].shape)\n",
    "        synthetic_data[column] += noise\n",
    "\n",
    "    return synthetic_data\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# Assuming `existing_data` is a DataFrame with your existing data\n",
    "existing_data = pd.DataFrame({\n",
    "    'f1': np.random.rand(100),\n",
    "    'f2': np.random.rand(100),\n",
    "    'target': np.random.rand(100)\n",
    "})\n",
    "\n",
    "synthetic_data = generate_synthetic_data_from_existing(\n",
    "    existing_data, noise_level=0.1, random_state=42)\n",
    "print(synthetic_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def representation_treshold(len_y, unique_y):\n",
    "    reurn(len_y/unique_y)-(m.sqrt(len_y/8 * 100)/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "def generate_synthetic_data_using_smote(existing_data, target_column, sampling_strategy='auto', random_state=None):\n",
    "    # Separate features and target\n",
    "    X = existing_data.drop(columns=[target_column])\n",
    "    y = existing_data[target_column]\n",
    "\n",
    "    # Apply SMOTE to generate synthetic data\n",
    "    smote = SMOTE(sampling_strategy=sampling_strategy,\n",
    "                  random_state=random_state)\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "    # Combine resampled features and target into a DataFrame\n",
    "    resampled_data = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "    resampled_data[target_column] = y_resampled\n",
    "\n",
    "    return resampled_data\n",
    "\n",
    "\n",
    "# Example usage\n",
    "# Assuming `existing_data` is a DataFrame with your existing data\n",
    "existing_data = pd.DataFrame({\n",
    "    'f1': np.random.rand(100),\n",
    "    'f2': np.random.rand(100),\n",
    "    # Imbalanced target\n",
    "    'target': np.random.choice([0, 1], size=100, p=[0.9, 0.1])\n",
    "})\n",
    "\n",
    "synthetic_data = generate_synthetic_data_using_smote(\n",
    "    existing_data, target_column='target', random_state=42)\n",
    "print(synthetic_data.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
